[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Analysis of Educational Data: A Field Guide Using R",
    "section": "",
    "text": "Preface\nWelcome to Computational Analysis of Educational Data: A Field Guide Using R!\n\nWhy this book?\nConventionally, as social scientists, we start the research process by generating research questions based on our previous knowledge and theories in the field. This is the conventional model of the order of operations in the process, and it is still the normative view of how it should work. However, it is also epistemologically possible that our observations of the world can guide our research questions. The process doesn’t always proceed in the orderly linear fashion suggested by the conventional model.\nWe are limited in how we see the world: we don’t know what we don’t know. For example, for someone who does not know that Twitter posts can be collected and analyzed as data to capture the state of the world, studying Twitter data will most likely not become a topic for a research question. Once you start seeing what can be data in the world, it starts shaping our ideas of what is “researchable”. Here is a simple model that we propose that shows the reciprocal nature of the research process, which is at the core of our aims in this book:\n\n\n\nFigure 1. Research - data relationship (Generated using Gemini Pro based on an earlier drawing by the authors)\n\n\nWe are writing this book because in the past few years what we described above has started happening for us. We have published work on Twitter data, Social Network analyses, natural language processing, and machine learning that was only possible after we learned what kind of data already existed around us. We thought other social scientists may benefit from a resource that not only tells them what is available as data but also guides them through concrete examples of going through this research journey along with us. We hope that along this journey you will develop your own research questions, and maybe even replicate some of the studies we imagined in this book.\nThe second most important aspect of this book that does not exist out in the wild is the “field guide” process where we work through the research design and reporting process. To do that, for each new computational research method, we follow this process:\n\nStart with what makes good data for that analysis (and how to capture it)\nSee what the data looks like (what it **has to **have, and what it can have)\nFormulate sample research questions based on the resources provided by the data and the type of analysis.\nGo through the analyses in R\nProvide a sample write up for Methods\nProvide a sample write up for Results\n\n\n\nWho is this book for?\nThis book will be beginner-friendly but not for the absolute beginner. We will dedicate the initial chapters to take you to resources that will help you get started. But, to make sense of this book, you should have basic research design knowledge, basic statistical knowledge, and a basic understanding of R and RStudio. At the same time, this book will not be for experts or expertise. There are already many great resources that delve into the topics that we cover (e.g., Silge’s book on using text data for machine learning).\nWe imagine that this book can become a part of doctoral coursework for future researchers, opening the doors for new ways to look at the world for research and data. Likewise, senior and junior academics/researchers would benefit greatly from this book to help them expand their research agendas.\nFor researchers like ourselves, we think this book can serve as a fun summer reading to rejuvenate and get excited about new research. At the same time, we also envision this book as a guidebook to keep on the side and frequently refer to, as researchers write up their work using these new methods.\nThis book will provide new ways to look at the world and formulate RQs. It will guide through the research process for each new method (including, friendly data organization tips, template for writing up and sharing this. We hope that you join us in this journey and this work will help open up new doors/embark on a journey of using computational research methods.\n\n\nOrganization of the books\nThe book is organized around four sections. Within these sections, there are specific chapters, which serve as field guide “entries” or “cases”. While the section overviews (the first chapter within each section) introduce the techniques or methodologies used in the rest of the section’s chapters, the chapters are intended to address a specific, narrow case, where we provide a sample write-up for researchers in writing their research questions, methods, results (and discussions) sections based on the analyses.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "chapter-1-1.html",
    "href": "chapter-1-1.html",
    "title": "Positron and RStudio",
    "section": "",
    "text": "Why Positron?\nYou need an environment for writing and running R code. We’re going to focus on Positron, Posit’s newer IDE built on the foundation of VS Code. If you’re already comfortable with RStudio, everything in this chapter applies there too—the fundamental concepts are identical, and the R code in this book works the same way in both.\nPositron represents where computational social science workflows are heading: multi-language support (R and Python in the same environment), modern editor features inherited from VS Code, and a more extensible architecture. If you’re coming from VS Code, Positron will feel immediately familiar. If you’re new to both, you’re learning a tool that’s designed for the future of data science.\nThat said, RStudio remains great. It’s mature, widely documented, and has a loyal community. Some researchers prefer its more opinionated interface—everything you need is visible and organized for R-first workflows. If you’re already using RStudio and prefer it, nothing in this book requires switching.\nThe instructions below work in both environments unless otherwise noted. When we say “Positron,” assume the same applies to RStudio.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Positron and RStudio</span>"
    ]
  },
  {
    "objectID": "chapter-1-1.html#the-four-essential-panes",
    "href": "chapter-1-1.html#the-four-essential-panes",
    "title": "Positron and RStudio",
    "section": "The Four Essential Panes",
    "text": "The Four Essential Panes\nBoth Positron and RStudio organize your workspace into four main areas (though the layout is customizable):\n\nSource/Editor (top-left): Where you write and edit your code files (.R scripts, .qmd documents)\nConsole (bottom-left): Where R actually executes code and shows results in real-time\nEnvironment/Variables (top-right): Shows the objects (data, functions, etc.) currently loaded in memory\nFiles/Plots/Help (bottom-right): Navigate your project files, view visualizations, and access documentation\n\n\n\n\nPositron Interface\n\n\n[Screenshot: Positron with all four panes visible and labeled]\n\nUnderstanding Each Pane\nSource/Editor Pane: This is where you spend most of your time writing code. The editor provides syntax highlighting (different colors for functions, strings, and comments), autocomplete suggestions, and the ability to work with multiple files in tabs. You can split the pane to view two files side-by-side, and for Quarto documents, you can preview the rendered output alongside your code.\nConsole Pane: When you run code, it executes here and displays results immediately. Press the up arrow to cycle through previously run commands—useful when you want to re-run or modify something. The prompt &gt; means R is ready for input; a + means R is waiting for you to complete a command (often because you forgot a closing parenthesis). You can clear the console output with Cmd/Ctrl + L, though this doesn’t delete your objects from memory.\nEnvironment/Variables Pane: This shows every object currently loaded in R—datasets, variables, functions you’ve created. Click on a dataset name to open it in a spreadsheet-style viewer. The display also shows object types (data frame, list, numeric vector) and basic information like the number of rows and columns. You can remove objects using the broom icon, which clears your workspace—useful when starting fresh.\nFiles/Plots/Help Pane: The Files tab lets you navigate your project directory, create new folders, and open files. The Plots tab shows your visualizations with forward/back arrows to review previous plots and an Export button to save images. The Help tab displays R documentation when you run commands like ?mean or search for functions. In RStudio, you’ll also see a Packages tab for installing and loading packages; Positron handles packages differently but with similar functionality.\nThe defaults work fine for now. As you get comfortable, you might adjust panel sizes or move things around, but the essential logic stays the same: code on the left, context on the right.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Positron and RStudio</span>"
    ]
  },
  {
    "objectID": "chapter-1-1.html#r-projects",
    "href": "chapter-1-1.html#r-projects",
    "title": "Positron and RStudio",
    "section": "R Projects",
    "text": "R Projects\nAlways work in an R Project. This is the single most important RStudio habit for reproducibility.\nAn R Project is a folder that contains all files for a particular analysis—data, scripts, outputs—and sets your working directory automatically. This means your code will work on any computer without changing file paths.\nTo create a new project:\n\nFile → New Project\nChoose “New Directory” or “Existing Directory”\nName your project (e.g., “my-first-css-analysis”)\nClick “Create Project”\n\nNotice the .Rproj file in your folder? That’s your project file. Double-click it to open Positron or RStudio with the correct working directory already set.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Positron and RStudio</span>"
    ]
  },
  {
    "objectID": "chapter-1-1.html#essential-keyboard-shortcuts",
    "href": "chapter-1-1.html#essential-keyboard-shortcuts",
    "title": "Positron and RStudio",
    "section": "Essential Keyboard Shortcuts",
    "text": "Essential Keyboard Shortcuts\nKeyboard shortcuts can speed up your workflow considerably. You don’t need to memorize them all at once—start with a few that feel natural, and others will follow as you get more comfortable.\n\n\n\n\n\n\n\n\nAction\nWindows/Linux\nmacOS\n\n\n\n\nRun current line or selection\nCtrl + Enter\nCmd + Enter\n\n\nNew document\nCtrl + Shift + N\nCmd + Shift + N\n\n\nSave file\nCtrl + S\nCmd + S\n\n\nFind in file\nCtrl + F\nCmd + F\n\n\nComment/uncomment code\nCtrl + Shift + C\nCmd + Shift + C\n\n\nInsert pipe operator (|&gt;)\nCtrl + Shift + M\nCmd + Shift + M\n\n\nInsert assignment operator (&lt;-)\nAlt + -\nOption + -\n\n\nRestart R session\nCtrl + Shift + F10\nCmd + Shift + 0\n\n\n\n[Screenshot: Help menu showing Keyboard Shortcuts option]\nThe run command (Ctrl/Cmd + Enter) is the one you’ll use most—it executes the line where your cursor is, or if you’ve selected multiple lines, it runs all of them. The comment shortcut is helpful when testing code: you can quickly comment out a section to see if removing it fixes an error.\nPositron and RStudio have slightly different shortcuts for a few operations, particularly around navigating panes and restarting R. To see the full list specific to your environment, go to Help → Keyboard Shortcuts or press Alt/Option + Shift + K.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Positron and RStudio</span>"
    ]
  },
  {
    "objectID": "chapter-1-1.html#extensions-and-customization",
    "href": "chapter-1-1.html#extensions-and-customization",
    "title": "Positron and RStudio",
    "section": "Extensions and Customization",
    "text": "Extensions and Customization\nPositron is built on VS Code, which means most VS Code extensions work in Positron. This gives you access to a large ecosystem of tools beyond what’s built into the base IDE.\nTo access extensions, open the Extensions panel—usually visible in the left sidebar, or accessible via View → Extensions. From there, you can search for extensions by name or browse categories.\n\nUseful Extensions for R Work\nWhile Positron comes with solid R support out of the box, these extensions can enhance your workflow:\n\nRainbow CSV: Colors columns in CSV files to make them easier to read\nGitLens: Shows detailed Git information inline (who changed what, when, and why)\nMarkdown Preview Enhanced: Better previews for Markdown documents with extended syntax support\n\nTo install an extension, search for it by name in the Extensions panel, click on it, and press Install. Most extensions work immediately without restarting Positron.\n[Screenshot: Extensions marketplace showing R-related extensions]\n\n\nRStudio’s Approach\nRStudio uses “addins” instead of extensions. These are R packages that add functionality to the IDE interface. The ecosystem is smaller than VS Code’s, but focused specifically on R workflows. If you’re using RStudio, you can find addins through the Addins menu once you’ve installed packages that provide them.\n\n\nCustomization\nBoth Positron and RStudio let you customize themes, font size, and panel arrangements. The defaults work well to start, but if you find yourself squinting at text or want a different color scheme, explore Preferences/Settings → Appearance. Changes are a matter of personal preference rather than workflow improvement, so don’t feel pressure to customize immediately.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Positron and RStudio</span>"
    ]
  },
  {
    "objectID": "chapter-1-1.html#what-makes-positron-stand-out",
    "href": "chapter-1-1.html#what-makes-positron-stand-out",
    "title": "Positron and RStudio",
    "section": "What Makes Positron Stand Out",
    "text": "What Makes Positron Stand Out\nBeyond the basics, Positron offers several features that become valuable as your projects grow in complexity:\nMulti-language support: You can work with R and Python in the same project without switching between different environments. If you later need to use Python libraries or call Python code from R (common in machine learning workflows), Positron handles both languages natively.\nTerminal integration: The built-in terminal in Positron has better shell support than RStudio’s terminal. This matters when you need to run command-line tools, manage Docker containers, or work with system utilities alongside your R code.\nRemote development: Through SSH extensions, you can connect to remote servers and work on them as if they were local. Your code runs on the server, but Positron’s interface stays responsive on your machine. This is useful when analyzing large datasets that don’t fit on your laptop or when you need more computing power.\nPerformance: The editor itself is noticeably faster with large files—syntax highlighting, scrolling, and searching all feel snappier. This doesn’t matter much for typical scripts, but when you’re working with thousand-line codebases or large Quarto documents, the difference shows.\nModern editor features: IntelliSense provides context-aware code completion (it knows what functions are available on specific objects), bracket matching highlights matching parentheses and braces automatically, and a minimap gives you a bird’s-eye view for navigating long files.\nThese features are there when you need them. For now, just know they exist—they’re part of why Positron represents where these tools are heading.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Positron and RStudio</span>"
    ]
  },
  {
    "objectID": "chapter-1-1.html#version-control-with-git-and-github",
    "href": "chapter-1-1.html#version-control-with-git-and-github",
    "title": "Positron and RStudio",
    "section": "Version Control with Git and GitHub",
    "text": "Version Control with Git and GitHub\nGit is how computational social scientists version, share, and collaborate on their work. GitHub is the most popular platform for hosting Git repositories. If you’ve never used version control before, this section will get you set up with a workflow that integrates directly into your IDE.\n\nWhy Git Matters\nWithout version control, you end up with files like analysis_final.R, analysis_final2.R, analysis_ACTUALLY_final.R. Git tracks every change you make, so you can:\n\nGo back to any previous version without keeping multiple copies\nExperiment freely—if something breaks, you can always revert\nCollaborate with others without emailing files back and forth\nShare your work publicly for transparency and reproducibility\n\nMost computational social science projects live in Git repositories, and employers expect researchers to be comfortable with basic version control.\n\n\nGit Integration\nOnce Git is configured, you’ll see version control features built into your environment:\n\nIn RStudio: A “Git” tab appears in the Environment pane (top-right)\nIn Positron: Git integration works through the Source Control panel (often on the left sidebar)\n\nEither way, you can stage changes, write commit messages, and push to GitHub without leaving your editor. This is far friendlier than memorizing terminal commands.\n\n\nSetting Up Git and GitHub: The Path of Least Resistance\nGit authentication is notoriously confusing. We’re going to use an R-native approach that minimizes pain: Personal Access Tokens (PAT) via the usethis package. This keeps you in familiar territory and works reliably for the kind of work you’ll do in this book.\n\nStep 1: Install Required Packages\ninstall.packages(\"usethis\")\ninstall.packages(\"gitcreds\")\n\n\nStep 2: Configure Your Git Identity\nTell Git who you are (one-time setup):\nusethis::use_git_config(\n  user.name = \"Jane Doe\",           # Your actual name\n  user.email = \"jane@example.com\"   # Email associated with GitHub\n)\nImportant: Use the same email address you’ll use (or already use) for your GitHub account.\n\n\nStep 3: Create a GitHub Personal Access Token\nusethis::create_github_token()\nThis opens GitHub in your browser. You’ll need to:\n\nLog in to GitHub if you haven’t already\nLeave the default scopes checked (at minimum: “repo” and “workflow”)\nSet expiration to 90 days (good security practice, though you’ll regenerate when it expires)\nClick “Generate token” at the bottom\nCopy the token immediately—GitHub only shows it once\n\nThe token looks like: ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n\nStep 4: Store Your Token in R\ngitcreds::gitcreds_set()\nWhen prompted, paste your token and press Enter. R stores it securely for future use.\n\n\nStep 5: Verify Everything Worked\nusethis::git_sitrep()\nThis diagnostic report should show: - Your name and email - “Personal access token for ‘https://github.com’: ‘&lt;discovered&gt;’”\nIf so, you’re all set. If not, see troubleshooting below.\n\n\n\nAlternative: SSH Keys (For the Long Haul)\nSSH keys can be convenient—set them up once and never think about authentication again. But they’re more complex to configure initially.\nInterested? See Happy Git with R, Chapters 10-12. For now, the PAT method will serve you well.\n\n\nUsing Git in Practice\nWe’ll introduce Git commands as you need them throughout the book. For now, know that your IDE gives you a graphical interface for the most common operations:\n\nPull: Download changes from GitHub (always do this before starting work)\nCommit: Save a snapshot of your changes with a descriptive message\nPush: Upload your commits to GitHub\n\n[Screenshot: Source Control panel showing staged changes]\nYou can also use terminal commands (git status, git add, git commit, git push) if you prefer, but the IDE interface handles 90% of everyday tasks.\nThe key habit: commit often with clear messages (“Add data cleaning script” rather than “changes”), and push regularly so your work is backed up and others can see your progress.\n\nA Brief Note on Branches\nSo far we’ve discussed the basic workflow on a single branch (usually called main). Branches let you create parallel versions of your code for experimentation or collaboration without affecting your main work.\nThink of branches as separate workspaces: you can try a new analysis approach on a feature branch, and if it works out, merge it back into main. If it doesn’t work, you can simply delete the branch—your main code remains untouched.\nWhen to use branches:\n\nExperimenting with new approaches: Try different analytical methods without risking your working code\nCollaborating on specific features: Each person works on their own branch, then merges when ready\nMaintaining different versions: Keep a stable main branch while developing updates separately\n\nYour IDE makes branch management visual. In Positron, the Source Control panel shows your current branch and lets you create, switch, and merge branches through menus. In RStudio, the Git pane has a branch dropdown for the same operations.\nFor now, working on the main branch is sufficient. We’ll cover branching in more detail when you need it for collaboration or when your projects become more complex.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Positron and RStudio</span>"
    ]
  },
  {
    "objectID": "chapter-1-1.html#going-further",
    "href": "chapter-1-1.html#going-further",
    "title": "Positron and RStudio",
    "section": "Going Further",
    "text": "Going Further\nThis chapter gave you a working environment. When you’re ready to explore more:\n\nPositron Documentation: Official docs for Positron features\nRStudio User Guide: Comprehensive guide to all RStudio capabilities\nHappy Git and GitHub for the useR: Everything Git-related for R users\nGitHub Skills: Interactive tutorials for Git and GitHub\n\nAdvanced features worth exploring later:\n\nCode snippets: Type abbreviations that expand to code templates\nDebugger: Step through code line-by-line when things break\nLive Share (Positron/VS Code): Collaborate in real-time by sharing your workspace with others",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Positron and RStudio</span>"
    ]
  },
  {
    "objectID": "chapter-1-2.html",
    "href": "chapter-1-2.html",
    "title": "R",
    "section": "",
    "text": "Three Ways to Write R Code\nWe’re not going to teach you RStudio comprehensively because excellent resources already exist for that purpose. We particularly recommend R for Data Science, Data Science in Education Using R, the RStudio primers, and Happy Git and GitHub for the useR.\nInstead, this chapter covers just enough to reproduce our workflows and extend them to your own data science work. Think of this as the minimal viable setup for doing the work in this book.\nPositron supports multiple file types for writing R code. You’ll encounter all three in data science work:",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R</span>"
    ]
  },
  {
    "objectID": "chapter-1-2.html#three-ways-to-write-r-code",
    "href": "chapter-1-2.html#three-ways-to-write-r-code",
    "title": "R",
    "section": "",
    "text": "R Scripts (.R)\nPlain text files containing R code, executed line-by-line or all at once. Great for data processing pipelines, functions, and code you’ll reuse.\nCreate one: File → New File → R Script\n# This is an R script\n# Lines starting with # are comments\ndata &lt;- read.csv(\"mydata.csv\")\nsummary(data)\nHere’s a more realistic example using student assessment data. Save this as explore-data.R:\n# Example: Quick data exploration in an R script\nlibrary(tidyverse)\n\n# Read the data\nstudents &lt;- read_csv(\"data/oulad-students-and-assessments.csv\")\n\n# Quick summaries\nsummary(students$mean_weighted_score)\ntable(students$final_result)\n[Screenshot: R script open in Source pane]\nThis script loads the tidyverse package (more on this in the next chapter), reads a CSV file, and produces quick summaries. You can run it line-by-line using Cmd/Ctrl + Enter, or run the entire script at once. The output appears in the Console, but the script itself is saved for later use.\n\n\nR Markdown (.Rmd)\nDocuments that weave together narrative text (in Markdown) and code chunks. When you “knit” an R Markdown file, it executes the code and generates a formatted document (HTML, PDF, or Word).\nYou should know R Markdown exists because you’ll encounter it in older resources and projects, but we’re not using it in this book.\n\n\nQuarto (.qmd)\nQuarto is the successor to R Markdown, with better multi-language support (R, Python, Julia) and more consistent syntax. It’s what we’ll use throughout this book.\nCreate one: File → New File → Quarto Document\nThe structure looks similar to R Markdown, but the rendering engine is more powerful:\n---\ntitle: \"My Analysis\"\nformat: html\n---\n\n## Introduction\n\nThis is regular text.\n```{r}\n# This is a code chunk\nx &lt;- 1:10\nmean(x)\n```\nClick the “Render” button (or Ctrl/Cmd + Shift + K) to execute all code and generate your document.\nHere’s a more complete Quarto example analyzing student data:\n---\ntitle: \"OULAD Student Analysis\"\nformat: html\n---\n\n## Loading Data\n\nFirst, we'll load the student assessment data from the Open University Learning Analytics Dataset:\n\n```{r}\nlibrary(tidyverse)\nstudents &lt;- read_csv(\"data/oulad-students-and-assessments.csv\")\n```\n\n## Pass Rates by Gender\n\nLet's examine how pass rates differ by gender:\n\n```{r}\nstudents |&gt;\n  group_by(gender, pass) |&gt;\n  count() |&gt;\n  pivot_wider(names_from = pass, values_from = n)\n```\n\nThe table above shows the number of students who passed (1) and didn't pass (0) for each gender. We can see that both groups have substantial representation in the dataset.\n[Screenshot: Rendered Quarto document showing code and output]\nWhen you render this document, Quarto executes each code chunk in order, captures the output, and weaves it together with your explanatory text. The result is an HTML document where readers see your code, your results, and your interpretation all together. This makes your analysis reproducible—anyone with your data can re-run your Quarto file and get the same results.\nKey difference for beginners: Think of R scripts as “just code” and Quarto as “code + explanation + output” in one document. Use scripts for behind-the-scenes work, Quarto for analysis you want to share or understand later.\n\n\nControlling Chunk Behavior with Options\nQuarto lets you control what appears in your rendered document using chunk options. These are specified at the top of each code chunk using #| syntax.\nCommon options include:\n\n#| echo: false — Hide the code, show only the output\n#| eval: false — Show the code but don’t run it\n#| warning: false — Suppress warning messages\n#| message: false — Suppress messages (like package loading notifications)\n#| fig-width: 8 and #| fig-height: 6 — Control plot dimensions in inches\n\nHere’s an example showing how to hide code while displaying a plot:\n```{r}\n#| echo: false\n#| message: false\n\nlibrary(tidyverse)\nstudents &lt;- read_csv(\"data/oulad-students-and-assessments.csv\")\n\nstudents |&gt;\n  ggplot(aes(x = final_result)) +\n  geom_bar() +\n  labs(title = \"Distribution of Final Results\")\n```\n[Screenshot: Quarto document showing chunk options at top of code block]\nWhen rendered, readers see the plot but not the code that created it. This is useful for final reports where you want to emphasize results rather than implementation details.\nYou can also set options globally for the entire document by adding them to your YAML header. For example, to hide all code by default:\n---\ntitle: \"My Analysis\"\nformat: html\nexecute:\n  echo: false\n---\nIndividual chunks can override global settings. For more options, see the Quarto documentation on execution options.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R</span>"
    ]
  },
  {
    "objectID": "chapter-1-2.html#console-vs.-source-exploration-vs.-preservation",
    "href": "chapter-1-2.html#console-vs.-source-exploration-vs.-preservation",
    "title": "R",
    "section": "Console vs. Source: Exploration vs. Preservation",
    "text": "Console vs. Source: Exploration vs. Preservation\nA common pattern in data science work:\n\nConsole: Quick exploration, testing ideas, checking output. Code here disappears when you close Positron.\nSource (scripts/Quarto): Code you want to keep, reproduce, or share. This is your permanent record.\n\nHere’s what this looks like in practice. Try this in the Console for a quick calculation:\n# In Console - just exploring\nmean(c(85, 92, 78, 95, 88))\n[1] 87.6\nThat’s useful for a quick check, but if you want to preserve this process for later:\n# In a script - keeping a record\ntest_scores &lt;- c(85, 92, 78, 95, 88)\naverage_score &lt;- mean(test_scores)\nprint(average_score)\nThe second approach creates a record you can return to, modify, and share. If you later need to change the scores or calculate something else, the script preserves your workflow.\nEarly on, you might type everything in the Console. That’s fine for learning! But as soon as you do something you want to remember, put it in a script or Quarto file.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R</span>"
    ]
  },
  {
    "objectID": "chapter-1-2.html#when-you-get-stuck",
    "href": "chapter-1-2.html#when-you-get-stuck",
    "title": "R",
    "section": "When You Get Stuck",
    "text": "When You Get Stuck\n\nReading Error Messages\nWhen something goes wrong, R prints an error message in the Console (usually in red). These messages often look cryptic at first, but they’re trying to help:\nError in mean(x) : object 'x' not found\nThis tells you exactly what went wrong: R couldn’t find an object called x. Either you misspelled it, or you haven’t created it yet.\n\n\nGetting Help on Functions\nR has built-in documentation for every function:\n?mean           # Opens help for the mean() function\n??regression    # Searches all help files for \"regression\"\nHelp files include descriptions, arguments, examples, and related functions.\n\n\nWhere Computational Social Scientists Get Help\nWhen you’re truly stuck, the R community is helpful:\n\nRStudio Community: Welcoming forum for all R questions\nStack Overflow: Searchable archive of millions of questions\n\nBefore asking a question, try searching for your error message. Someone has likely encountered it before. Of course, turning to AI can also help, as we describe next.\n\n\nLarge Language Models and AI Tools for Getting Unstuck\nIn Section 3, we cover using large language models as part of a research workflow. But LLMs are also useful for getting unstuck when you’re learning and this is increasingly how working data scientists operate.\nTwo approaches work well:\n\nAsk LLMs directly: ChatGPT, Claude, or your preferred model can explain error messages, suggest debugging approaches, and clarify confusing documentation. Copy your error message, describe what you were trying to do, and ask for help.\nUse AI tools directly in Positron: AI coding assistants can work inside your editor to suggest code completions, explain what existing code does, and generate boilerplate. We’ll cover how to set these up in the next section.\n\nThe best programmers don’t memorize every function—they know how to find answers quickly. LLMs have become part of that toolkit, alongside documentation, Stack Overflow, and experimentation. Use them when you’re stuck, but also pay attention to why solutions work. That’s how you build intuition.\n\nUsing AI Tools Directly in Positron\nPositron, being built on VS Code, supports AI assistant extensions that can help you write code more efficiently and understand errors more quickly.\nAI Integration Options\nSeveral AI extensions work in Positron:\n\nGitHub Copilot: The most popular AI pair programmer, offering real-time code suggestions\nClaude for VS Code: Provides AI assistance with code explanation and generation\nContinue: An open-source AI coding assistant that works with multiple LLM providers\n\nWe’ll focus on GitHub Copilot as it’s widely adopted in data science work, but the workflow is similar for other extensions.\nSetting Up GitHub Copilot\n\nOpen the Extensions panel in Positron (View → Extensions, or the extensions icon in the left sidebar)\nSearch for “GitHub Copilot”\nClick Install on the official GitHub Copilot extension\nAfter installation, you’ll be prompted to sign in with your GitHub account\nClick “Sign in to GitHub” and complete the authentication in your browser\n\nNote on access: GitHub Copilot requires a subscription (around $10/month), though it’s free for verified students and teachers. If you have a student email address, you can get free access through GitHub Education.\nUsing Copilot\nOnce installed, Copilot suggests code as you type. For example, if you write a comment describing what you want to do:\n# Calculate the mean score by gender and pass status\nCopilot will suggest the code to accomplish this. Press Tab to accept the suggestion, or keep typing to ignore it. The suggestions appear in gray text as you work.\n[Screenshot: Copilot providing code suggestion in Positron]\nYou can also:\n\nAsk Copilot to explain code: Highlight code and right-click → Copilot → Explain This\nRequest alternatives: If a suggestion isn’t quite right, you can cycle through alternatives using Alt/Option + ]\nUse Copilot Chat: Open the chat panel to have a conversation about your code\n\nWhen to Use Inline AI vs. External LLMs\nDifferent tools excel at different tasks:\n\nInline AI (Copilot): Quick code completions, writing repetitive code, remembering function syntax, generating common patterns\nExternal LLMs (ChatGPT/Claude): Complex explanations, understanding concepts, debugging logic errors, learning new approaches\n\nMany programmers use both: Copilot for day-to-day coding, external LLMs when they’re truly stuck or learning something new.\nImportant Caution\nAI suggestions aren’t always correct. Always review code before accepting it, especially for data analysis where subtle errors can lead to wrong conclusions. Use AI tools to speed up your work, but verify that the code does what you think it does.\nPlease see Section 1.4 for more information about the relationship between you, data, and LLMs.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R</span>"
    ]
  },
  {
    "objectID": "chapter-1-3.html",
    "href": "chapter-1-3.html",
    "title": "Tidyverse",
    "section": "",
    "text": "What’s in the Tidyverse?\nThe tidyverse is a collection of R packages designed for data science with a shared philosophy: code should be readable, consistent, and focused on the data transformations you’re actually trying to accomplish. If you’ve struggled with base R’s sometimes cryptic syntax, the tidyverse will feel like a relief.\nMost computational social science workflows rely on tidyverse packages for data manipulation, visualization, and modeling. Learning this ecosystem means learning the tools your collaborators use, the examples you’ll find online, and the approaches that scale from quick exploration to publication-ready analysis.\nThe tidyverse is actually a meta-package that loads eight core packages when you run library(tidyverse):\nMost of the work we do in this chapter relies heavily on dplyr and ggplot2, with the others playing supporting roles as needed.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "chapter-1-3.html#whats-in-the-tidyverse",
    "href": "chapter-1-3.html#whats-in-the-tidyverse",
    "title": "Tidyverse",
    "section": "",
    "text": "ggplot2: Data visualization with a grammar of graphics\ndplyr: Data manipulation (filtering, selecting, summarizing, joining)\ntidyr: Reshaping data between wide and long formats\nreadr: Reading rectangular data (CSV, TSV) efficiently\npurrr: Functional programming tools for iteration\ntibble: Modern reimagining of data frames\nstringr: String manipulation\nforcats: Working with categorical variables (factors)",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "chapter-1-3.html#installing-and-loading-the-tidyverse",
    "href": "chapter-1-3.html#installing-and-loading-the-tidyverse",
    "title": "Tidyverse",
    "section": "Installing and Loading the Tidyverse",
    "text": "Installing and Loading the Tidyverse\nYou install packages like any others in R.\nInstallation (one time):\ninstall.packages(\"tidyverse\")\nLoading (at the start of each session or script):\nlibrary(tidyverse)\nWhen you load the tidyverse, you’ll see a message listing which packages were attached and any conflicts (functions from tidyverse packages that mask base R functions). The conflicts are normal and intentional—tidyverse functions are generally preferable for data science work.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "chapter-1-3.html#working-with-real-data",
    "href": "chapter-1-3.html#working-with-real-data",
    "title": "Tidyverse",
    "section": "Working with Real Data",
    "text": "Working with Real Data\nLet’s load actual data to see the tidyverse in action. We’ll use student assessment data from the Open University Learning Analytics Dataset (OULAD), which contains information about student demographics and course outcomes.\nlibrary(tidyverse)\n\n# Read the data\nstudents &lt;- read_csv(\"data/oulad-students-and-assessments.csv\")\n\n# Take a quick look\nglimpse(students)\n[Screenshot: Console showing glimpse() output of students data]\nThe glimpse() function shows you the structure of your data: how many rows and columns, what type each column is (character, numeric, etc.), and the first few values. This dataset contains information about students (gender, age, region, disability status) along with their course performance (final_result, pass status, mean_weighted_score).\nThis is the dataset we’ll use in the examples below. It represents the kind of educational data you’ll work with in computational social science: thousands of observations with both categorical and continuous variables.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "chapter-1-3.html#essential-dplyr-functions",
    "href": "chapter-1-3.html#essential-dplyr-functions",
    "title": "Tidyverse",
    "section": "Essential dplyr Functions",
    "text": "Essential dplyr Functions\nThe dplyr package provides functions for the most common data manipulation tasks. Here are the ones you’ll use repeatedly.\n\nfilter() - Selecting Rows\nUse filter() to keep only rows that meet certain conditions:\n# Keep only students who passed\npassed_students &lt;- students |&gt;\n  filter(pass == 1)\n\n# Multiple conditions with AND (comma means AND)\npassed_females &lt;- students |&gt;\n  filter(pass == 1, gender == \"F\")\n\n# OR condition using |\ngood_outcomes &lt;- students |&gt;\n  filter(pass == 1 | mean_weighted_score &gt; 800)\nThis gives us 22,333 passed students from the original 32,593. Use == for equality tests, &gt; and &lt; for comparisons, and combine conditions with commas (AND) or | (OR).\n\n\nselect() - Choosing Columns\nUse select() to pick which columns to keep or remove:\n# Select specific columns\nstudent_basics &lt;- students |&gt;\n  select(id_student, gender, age_band, final_result)\n\n# Remove columns with minus sign\nstudent_no_dates &lt;- students |&gt;\n  select(-date_registration, -date_unregistration)\n\n# Select by pattern\nstudent_demographics &lt;- students |&gt;\n  select(starts_with(\"id\"), gender, region)\nThe select() function is useful when you have many columns but only need a few for analysis. Helper functions like starts_with(), ends_with(), and contains() make it easy to select groups of related columns.\n\n\nmutate() - Creating or Modifying Columns\nUse mutate() to add new columns or change existing ones:\n# Create new column\nstudents &lt;- students |&gt;\n  mutate(high_achiever = mean_weighted_score &gt; 800)\n\n# Modify existing column\nstudents &lt;- students |&gt;\n  mutate(age_band = factor(age_band))\n\n# Multiple new columns at once\nstudents &lt;- students |&gt;\n  mutate(\n    score_category = case_when(\n      mean_weighted_score &gt; 800 ~ \"High\",\n      mean_weighted_score &gt; 650 ~ \"Medium\",\n      TRUE ~ \"Low\"\n    )\n  )\nThe case_when() function inside mutate() handles conditional logic: if mean_weighted_score is over 800, assign “High”; if over 650, assign “Medium”; otherwise (TRUE), assign “Low”.\n\n\nsummarize() - Calculating Summaries\nUse summarize() to collapse your data into summary statistics:\n# Overall statistics\nstudents |&gt;\n  summarize(\n    mean_score = mean(mean_weighted_score, na.rm = TRUE),\n    pass_rate = mean(pass, na.rm = TRUE),\n    n_students = n()\n  )\n\n# Output:\n#   mean_score pass_rate n_students\n#        722.4     0.685     32,593\nThe na.rm = TRUE argument tells R to ignore missing values when calculating means. The n() function counts the number of rows.\n\n\ngroup_by() - Operations by Group\nUse group_by() before summarize() to calculate statistics for each group separately:\n# Pass rates by gender\nstudents |&gt;\n  group_by(gender) |&gt;\n  summarize(\n    pass_rate = mean(pass, na.rm = TRUE),\n    n = n()\n  )\n\n# Output:\n#   gender pass_rate     n\n#   F          0.701 17,329\n#   M          0.665 15,264\n\n# Multiple grouping variables\nstudents |&gt;\n  group_by(gender, disability) |&gt;\n  summarize(mean_score = mean(mean_weighted_score, na.rm = TRUE))\nAfter group_by(), subsequent operations happen separately for each group. This is one of the most powerful patterns in data analysis: split your data into groups, apply a function to each group, then combine the results.\n\n\narrange() - Sorting Rows\nUse arrange() to reorder rows:\n# Sort by score (ascending by default)\nstudents |&gt;\n  arrange(mean_weighted_score)\n\n# Sort descending\nstudents |&gt;\n  arrange(desc(mean_weighted_score))\n\n# Multiple sort keys\nstudents |&gt;\n  arrange(gender, desc(mean_weighted_score))\nThis is useful when you want to see the highest or lowest values, or when you need data in a particular order for plotting or reporting.\n\n\ncount() - Quick Frequency Tables\nUse count() as a shortcut for group_by() + summarize(n = n()):\n# Count final results\nstudents |&gt;\n  count(final_result)\n\n# Output:\n#   final_result     n\n#   Distinction  3,124\n#   Fail         7,052\n#   Pass        18,229\n#   Withdrawn    4,188\n\n# Count with percentages\nstudents |&gt;\n  count(final_result) |&gt;\n  mutate(percent = n / sum(n) * 100)\nThe count() function is perfect for quickly understanding the distribution of categorical variables.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "chapter-1-3.html#the-pipe-operator-and",
    "href": "chapter-1-3.html#the-pipe-operator-and",
    "title": "Tidyverse",
    "section": "The Pipe Operator: |> and %>%",
    "text": "The Pipe Operator: |&gt; and %&gt;%\nThe tidyverse introduced a distinctive style: piping operations together to create readable data transformation pipelines. Instead of nesting functions inside each other, you “pipe” the output of one function into the next.\nR now has a native pipe operator |&gt; (introduced in R 4.1), while the tidyverse originally used %&gt;% from the magrittr package. They work almost identically for most purposes.\nHere’s a concrete example using the dplyr functions we just learned:\n# Without pipes (nested, hard to read)\nsummarize(\n  group_by(\n    filter(students, pass == 1),\n    gender\n  ),\n  mean_score = mean(mean_weighted_score, na.rm = TRUE)\n)\n\n# With pipes (clear, step-by-step)\nstudents |&gt;\n  filter(pass == 1) |&gt;\n  group_by(gender) |&gt;\n  summarize(mean_score = mean(mean_weighted_score, na.rm = TRUE))\n\n# Output:\n#   gender mean_score\n#   F           736.4\n#   M           724.8\nThe piped version reads like instructions: “Take students, then keep only those who passed, then group by gender, then calculate mean scores.” Each step is on its own line, making it easy to follow the logic.\nWe’ll use the native pipe |&gt; throughout this book because it’s now built into R, but you’ll encounter %&gt;% in older code and documentation. For practical purposes, they’re interchangeable in most tidyverse workflows.\nHow to read it: Think of |&gt; as “then.” The pipe takes the result from the left and passes it as the first argument to the function on the right.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "chapter-1-3.html#data-visualization-with-ggplot2",
    "href": "chapter-1-3.html#data-visualization-with-ggplot2",
    "title": "Tidyverse",
    "section": "Data Visualization with ggplot2",
    "text": "Data Visualization with ggplot2\nThe ggplot2 package uses a “grammar of graphics” approach where you build plots in layers. You start with your data, specify how variables map to visual properties (x-axis, y-axis, colors), then add geometric objects like points, bars, or lines.\n\nBasic Structure\nEvery ggplot follows this template:\nggplot(data = DATA, aes(x = X_VAR, y = Y_VAR)) +\n  geom_FUNCTION()\nThe aes() function (short for “aesthetics”) maps your data columns to visual properties. The geom_ functions specify what kind of plot to draw.\n\n\nBar Charts - Categorical Data\nUse geom_bar() to visualize counts of categorical variables:\n# Simple bar chart\nstudents |&gt;\n  ggplot(aes(x = final_result)) +\n  geom_bar()\nThis automatically counts how many students have each final result (Pass, Fail, Distinction, Withdrawn) and displays the counts as bars.\n# Stacked bar chart by group\nstudents |&gt;\n  ggplot(aes(x = gender, fill = final_result)) +\n  geom_bar()\n[Screenshot: Stacked bar chart showing final results by gender]\nThe fill aesthetic colors the bars by final_result, creating stacked bars that show how outcomes differ between genders.\n# Side-by-side bars\nstudents |&gt;\n  ggplot(aes(x = gender, fill = final_result)) +\n  geom_bar(position = \"dodge\")\nSetting position = \"dodge\" puts bars next to each other instead of stacking them, making it easier to compare counts directly.\n\n\nHistograms - Distributions\nUse geom_histogram() to see the distribution of continuous variables:\n# Distribution of scores\nstudents |&gt;\n  ggplot(aes(x = mean_weighted_score)) +\n  geom_histogram(binwidth = 50)\nThe binwidth argument controls how wide each bar is. Smaller values show more detail, larger values show broader patterns.\n# Separate distributions by group\nstudents |&gt;\n  filter(!is.na(mean_weighted_score)) |&gt;\n  ggplot(aes(x = mean_weighted_score, fill = factor(pass))) +\n  geom_histogram(binwidth = 50, position = \"identity\", alpha = 0.5)\nUsing position = \"identity\" overlays the histograms, and alpha = 0.5 makes them semi-transparent so you can see both distributions.\n\n\nBoxplots - Comparing Distributions\nUse geom_boxplot() to compare distributions across groups:\n# Scores by final result\nstudents |&gt;\n  filter(!is.na(mean_weighted_score)) |&gt;\n  ggplot(aes(x = final_result, y = mean_weighted_score)) +\n  geom_boxplot()\n[Screenshot: Boxplot showing score distributions by final result]\nBoxplots show the median (middle line), quartiles (box edges), and outliers (individual points). This makes it easy to see that Distinction students have higher median scores than Pass students, who score higher than those who Fail or Withdraw.\n# With colors\nstudents |&gt;\n  filter(!is.na(mean_weighted_score)) |&gt;\n  ggplot(aes(x = final_result, y = mean_weighted_score, fill = final_result)) +\n  geom_boxplot()\nAdding fill colors each boxplot by the category, making them easier to distinguish.\n\n\nScatter Plots - Relationships\nUse geom_point() to explore relationships between two continuous variables:\n# Registration timing vs. scores\nstudents |&gt;\n  filter(!is.na(mean_weighted_score)) |&gt;\n  ggplot(aes(x = date_registration, y = mean_weighted_score)) +\n  geom_point(alpha = 0.3)\nThe alpha parameter makes points semi-transparent, which helps when many points overlap. This plot would show whether students who register earlier tend to score higher or lower.\n# Add a trend line\nstudents |&gt;\n  filter(!is.na(mean_weighted_score)) |&gt;\n  ggplot(aes(x = date_registration, y = mean_weighted_score)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\")\nAdding geom_smooth(method = \"lm\") fits a linear model and displays the trend line with a confidence band, making patterns easier to see.\n\n\nImproving Plots with Labels and Themes\nGood plots need clear labels:\nstudents |&gt;\n  ggplot(aes(x = final_result)) +\n  geom_bar(fill = \"steelblue\") +\n  labs(\n    title = \"Distribution of Student Outcomes\",\n    x = \"Final Result\",\n    y = \"Number of Students\"\n  ) +\n  theme_minimal()\nThe labs() function adds titles and axis labels. The theme_minimal() function applies a clean, simple theme. Other themes include theme_bw(), theme_classic(), and theme_light().\n\n\nFaceting - Small Multiples\nUse faceting to create separate plots for different groups:\n# Separate plot for each course module\nstudents |&gt;\n  ggplot(aes(x = final_result)) +\n  geom_bar() +\n  facet_wrap(~code_module)\nThis creates a grid of small bar charts, one for each module. It’s useful when you want to compare patterns across many categories.\n# Grid by two variables\nstudents |&gt;\n  filter(!is.na(mean_weighted_score)) |&gt;\n  ggplot(aes(x = mean_weighted_score)) +\n  geom_histogram(binwidth = 50) +\n  facet_grid(gender ~ disability)\n[Screenshot: Faceted histogram showing score distributions by gender and disability status]\nfacet_grid() creates a matrix of plots: rows for one variable (gender), columns for another (disability). This lets you see how score distributions vary across combinations of factors.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "chapter-1-3.html#next-steps",
    "href": "chapter-1-3.html#next-steps",
    "title": "Tidyverse",
    "section": "Next Steps",
    "text": "Next Steps\nYou’ve now seen core tidyverse functions in action with real data. The seven dplyr functions covered here—filter(), select(), mutate(), summarize(), group_by(), arrange(), and count()—handle the majority of everyday data manipulation tasks. The ggplot2 visualizations—bar charts, histograms, boxplots, scatter plots, and faceting—cover most exploratory analysis needs.\nThe subsequent chapters in this book use these functions extensively, and we’ll introduce additional tidyverse capabilities as they become relevant. You’ll continue learning by doing, working with real data and real research questions.\nWhen you want to go deeper:\n\nR for Data Science (2e): The definitive guide to the tidyverse\nTidyverse documentation: Reference docs for all packages\nRStudio Cheatsheets: Visual quick references for dplyr, ggplot2, and more\n\nThe tidyverse community is large and welcoming. When you get stuck, someone has likely asked your question before.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "chapter-1-4.html",
    "href": "chapter-1-4.html",
    "title": "LLMs and Data Science",
    "section": "",
    "text": "The (Responsible) Use of LLMs in Data Science\nAs we integrate large language models into data analysis, our responsibilities as researchers evolve in important ways . There are three primary approaches to this integration:\n\nfirst, using LLMs for code autocompletion and inline coding assistance, such as through integrated development environments like Positron;\nsecond, using LLMs to support the analysis process through complete code generation and supervised execution, such as with tools like Posit Databot; and\nthird, deploying LLMs to directly analyze qualitative (e.g., text data).\n\n\n\nThe Responsible Use Framework\nThe responsible use framework for LLMs in research, presented by Joe Cheng at R+AI Conference 2025 (Cheng2025LLMResponsible?) can be used as a practical evaluation tool centering on three essential criteria. Correctness refers to whether the LLM produces accurate and reliable results that can be verified and trusted. Transparency addresses whether the process and reasoning behind the AI’s outputs are visible and understandable to researchers, allowing them to inspect how conclusions were reached. Reproducibility concerns whether the same analysis can be repeated and yield consistent results, a foundational requirement of scientific research. These three categories align with broader responsible AI governance principles commonly found in organizational frameworks but are specifically tailored to the research context where verifiability and scientific rigor are key. For an LLM application in research to be considered responsibly used, it should ideally achieve “yes” answers across all three criteria, ensuring that the technology enhances rather than compromises research integrity.\n\n\nApplying the Resposible Use Framework\nWhen we examine the first two usage types (e.g., code autocomplete and code generation) through the responsible usage framework, we find more encouraging alignment with the three essential criteria of correctness, transparency, and reproducibility . Code-generating and code-assisting LLMs produce verifiable output that researchers can inspect, test, and validate before execution, ensuring correctness through human review. The process maintains transparency because the generated code itself is visible and interpretable, allowing researchers to understand exactly what analytical steps are being performed. Reproducibility is achieved because the same code can be run multiple times on the same data to yield consistent results, and the code can be shared alongside research findings.\nIn contrast, the third approach where LLMs directly analyze qualitative or text data within a black box that is the LLM may raise critical questions about research integrity. Using LLMs for this purpose present inherent challenges against these principles: they are notorious for generating convincing but incorrect answers, operate as black boxes with limited transparency, and lack reproducibility due to their non-deterministic nature. When we apply this framework to direct text analysis by LLMs, significant concerns may emerge: we cannot guarantee correctness, the process lacks transparency, and reproducibility remains uncertain at best.\n\n\nAchieving Responsible Usage Through Evidence-Based Results\nHowever, for the third type of usage where LLMs directly analyze data (as we do in Section X), we can work toward achieving “yes” answers across all (or as many as possible) three framework criteria by requiring LLMs to produce evidence-based results. As we cover in Section 6, it is possible to use a Local LLM that is connected to R/Positron. In this approach, the researcher has control over the data analysis over simply pasting text into an LLMs chatbox. For example, this can be done by structuring prompts to demand transparent, verifiable outputs, such as requiring the LLM to identify themes, provide verbatim quotes from the source data, report frequencies with counts and percentages, and present findings in standardized tabular formats. We transform the black box into a more transparent analytical tool. This approach ensures correctness by grounding every claim in specific textual evidence that researchers can verify, enhances transparency by making the analytical reasoning traceable through quoted examples and quantitative metrics, and improves reproducibility by standardizing the output format and maintaining clear documentation of the analytical process. When LLMs are constrained to cite their sources, quantify their observations, and structure their findings systematically, they shift from opaque pattern generators to accountable research assistants whose work can be validated against the original data. This methodology aligns with the recommendation for constrained use and micromanaged implementation, ensuring that LLMs remain within appropriate boundaries while still providing valuable analytical support for qualitative research.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LLMs and Data Science</span>"
    ]
  },
  {
    "objectID": "section-2-Intro.html",
    "href": "section-2-Intro.html",
    "title": "Computational Methods",
    "section": "",
    "text": "Overview\nThis section introduces the core computational methods that form the basis of educational data analysis. Across the next three chapters, you will learn how to analyze textual, relational, and numeric data—three major forms of information that appear in learning environments, institutional records, and educational research projects.\nThese chapters emphasize hands-on, transparent, and reproducible approaches using R. By engaging with real examples, you will gain practical experience in transforming raw educational data into interpretable results that inform theory and practice.",
    "crumbs": [
      "Computational Methods"
    ]
  },
  {
    "objectID": "section-2-Intro.html#the-analytical-scope-of-this-section",
    "href": "section-2-Intro.html#the-analytical-scope-of-this-section",
    "title": "Computational Methods",
    "section": "The Analytical Scope of This Section",
    "text": "The Analytical Scope of This Section\n\n\n\n\n\n\n\n\n\nChapter\nData Type\nAnalytical Focus\nExample Research Question\n\n\n\n\nChapter 2\nText Data (unstructured)\nNatural language processing, tokenization, sentiment, topic modeling\n“What themes emerge in student reflections or policy statements?”\n\n\nChapter 3\nRelational Data\nSocial network analysis: centrality, community detection, visualization\n“How do students or instructors connect and collaborate in learning networks?”\n\n\nChapter 4\nNumeric / Big Data\nStatistical modeling, regression, clustering, predictive analysis\n“Which factors best predict academic outcomes or engagement?”\n\n\n\n\n📚 These three perspectives together illustrate how computational techniques can capture different dimensions of learning—language, interaction, and measurement.",
    "crumbs": [
      "Computational Methods"
    ]
  },
  {
    "objectID": "section-2-Intro.html#setting-up-the-computational-environment",
    "href": "section-2-Intro.html#setting-up-the-computational-environment",
    "title": "Computational Methods",
    "section": "Setting Up the Computational Environment",
    "text": "Setting Up the Computational Environment\nBefore exploring the examples, ensure that your R environment contains the essential packages used throughout this section.\ninstall.packages(c(\n  # Core workflow and visualization\n  \"tidyverse\", \"ggplot2\", \"readr\", \"stringr\",\n\n  # Text analysis\n  \"tidytext\", \"quanteda\", \"textdata\",\n\n  # Network analysis\n  \"igraph\", \"ggraph\", \"tidygraph\",\n\n  # Numeric and machine learning tools\n  \"caret\", \"cluster\"\n))\nOptional visualization and interaction packages:\ninstall.packages(c(\"plotly\", \"RColorBrewer\", \"visNetwork\"))\n\n💡 Tip: Use a consistent project structure so each chapter builds on the same foundation:\nproject/\n├── data/        # datasets\n├── scripts/     # reusable code\n├── outputs/     # tables and processed files\n└── figures/     # charts and visualizations\nThis structure promotes reproducibility and helps keep analysis pipelines organized.",
    "crumbs": [
      "Computational Methods"
    ]
  },
  {
    "objectID": "section-2-Intro.html#a-general-computational-workflow",
    "href": "section-2-Intro.html#a-general-computational-workflow",
    "title": "Computational Methods",
    "section": "A General Computational Workflow",
    "text": "A General Computational Workflow\nRegardless of data type, the analytical logic follows a similar cycle:\n\nLoad data — read files from local or online sources.\nClean data — handle missing values, normalize text or numeric fields.\nTransform data — create tokens, build networks, or scale variables.\nAnalyze — apply the method appropriate to the data form.\nVisualize and interpret — generate plots and summaries to support interpretation.\n\nlibrary(tidyverse)\n\ndata &lt;- read_csv(\"data/example.csv\")\n\ncleaned &lt;- data |&gt;\n  mutate(across(everything(), str_squish)) |&gt;\n  drop_na()\n\nsummary(cleaned)\n\n🧩 This five-step workflow—load, clean, transform, analyze, interpret—appears throughout all chapters in this section.",
    "crumbs": [
      "Computational Methods"
    ]
  },
  {
    "objectID": "section-2-Intro.html#ethics-transparency-and-reproducibility",
    "href": "section-2-Intro.html#ethics-transparency-and-reproducibility",
    "title": "Computational Methods",
    "section": "Ethics, Transparency, and Reproducibility",
    "text": "Ethics, Transparency, and Reproducibility\nEducational data often include sensitive or identifiable information.\nResponsible computational research requires attention to both ethical and methodological rigor.\n\nPrivacy: Remove or anonymize all personal identifiers.\nTransparency: Keep analysis scripts in Quarto or R Markdown files for version control.\nReproducibility: Record package versions and parameters used in each analysis.\nInterpretability: Combine quantitative patterns with contextual educational insight.\n\n\n⚖️ Ethical and transparent practices ensure that computational results remain credible, interpretable, and usable for improving learning.",
    "crumbs": [
      "Computational Methods"
    ]
  },
  {
    "objectID": "section-2-Intro.html#transition-to-analytical-chapters",
    "href": "section-2-Intro.html#transition-to-analytical-chapters",
    "title": "Computational Methods",
    "section": "Transition to Analytical Chapters",
    "text": "Transition to Analytical Chapters\nWith the environment prepared and workflow established, you are ready to begin applying computational methods to real educational data:\n\nChapter 2 — Text Analysis: Working with unstructured language to uncover patterns in meaning and discourse.\nChapter 3 — Network Analysis: Examining connections and relationships among learners, instructors, or resources.\nChapter 4 — Numeric and Big Data: Exploring structured data to identify trends and predictors in education.\n\n\nThe following chapter begins with text data, illustrating how natural language processing can transform qualitative information into structured, interpretable results.",
    "crumbs": [
      "Computational Methods"
    ]
  },
  {
    "objectID": "chapter-2.html",
    "href": "chapter-2.html",
    "title": "Text Data",
    "section": "",
    "text": "2.1 Overview\nIn social sciences, analyzing text data is usually considered the “job” of qualitative researchers. Traditionally, qualitative research involves identifying patterns in non-numeric data, and this pattern recognition is typically done manually. This process is time-intensive but can yield rich research results. Traditional methods for analyzing text data involve human coding and can include direct (e.g., books, online texts) or indirect sources (e.g., interview transcripts).\nWith the advent of new software, we can capture and analyze text data in ways that were previously not possible, such as web scraping, accessing social media APIs, or downloading large online documents. Given the increased (opportunities for collecting and) size of text data, analysis now can benefit from computational approaches (e.g., dictionary-based, frequency-based, machine learning) that go beyond manual coding. As we discussed on the front matter of the book, these computational methods allow social scientists to ask new types of research questions, expanding the scope and depth of possible insights.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Text Data</span>"
    ]
  },
  {
    "objectID": "chapter-2.html#overview",
    "href": "chapter-2.html#overview",
    "title": "Text Data",
    "section": "",
    "text": "Disclaimer: While resources are available that discuss these analysis methods in depth, this book aims to provide a practical guide for social scientists, using data they will likely encounter. Our goal is to present a “cookbook” for guiding research projects through real-world examples.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Text Data</span>"
    ]
  },
  {
    "objectID": "chapter-2.html#accessing-text-data",
    "href": "chapter-2.html#accessing-text-data",
    "title": "Text Data",
    "section": "2.2 Accessing Text Data",
    "text": "2.2 Accessing Text Data\nNowadays, text can be found and collected in many different ways. For example, social media can serve as rich with text data (e.g., Reddit posts), likewise text created in classrooms, especially online (e.g., every student writing) can become part of text data. In this section, we will cover a few basic ways of accessing text data. Please note that although we cover some prominent ways, this is by no means an exhaustive list. Therefore, please refer to the additional resources section at the end of the section to dive deeper.\n\n2.2.1 Web Scraping (Unstructured or API)\n\nWhat is Web Scraping?\nWeb scraping refers to the automated process of extracting data from web pages. It is particularly useful when dealing with extensive lists of websites that would be tedious to mine manually. A typical web scraping program follows these steps:\n\nLoads a webpage.\nDownloads the HTML or XML structure.\nIdentifies the desired data.\nConverts the data into a format suitable for analysis, such as a data frame.\n\nIn addition to text, web scraping can also be used to download other content types, such as audio-visual files.\n\n\nIs Web Scraping Legal?\nWeb scraping was common in the early days of the internet, but with the increasing value of data, legal norms have evolved. To avoid legal issues, check the “Terms of Service” for specific permissions on the website, often accessible via “robots.txt” files. Consult legal advice when in doubt.\n\n\nReading a Web Page into R\nOnce permissions are confirmed, the first step in web scraping is to download the webpage’s source code into R, typically using the rvest package by Hadley Wickham.\n\n# Install and load the rvest package\ninstall.packages(\"rvest\")\nlibrary(rvest)\n\nTo demonstrate, we will scrape a simple Wikipedia page. Static pages, which lack interactive elements like JavaScript, are simpler to scrape. You can view the page’s HTML source in your browser by selecting Developer Tools &gt; View Source.\n\n# Load the webpage\nwikipedia_page &lt;- read_html(\"https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000\")\n\n# Verify that the webpage loaded successfully\nwikipedia_page\n\n\n\nParsing HTML\nThe next challenge is extracting specific information from the HTML structure. HTML files have a “tree-like” format, allowing us to target particular sections. Use your browser’s “Developer Tools” to inspect elements and locate the data. Right-click the desired element and select Inspect to view its structure.\nTo isolate data sections within the HTML structure, identify the XPath or CSS selectors. For instance:\n\n# Extract specific section using XPath\nsection_of_wikipedia &lt;- html_node(wikipedia_page, xpath='//*[@id=\"mw-content-text\"]/div/table')\nhead(section_of_wikipedia)\n\nTo convert the extracted section into a data frame, use html_table():\n\n# Convert the extracted data into a table\nhealth_rankings &lt;- html_table(section_of_wikipedia)\nhead(health_rankings[ , (1:2)])  # Display the first two columns\n\n\n\nParsing with CSS Selectors\nFor more complex web pages, CSS selectors can be an alternative to XPath. Tools like Selector Gadget can help identify the required CSS selectors.\nFor example, to scrape event information from Duke University’s main page:\n\n# Load the webpage\nduke_page &lt;- read_html(\"https://www.duke.edu\")\n\n# Extract event information using CSS selector\nduke_events &lt;- html_nodes(duke_page, css=\"li:nth-child(1) .epsilon\")\nhtml_text(duke_events)\n\n\n\nScraping with Selenium\nFor tasks involving interactive actions (e.g., filling search fields), use RSelenium, which enables automated browser operations.\nTo set up Selenium, install the Java SE Development Kit and Docker. Then, start Selenium in R:\n\n# Install and load RSelenium\ninstall.packages(\"RSelenium\")\nlibrary(RSelenium)\n\n# Start a Selenium session\nrD &lt;- rsDriver()\nremDr &lt;- rD$client\nremDr$navigate(\"https://www.duke.edu\")\n\nTo automate data entry, identify the CSS selector for the search box and input the query:\n\n# Find the search box element and enter a query\nsearch_box &lt;- remDr$findElement(using = 'css selector', 'fieldset input')\nsearch_box$sendKeysToElement(list(\"data science\", \"\\uE007\"))  # \"\\uE007\" represents Enter key\n\n\n\nWeb Scraping within a Loop\nTo scrape multiple pages, embed the code within a loop to automate tasks across different URLs. Since each site may have a unique structure, generalized scraping can be time-intensive and error-prone. Implement error handling to manage interruptions.\n\n\nWhen to Use Web Scraping\nWeb scraping is appropriate if:\n\nPage structure is consistent across sites: For example, a government site with date suffixes but a uniform layout.\nManual data collection is prohibitive: For extensive text or embedded tables.\n\nWhen feasible, consider alternatives like APIs or data-entry services (e.g., Amazon Mechanical Turk) for better efficiency and legal compliance.\n\n\nWhat is an API?\nAn Application Programming Interface (API) is a set of protocols that allows computers to communicate and exchange information. A common type is the REST API, where one machine sends a request, and another returns a response. APIs provide standardized access to data, services, and functionalities, making them essential in software development.\n\n\nWhen to Use an API\nAPIs are commonly used for:\n\nIntegrating with Third-Party Services: APIs connect applications to services like payment gateways or social media.\nAccessing Data: APIs retrieve data from systems or databases (e.g., real-time weather data).\nAutomating Tasks: APIs automate processes within applications, such as email marketing.\nBuilding New Applications: APIs allow developers to build new apps or services (e.g., a mapping API for navigation).\nStreamlining Workflows: APIs enable seamless communication and data exchange across systems.\n\n\n\nUsing Reddit API with RedditExtractoR\nReddit is a social media platform featuring a complex network of users and discussions, organized into “subreddits” by topic. RedditExtractoR, an R package, enables data extraction from Reddit to identify trends and analyze interactions.\n\n# Install and load RedditExtractoR\ninstall.packages(\"RedditExtractoR\")\nlibrary(RedditExtractoR)\n\n# Access data from the GenAI subreddit\nGenAI_reddit &lt;- find_thread_urls(subreddit = \"GenAI\", sort_by = \"new\", period = \"day\")\nview(GenAI_reddit)\n\n\n\n\n2.2.2 Audio Transcripts (Zoom, etc.)\nAudio transcripts are a rich source of text data, especially useful for capturing spoken content from meetings, interviews, or webinars. Many platforms, such as Zoom, provide automated transcription services that can be downloaded as text files for analysis. By processing these transcripts, researchers can analyze conversation themes, sentiment, or other linguistic features. Here’s how to access and prepare Zoom transcripts for analysis in R.\n\nKey Steps for Extracting Text Data from Audio Transcripts\n\nAccess the Zoom Transcript\n\nLog in to your Zoom account.\nNavigate to the “Recordings” section.\nSelect the recording you wish to analyze and download the “Audio Transcript” file.\n\nImport the Transcript into R\nOnce the file is downloaded, you can load it into R for analysis. Depending on the file format (usually a .txt file with tab or comma delimiters), use read.table(), read.csv(), or functions from the readr package to load the data.\n\n\n   # Load the transcript into R\n   transcript_data &lt;- read.table(\"path/to/your/zoom_transcript.txt\", sep = \"\\t\", header = TRUE)\n\nAdjust the sep parameter based on the delimiter used in the transcript file (typically \\t for tab-delimited files).\n\nData Cleaning (if necessary)\nClean up the text data to remove unnecessary characters, standardize formatting, and prepare it for further analysis.\n\nRemove Unwanted Characters\nUse gsub() to eliminate special characters and punctuation, keeping only alphanumeric characters and spaces.\n\n\n     # Remove special characters\n     transcript_data$text &lt;- gsub(\"[^a-zA-Z0-9 ]\", \"\", transcript_data$text)\n\n\nConvert Text to Lowercase\nStandardize text to lowercase for consistency in text analysis.\n\n\n\n     # Convert text to lowercase\n     transcript_data$text &lt;- tolower(transcript_data$text)\n\n\n\n\n2.2.3 PDF\nPDF files are a valuable source of text data, often found in research publications, government documents, and industry reports. We’ll explore two main methods for extracting text from PDFs:\n\nExtracting from Local PDF Files: This method involves accessing and parsing text from PDF files stored locally, providing tools and techniques to efficiently retrieve text data from offline documents.\nDownloading and Extracting PDF Files: This approach covers downloading PDFs from online sources and extracting their text. This method is useful for scraping publicly available documents from websites or databases for research purposes.\nPDF Data Extractor (PDE)\nFor more advanced PDF text extraction and processing, you can use the PDF Data Extractor (PDE) package. This package provides tools for extracting text data from complex PDF documents, supporting additional customization options for text extraction. PDE is a R package that easily extracts information and tables from PDF files. The PDE_analyzer_i() performs the sentence and table extraction while the included PDE_reader_i() allows the user-friendly visualization and quick-processing of the obtained results.\n\n\nSteps for Extracting Text from Local PDF Files\n\nInstall and Load the pdftools Package\nStart by installing and loading the pdftools package, which is specifically designed for reading and extracting text from PDF files in R.\n\n\n   install.packages(\"pdftools\")\n   library(pdftools)\n\n\nRead the PDF as a Text File\nUse the pdf_text() function to read the PDF file into R as a text object. This function returns each page as a separate string in a character vector.\n\n\n   txt &lt;- pdf_text(\"path/to/your/file.pdf\")\n\n\nExtract Text from a Specific Page\nTo access a particular page from the PDF, specify the page number in the text vector. For example, to extract text from page 24:\n\n\n   page_text &lt;- txt[24]  # page 24\n\n\nExtract Rows into a List\nIf the page contains a table or structured text, use the scan() function to read each row as a separate element in a list. The textConnection() function converts the page text for processing.\n\n\n   rows &lt;- scan(textConnection(page_text), what = \"character\", sep = \"\\n\")\n\n\nSplit Rows into Cells\nTo further parse each row, split it into cells by specifying the delimiter, such as whitespace (using \"\\\\s+\"). This converts each row into a list of individual cells.\n\n\n   row &lt;- unlist(strsplit(rows[24], \"\\\\s+\"))  # Example with the 24th row\n\n\n\nSteps for Downloading and Extracting Text from PDF Files\n\nDownload the PDF from the Web\nUse the download.file() function to download the PDF file from a specified URL. Set the mode to \"wb\" (write binary) to ensure the file is saved correctly.\n\n\n   link &lt;- paste0(\n     \"http://www.singstat.gov.sg/docs/\",\n     \"default-source/default-document-library/\",\n     \"publications/publications_and_papers/\",\n     \"cop2010/census_2010_release3/\",\n     \"cop2010sr3.pdf\"\n   )\n   download.file(link, \"census2010_3.pdf\", mode = \"wb\")\n\n\nRead the PDF as a Text File\nAfter downloading, read the PDF into R as a text object using the pdf_text() function from the pdftools package. Each page of the PDF will be stored as a string in a character vector.\n\n\n   txt &lt;- pdf_text(\"census2010_3.pdf\")\n\n\nExtract Text from a Specific Page\nAccess the desired page (e.g., page 24) by specifying the page number in the character vector.\n\n\n   page_text &lt;- txt[24]  # Page 24\n\n\nExtract Rows into a List\nUse the scan() function to split the page text into rows, with each row representing a line of text in the PDF. This creates a list where each line from the page is an element.\n\n\n   rows &lt;- scan(textConnection(page_text), what = \"character\", sep = \"\\n\")\n\n\nLoop Through Rows and Extract Data\nStarting from a specific row (e.g., row 7), loop over each row. For each row:\n\nSplit the text by spaces (\"\\\\s+\") using strsplit().\nConvert the result to a vector with unlist().\nIf the third cell in the row is not empty, store the second cell as name and the third cell as total, converting it to a numeric format after removing commas.\n\n\n\n   name &lt;- c()\n   total &lt;- c()\n\n   for (i in 7:length(rows)) {\n     row &lt;- unlist(strsplit(rows[i], \"\\\\s+\"))\n     if (!is.na(row[3])) {\n       name &lt;- c(name, row[2])\n       total &lt;- c(total, as.numeric(gsub(\",\", \"\", row[3])))\n     }\n   }\n\n\n\n\n2.2.4 Survey, Discussions, etc.\nSurveys and discussion posts are valuable sources of text data in social science research, providing insights into participants’ perspectives, opinions, and experiences. These data sources often come from open-ended survey responses, online discussion boards, or educational platforms. Extracting and preparing text data from these sources can reveal recurring themes, sentiment, and other patterns that support both quantitative and qualitative analysis. Below are key steps and code examples for processing text data from surveys and discussions in R.\n\nKey Steps for Processing Survey and Discussion Text Data\n\nLoad the Data\nSurvey and discussion data are typically stored in spreadsheet formats like CSV. Begin by loading this data into R for processing. Here, readr is used for reading CSV files with read_csv().\n\n\n   # Install and load necessary packages\n   install.packages(\"readr\")\n   library(readr)\n   \n   # Load data\n   survey_data &lt;- read_csv(\"path/to/your/survey_data.csv\")\n\n\nExtract Text Columns\nIdentify and isolate the relevant text columns. For example, if the text data is in a column named “Response,” you can create a new vector for analysis.\n\n\n   # Extract text data from the specified column\n   text_data &lt;- survey_data$Response\n\n\nData Cleaning\nPrepare the text data by cleaning it, removing any unnecessary characters, and standardizing the text. This includes removing punctuation, converting text to lowercase, and handling extra whitespace.\n\nRemove Unwanted Characters\nUse gsub() from base R to remove any non-alphanumeric characters, retaining only words and spaces.\n\n\n\n     # Remove special characters\n     text_data &lt;- gsub(\"[^a-zA-Z0-9 ]\", \"\", text_data)\n\n\nConvert to Lowercase\nStandardize the text by converting all characters to lowercase.\n\n\n     # Convert text to lowercase\n     text_data &lt;- tolower(text_data)\n\n\nRemove Extra Whitespace\nRemove any extra whitespace that may be left after cleaning.\n\n\n     # Remove extra spaces\n     text_data &lt;- gsub(\"\\\\s+\", \" \", text_data)\n\n\nTokenization and Word Counting (Optional)\nIf further analysis is needed, such as frequency-based analysis, split the text into individual words (tokenization) or count the occurrence of specific words. Here, dplyr is used to organize the word counts.\n\n\n   # Install and load necessary packages\n   install.packages(\"dplyr\")\n   library(dplyr)\n   \n   # Tokenize and count words\n   word_count &lt;- strsplit(text_data, \" \") %&gt;%\n                 unlist() %&gt;%\n                 table() %&gt;%\n                 as.data.frame()",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Text Data</span>"
    ]
  },
  {
    "objectID": "chapter-2.html#frequency-based-analysis",
    "href": "chapter-2.html#frequency-based-analysis",
    "title": "Text Data",
    "section": "2.3 Frequency-based Analysis",
    "text": "2.3 Frequency-based Analysis\nIn the following section, we will provide a “recipe” for the social scientist interested in these new methods of analyzing text data to get you from the initial stages of getting the data to running the analyses and the write up. Often left out is also a research question that suits or requires a method. Since we have a data and method-centric approach here, we will backtrack and also provide a research question, so that you can model after it in your own work. Finally, we will provide a sample results and discussions section.\n\n2.3.1 Purpose\nThe purpose of the frequency-based approach is to count the number of words as they appear in a text file, whether it is a collection of tweets, documents, or interview transcripts. This approach aligns with the frequency-coding method (e.g., Saldaña ,2021) and can supplement human coding by revealing the most/least commonly occurring words, which can then be compared across dependent variables.\n\nCase Study: Frequency-Based Analysis of GenAI USage Guidelines in Higher Education\nAs AI writing tools like ChatGPT become more prevalent, educators are working to understand how best to integrate them within academic settings, while many students and instructors remain uncertain about acceptable use cases. Our research into AI usage guidelines from the top 100 universities in North America aims to identify prominent themes and concerns in institutional policies regarding GenAI.\n\n\n\n2.3.2 Sample Research Questions\nTo investigate the nature of AI use policies within higher education institutions, in this study, our research questions are:\n\nRQ1: What are the most frequently mentioned words in university GenAI writing usage policies?\nRQ2: Which keywords reflect common concerns or focal points related to GenAI writing usage in academic settings?\n\n\n\n2.3.3 Sample Methods\n\nData Source\nThe dataset consists of publicly available AI policy texts from 100 universities(USA), the data has been downloaded and saved as a CSV file for analysis. –we might have to write more here to model how a research should be describing the data from its acquisition to use for research.\n\n\nData Analysis\nIn order to analyze the data we used xyz, —-let’s provide a sample write up for the researcher to adapt.\n\n\n\n\n\n\n2.3.4 Analysis\n\nStep 1: Load Required Libraries\nInstall and load libraries for data processing, visualization, and word cloud generation.\n\n# Install necessary packages if not already installed\n#install.packages(c(\"tibble\", \"dplyr\", \"tidytext\", \"ggplot2\", \"viridis\",\"tm\",wordcloud\" \"wordcloud2\",\"webshot\"))\n\n# Load libraries\nlibrary(readr)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(tm)\nlibrary(wordcloud)\nlibrary(wordcloud2)\nlibrary(webshot)\n\n\n\nStep 2: Load Data\nRead the CSV file containing policy texts from the top 100 universities.\n\n# Load the dataset\nuniversity_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\")\n\n\n\nStep 3: Tokenize Text and Count Word Frequency\nProcess the text data by tokenizing words, removing stop words, and counting word occurrences.\n\n# Tokenize text, remove stop words, and count word frequencies\nword_frequency &lt;- university_policies %&gt;%\n  unnest_tokens(word, Stance) %&gt;%  # Tokenize the 'Policy_Text' column\n  anti_join(stop_words) %&gt;%             # Remove common stop words\n  count(word, sort = TRUE)              # Count and sort words by frequency\nword_frequency\n\n# A tibble: 1,108 × 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 ai            124\n 2 students      120\n 3 chatgpt        80\n 4 tools          73\n 5 academic       68\n 6 policy         35\n 7 assignments    34\n 8 instructors    34\n 9 integrity      33\n10 student        32\n# ℹ 1,098 more rows\n\n\n\n\nStep 4: Create a Word Cloud\nGenerate a word cloud to visualize the frequency of words in a circular shape.\n\n# Create and display the GenAI usage Stance wordcloud\n\nwordcloud(words = word_frequency$word, freq = word_frequency$n, scale = c(4, 0.5), random.order = FALSE, min.freq = 10, colors = brewer.pal(8, \"Dark2\"), rot.per = 0.35)\n\n\n\n\n\n\n\n\n\n\nStep 5: Visualize Top 12 Words in University Policies\nSelect the top 12 most frequent words and create a bar chart to visualize the distribution.\n\n# Select the top 12 words\ntop_words &lt;- word_frequency %&gt;% slice(1:12)\n\n# Generate the bar chart\npolicy_word_chart &lt;- ggplot(top_words, aes(x = reorder(word, n), y = n, fill = word)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(\n    title = \"Top Words in University GenAI Policies\",\n    x = NULL,\n    y = \"Frequency\",\n    caption = \"Source: University AI Policy Text\",\n    fill = \"Word\"\n  ) +\n  scale_fill_viridis(discrete = TRUE) +\n  geom_text(aes(label = n), vjust = 0.5, hjust = -0.1, size = 3)\n\n# Print the bar chart\nprint(policy_word_chart)\n\n\n\n\n\n\n\n\n\n\n\n2.3.5 Results and Discussions\n\nRQ1: What are the most frequently mentioned words in university GenAI writing usage policies?\nThe results of the frequency analysis showed that keywords such as “assignment,” “student,” and “writing” were among the most commonly mentioned terms across AI policies at 100 universities. This emphasis reflects a focus on using AI tools to support student learning and enhance teaching content. The frequent mention of these words suggests that institutions are considering the role of AI in academic assignments and course design, indicating a strategic commitment to integrating AI within educational tasks and student interactions.\nRQ2: Which keywords reflect common concerns or focal points related to GenAI writing usage in academic settings?\nThe analysis of the top 12 frequently mentioned terms highlighted additional focal points, including “tool,” “academic,” “instructor,” “integrity,” and “expectations.” These terms reveal concerns around the ethical use of AI tools, the need for clarity in academic applications, and the central role of instructors in AI policy implementation. Keywords like “integrity” and “expectations” emphasize the importance of maintaining academic standards and setting clear guidelines for AI use in classrooms, while “instructor” underscores the influence faculty members have in shaping AI-related practices. Together, these terms reflect a commitment to transparent policies that support ethical and effective AI integration, enhancing the academic experience for students.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Text Data</span>"
    ]
  },
  {
    "objectID": "chapter-2.html#dictionary-based-analysis",
    "href": "chapter-2.html#dictionary-based-analysis",
    "title": "Text Data",
    "section": "2.4 Dictionary-based Analysis",
    "text": "2.4 Dictionary-based Analysis\n\n2.4.1 Purpose\nThe purpose of dictionary-based analysis in text data is to assess the presence of predefined categories, like emotions or sentiments, within the text using lexicons or dictionaries. This approach allows researchers to quantify qualitative aspects, such as positive or negative sentiment, based on specific words that correspond to these categories.\n\nCase:\nIn this analysis, we examine the stance of 100 universities on the use of GenAI by applying the Bing sentiment dictionary. By analyzing sentiment scores, we aim to identify the general tone in these policies, indicating whether the institutions’ attitudes toward GenAI are predominantly positive or negative.\n\n\n\n2.4.2 Sample Research Questions\n\nRQ: What is the dominant sentiment expressed in GenAI policy texts across universities, and is it primarily positive or negative?\n\n\n\n2.4.3 Analysis\n\nStep 1: Install and Load Necessary Libraries\nFirst, install and load the required packages for text processing and visualization.\n\n# Install necessary packages if not already installed\n#install.packages(c(\"tidytext\",\"tidyverse\" \"dplyr\", \"ggplot2\", \"tidyr\"))\n\n# Load libraries\nlibrary(tidytext)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\n\nStep 2: Load and Prepare Data(same as 2.3)\nLoad the GenAI policy stance data from a CSV file. Be sure to update the file path as needed. we use the same data as 2.3.\n\n# Load the dataset (replace \"University_GenAI_Policy_Stance.csv\" with the actual file path)\nuniversity_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\")\n# Tokenize text, remove stop words, and count word frequencies\nword_frequency &lt;- university_policies %&gt;%\n  unnest_tokens(word, Stance) %&gt;%  # Tokenize the 'Policy_Text' column\n  anti_join(stop_words) %&gt;%             # Remove common stop words\n  count(word, sort = TRUE)              # Count and sort words by frequency\nword_frequency\n\n\n\nStep 3: Tokenize Text Data and Apply Sentiment Dictionary\nTokenize the policy text data to separate individual words. Then, use the Bing sentiment dictionary to label each word as positive or negative.\n\n# Tokenize 'Stance' column and apply Bing sentiment dictionary\nsentiment_scores &lt;- word_frequency %&gt;%\n  inner_join(get_sentiments(\"bing\")) %&gt;% # Join with Bing sentiment lexicon\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%\n  mutate(sentiment_score = positive - negative) # Calculate net sentiment score\n\nsentiment_scores\n\n# A tibble: 142 × 4\n   word         positive negative sentiment_score\n   &lt;chr&gt;           &lt;int&gt;    &lt;int&gt;           &lt;int&gt;\n 1 honor              18        0              18\n 2 cheating            0       11             -11\n 3 dishonesty          0       10             -10\n 4 guidance            9        0               9\n 5 honesty             7        0               7\n 6 intelligence        7        0               7\n 7 transparent         7        0               7\n 8 violation           0        7              -7\n 9 encourage           6        0               6\n10 difficult           0        5              -5\n# ℹ 132 more rows\n\n\n\n\nStep 4: Create a Density Plot for Sentiment Distribution\nVisualize the distribution of sentiment scores with a density plot, showing the prevalence of positive and negative sentiments across university policies.\n\n# Generate a density plot of sentiment scores\ndensity_plot &lt;- ggplot(sentiment_scores, aes(x = sentiment_score, fill = sentiment_score &gt; 0)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"red\", \"green\"), name = \"Sentiment\",\n                    labels = c(\"Negative\", \"Positive\")) +\n  labs(\n    title = \"Density Plot of University AI Policy Sentiment\",\n    x = \"Sentiment Score\",\n    y = \"Density\",\n    caption = \"Source: University Policy Text\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 20),\n    axis.text = element_text(size = 14),\n    axis.title = element_text(face = \"bold\", size = 16),\n    plot.caption = element_text(size = 12)\n  )\n\n# Print the plot\nprint(density_plot)\n\n\n\n\n\n\n\n\n\n\n\n2.4.4 Results and Discussions\n\nRQ: What is the dominant sentiment expressed in GenAI policy texts across universities, and is it primarily positive or negative?\nThe dictionary-based sentiment analysis reveals the prevailing sentiments in university policies on GenAI usage. Using the Bing lexicon to assign positive and negative scores, the density plot illustrates the distribution of sentiment scores across the 100 institutions.\nThe results indicate a balanced perspective with a slight tendency toward positive sentiment, as reflected by a higher density of positive scores. This analysis provides insights into the varying degrees of acceptance and caution universities adopt in their AI policy frameworks, demonstrating the diverse stances that shape institutional AI guidelines.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Text Data</span>"
    ]
  },
  {
    "objectID": "chapter-2.html#clustering-based-analysis",
    "href": "chapter-2.html#clustering-based-analysis",
    "title": "Text Data",
    "section": "2.5 Clustering-Based Analysis",
    "text": "2.5 Clustering-Based Analysis\nClustering-based analysis involves grouping similar text documents or text segments into clusters based on their underlying topics or themes. This approach is particularly useful for identifying dominant themes in text data, such as university AI policy documents.\n\n2.5.1 Purpose\nPurpose: The goal of clustering-based analysis is to uncover latent themes in text data using unsupervised machine learning techniques. Topic modeling is one popular method for clustering documents into groups based on their content.\nCase: Using the GenAI policy texts from 100 universities, we apply Latent Dirichlet\nAllocation (LDA) to identify dominant themes in these policy documents. This analysis will help categorize policies into overarching themes, such as academic integrity, student support, and instructor discretion.\n\n\n2.5.2 Sample Research Questions\n\nRQ1: What are the prominent themes present in university policies regarding GenAI usage Stance?\nRQ2: How do these themes reflect the key concerns or opportunities for intergrating GenAI in higher education?\n\n\n\n2.5.3 Analysis\n\nStep 1: Install and Load Necessary Libraries\nInstall and load the required libraries for text processing and topic modeling.\n\n# Install necessary packages\n#install.packages(c(\"dplyr\", \"tidytext\", \"topicmodels\", \"ggplot2\"))\n\n# Load libraries\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(topicmodels)\nlibrary(ggplot2)\n\n\n\nStep 2: Prepare the Data\nLoad the data and create a document-term matrix (DTM) for topic modeling.\n\n# Load the dataset\nuniversity_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\")\n\n# Tokenize text data and count word frequencies\nword_frequency &lt;- university_policies %&gt;%\n  unnest_tokens(word, Stance) %&gt;%  # same as section 2.3\n  anti_join(stop_words) %&gt;%             # Remove common stop words\n  count(word, sort = TRUE)              # Count and sort words by frequency\nword_frequency\n\n# A tibble: 1,108 × 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 ai            124\n 2 students      120\n 3 chatgpt        80\n 4 tools          73\n 5 academic       68\n 6 policy         35\n 7 assignments    34\n 8 instructors    34\n 9 integrity      33\n10 student        32\n# ℹ 1,098 more rows\n\n# Creating Documents - Word Frequency Matrix\ngpt_dtm &lt;- word_frequency %&gt;%\n  group_by(word) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  ungroup() %&gt;%\n  cast_dtm(document = \"id\", term = \"word\", value = \"n\")\n\n\nlibrary(topicmodels)\nlibrary(ggplot2)\n\n# Define range of k values\nk_values &lt;- c(2, 3, 4, 5)\n\n# Initialize a data frame to store perplexities\nperplexities &lt;- data.frame(k = integer(), perplexity = numeric())\n\n# Calculate perplexity for each k\nfor (k in k_values) {\n  lda_model &lt;- LDA(gpt_dtm, k = k, control = list(seed = 1234))  # Fit LDA model\n  perplexity_score &lt;- perplexity(lda_model, gpt_dtm)            # Calculate perplexity\n  perplexities &lt;- rbind(perplexities, data.frame(k = k, perplexity = perplexity_score))\n}\n\n# Plot perplexity vs number of topics\nggplot(perplexities, aes(x = k, y = perplexity)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Perplexity vs Number of Topics\",\n    x = \"Number of Topics (k)\",\n    y = \"Perplexity\"\n  ) +\n  theme_minimal()\n\n\n\nStep 3: Fit the LDA Model\nFit an LDA model with k = 3 topics.\n\n# Converting document-word frequency matrices to sparse matrices\ngpt_dtm_sparse&lt;- as(gpt_dtm, \"matrix\")\n\n# Fit the LDA model\nlda_model &lt;- LDA(gpt_dtm_sparse, k = 3, control = list(seed = 1234))\n\n# View model results\ngpt_policy_topics_k3 &lt;- tidy(lda_model, matrix = \"beta\")\n\nprint(gpt_policy_topics_k3)\n\n# A tibble: 3,324 × 3\n   topic term        beta\n   &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1     1 ai       0.0408 \n 2     2 ai       0.00593\n 3     3 ai       0.0650 \n 4     1 students 0.0435 \n 5     2 students 0.0635 \n 6     3 students 0.0151 \n 7     1 chatgpt  0.0396 \n 8     2 chatgpt  0.0248 \n 9     3 chatgpt  0.0126 \n10     1 tools    0.0197 \n# ℹ 3,314 more rows\n\n\n\n\nStep 4: Visualize Topics\nExtract the top terms for each topic and visualize them.\n\n# Visualizing top terms for each topic\ngpt_policy_ap_top_terms_k3 &lt;- gpt_policy_topics_k3 %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\ngpt_policy_ap_top_terms_k3 %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\n\n\n\n\n\n\n2.5.4 Results and Discussions\n\nResearch Question 1:\nWhat are the prominent themes present in university policies regarding GenAI usage?\nAnswer:\nThe topic modeling analysis revealed three distinct themes in the university GenAI policies:\n\nTheme 1: Student-Centric Guidelines and Ethical Considerations\n\nKey terms: students, integrity, tools, instructors, assignment\nThis theme emphasizes student usage of GenAI in academic settings, with a focus on ethics (integrity) and guidelines for instructors to manage assignments involving AI tools.\n\nTheme 2: Academic Standards and Faculty Expectations\n\nKey terms: students, academic, faculty, honor, expectations\nThis theme focuses on maintaining academic integrity and clarifying expectations for faculty and students regarding GenAI usage in assignments and assessments.\n\nTheme 3: Policy-Level Governance and Technology Integration\n\nKey terms: ai, tools, policy, learning, generative\nThis theme revolves around institutional policies on AI integration, highlighting broader governance strategies and how generative AI (like GenAI) fits into learning environments.\n\n\n\n\nResearch Question 2:\nHow do these themes reflect the key concerns or opportunities for integrating GenAI in higher education?\nAnswer:\nThe identified themes reflect both concerns and opportunities:\n\nConcerns:\nTheme 1: Highlights the ethical challenges, such as ensuring academic integrity when students use AI tools in their coursework. Institutions are keen on setting clear guidelines for both students and instructors to avoid misuse.\nTheme 2: Underlines the potential for conflict between maintaining academic standards (honor, expectations) and leveraging AI to support learning. This shows a cautious approach to integrating AI while upholding traditional values.\nTheme 3: Raises policy-level questions on AI governance, such as whether existing institutional frameworks are adequate to regulate emerging generative AI technologies.\nOpportunities:\nTheme 1: Presents a chance to redefine how students interact with AI tools to foster responsible and innovative usage, particularly for assignments and creative tasks.\nTheme 2: Encourages collaboration between faculty and administration to develop robust expectations and support systems for integrating AI in the classroom.\nTheme 3: Offers a strategic opportunity for universities to lead in AI adoption by establishing comprehensive policies that guide AI’s role in education and research.\n\n\n\nDiscussion:\nThe topic modeling results suggest that universities are navigating a complex landscape of opportunities and challenges as they incorporate GenAI into academic contexts. While student-centric policies aim to balance innovation with ethical considerations, institutional-level themes signal the need for governance frameworks to ensure responsible AI use. These findings indicate that higher education institutions are positioned to play a pivotal role in shaping the future of generative AI in learning, provided they address the ethical, pedagogical, and policy challenges identified in this analysis.\n\n\nReferences:\nSaldaña, J. (2021). Coding techniques for quantitative and mixed data. The Routledge reviewer’s guide to mixed methods analysis, 151-160.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Text Data</span>"
    ]
  },
  {
    "objectID": "chapter-3.html",
    "href": "chapter-3.html",
    "title": "Networks Data",
    "section": "",
    "text": "3.1 Overview\nSocial network analysis (SNA) is the process of investigating social structures through the use of networks and graph theory. It is a technique used to map and measure relationships and flows between people, groups, organizations, computers, or other information/knowledge processing entities. SNA can be a useful tool for understanding the team structures, for example, in an online classroom. It can be an additional layer of understanding the outcomes (or predictors) of certain instructional interventions. Used this way SNA can be used to identify patterns and trends in social networks, as well as to understand how these networks operate. Additionally, SNA can be used to predict future behavior in social networks, and to design interventions that aim to improve the functioning of these networks.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Networks Data</span>"
    ]
  },
  {
    "objectID": "chapter-3.html#accessing-sna-data",
    "href": "chapter-3.html#accessing-sna-data",
    "title": "Networks Data",
    "section": "3.2 Accessing SNA Data",
    "text": "3.2 Accessing SNA Data\nSocial Network Analysis (SNA) relies on relational data—information about connections (edges) between entities (nodes) such as students, teachers, or organizations. Compared to traditional survey or tabular data, SNA requires pairwise relational information. In education, this could include “who collaborates with whom,” “who talks to whom,” or digital traces of discussion and collaboration in online platforms.\n\n3.2.1 Types and Sources of SNA Data\nThere are several common sources and structures for SNA data in educational and social science contexts:\n\nSurvey-based Network Data: Collected via roster or name generator questions, e.g., “List the classmates you discuss assignments with.”\nBehavioral/Observational Data: Derived from logs of actual interactions, e.g., forum replies, emails, classroom seating.\nArchival or Digital Trace Data: Extracted from digital platforms such as MOOCs, LMS discussion forums, Slack, Twitter, or Facebook.\nAdministrative/Organizational Data: Information about formal structures such as team membership or co-authorship.\n\nData Structure: Most SNA data are formatted as: - Edge List (two columns: source and target) - Adjacency Matrix (rows and columns are actors; cell values indicate a tie) - Node Attributes (supplementary information about each node, e.g., gender, role)\n\n\n3.2.2 Example 1: Creating a Simple Network from an Edge List\nBelow is an example of constructing a network from a simple CSV edge list. This mirrors typical classroom survey data (“who do you consider your friend in this class?”).\n\n# Install and load the igraph package\ninstall.packages(\"igraph\")\nlibrary(igraph)\n\n# Example: Load an edge list from CSV\nedge_list &lt;- read.csv(\"data/friendship_edges.csv\")\n\n# Create the graph object (directed network)\ng &lt;- graph_from_data_frame(edge_list, directed = TRUE)\n\n# Plot the network\nplot(g, main = \"Friendship Network\")\n\n\n\n3.2.3 Example 2: Generating Network Data from Digital Traces\nMany educational datasets now come from online discussion forums, MOOCs, or LMS systems. For example, the MOOC case study (Kellogg & Edelmann, 2015) uses reply relationships in online courses to construct discussion networks.\n# Suppose you have a data frame with columns: from_user, to_user\nmooc_edges &lt;- read.csv(\"data/mooc_discussion_edges.csv\")\ng_mooc &lt;- graph_from_data_frame(mooc_edges, directed = TRUE)\nplot(g_mooc, main = \"MOOC Discussion Network\")\n\n\n3.2.4 Example 3: Collecting SNA Data via Surveys\nIf you want to collect your own network data:\n\nAsk participants to name or select (from a roster) their friends, collaborators, or contacts.\nCompile responses into an edge list.\nExample survey prompt:\n\n“Please list up to five classmates you seek help from most frequently.”\n\n\nTip:\nSurvey-based SNA is easier to manage with small to medium groups. For larger networks, digital trace or archival data may be more practical.\n\n\n3.2.5 Node Attribute Data\nYou can also load additional data about each node (student, teacher, etc.) to enable richer analyses (e.g., centrality by gender or role).\nnode_attributes &lt;- read.csv(\"data/friendship_nodes.csv\")\n# Add attributes to igraph object\nV(g)$gender &lt;- node_attributes$gender[match(V(g)$name, node_attributes$name)]\n\n\n3.2.6 Further Examples\n\nPublic Datasets:\n\nMOOC Discussion Networks\nAdd Health\nCommon Core Twitter Networks (Supovitz et al.)\n\nSynthetic Data:\n\nR’s igraph package can also generate sample networks for practice:\ng_sample &lt;- sample_gnp(n = 10, p = 0.3)\nplot(g_sample, main = \"Random Sample Network\")\n\n\n\n\n3.2.7 Best Practices and Tips\n\nEthics: Social network data can be sensitive. Protect anonymity and comply with IRB/data use guidelines.\nFormat Consistency: Always clarify whether ties are directed/undirected, binary/weighted, and ensure consistent formatting.\nMissing Data: Especially in survey-based SNA, missing responses can impact network structure and interpretation.\n\n\n\n3.2.8 Summary\nAccessing SNA data involves both careful design (in the case of surveys/observations) and extraction/wrangling (in the case of digital traces or archival records). The choice of data source and structure will directly influence the kinds of questions you can answer with SNA.\n\nRecommended Reading:\n\nBorgatti, S. P., Everett, M. G., & Johnson, J. C. (2018). Analyzing Social Networks (2nd ed). SAGE.\nKellogg, S., & Edelmann, A. (2015). Massive open online course discussion forums as networks.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Networks Data</span>"
    ]
  },
  {
    "objectID": "chapter-3.html#network-management-measurement-in-social-network-analysis",
    "href": "chapter-3.html#network-management-measurement-in-social-network-analysis",
    "title": "Networks Data",
    "section": "3.3 Network Management & Measurement in Social Network Analysis",
    "text": "3.3 Network Management & Measurement in Social Network Analysis\n\n3.3.1 Purpose + Case\nPurpose: This section demonstrates how to manage, measure, and visualize large-scale discussion networks from online professional development settings. Through this real-world example, we guide readers in loading relational data, constructing a directed network, and conducting a suite of essential SNA measures. The focus is on classroom- and course-level online discussions, which are representative of many contemporary educational and research settings.\nCase Study: The case data comes from two cohorts of an online professional development program (“DLT1” and “DLT2”). Each cohort’s discussion data includes (a) edge list data capturing who replied to whom, and (b) node/actor attributes describing roles (e.g., facilitator, expert). These data allow us to reconstruct and analyze the full structure of communication in two authentic online learning communities.\n\n\n3.3.2 Sample Research Questions\n\nRQ1: What is the overall structure of interaction in each online discussion cohort? Are they densely connected, or fragmented?\nRQ2: Who are the most central or influential actors in the network? How do facilitators or experts compare with regular participants?\nRQ3: To what extent are ties reciprocated (mutual) and how cohesive are the networks?\nRQ4: How do the network properties (e.g., density, reciprocity, clustering) compare between cohorts?\n\n\n\n3.3.3 Analysis\n\nStep 1: Install and Load Required Packages\n\n# Install and load necessary libraries\n#install.packages(c(\"tidygraph\", \"ggraph\", \"readr\", \"janitor\"))\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(readr)\nlibrary(janitor)\nlibrary(igraph)\nlibrary(dplyr)\n\n\n\nStep 2: Import and Clean Data：Load Edges and Node Attributes for DLT1:\n\n# Load edge list (who replied to whom)\ndlt1_ties &lt;- read_csv(\"data/dlt1-edges.csv\", \n  col_types = cols(Sender = col_character(), \n                   Receiver = col_character(), \n                   `Category Text` = col_skip(), \n                   `Comment ID` = col_character(), \n                   `Discussion ID` = col_character())) |&gt; \n  clean_names()\n\n# Load node attributes (participant roles, etc.)\ndlt1_actors &lt;- read_csv(\"data/dlt1-nodes.csv\", \n  col_types = cols(UID = col_character(), \n                   Facilitator = col_character(), \n                   expert = col_character(), \n                   connect = col_character())) |&gt; \n  clean_names()\n\nhead(dlt1_ties)\n\n# A tibble: 6 × 9\n  sender receiver timestamp discussion_title discussion_category parent_category\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;               &lt;chr&gt;          \n1 360    444      4/4/13 1… Most important … Group N             Units 1-3 Disc…\n2 356    444      4/4/13 1… Most important … Group D-L           Units 1-3 Disc…\n3 356    444      4/4/13 1… DLT Resources—C… Group D-L           Units 1-3 Disc…\n4 344    444      4/4/13 1… Most important … Group O-T           Units 1-3 Disc…\n5 392    444      4/4/13 1… Most important … Group U-Z           Units 1-3 Disc…\n6 219    444      4/4/13 1… Most important … Group M             Units 1-3 Disc…\n# ℹ 3 more variables: discussion_identifier &lt;chr&gt;, comment_id &lt;chr&gt;,\n#   discussion_id &lt;chr&gt;\n\nhead(dlt1_actors)\n\n# A tibble: 6 × 13\n  uid   facilitator role1  experience experience2 grades location region country\n  &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  \n1 1     0           libme…          1 6 to 10     secon… VA       South  US     \n2 2     0           class…          1 6 to 10     secon… FL       South  US     \n3 3     0           distr…          2 11 to 20    gener… PA       North… US     \n4 4     0           class…          2 11 to 20    middle NC       South  US     \n5 5     0           other…          3 20+         gener… AL       South  US     \n6 6     0           class…          1 4 to 5      gener… AL       South  US     \n# ℹ 4 more variables: group &lt;chr&gt;, gender &lt;chr&gt;, expert &lt;chr&gt;, connect &lt;chr&gt;\n\n\n\n\nStep 3: Construct and Explore the Network\n\n# Build the directed network graph (nodes: uid, edges: sender-&gt;receiver)\ndlt1_network &lt;- tbl_graph(\n  edges = dlt1_ties,\n  nodes = dlt1_actors,\n  node_key = \"uid\",\n  directed = TRUE\n)\n\n# Overview of the network\ndlt1_network\n\n# A tbl_graph: 445 nodes and 2529 edges\n#\n# A directed multigraph with 4 components\n#\n# Node Data: 445 × 13 (active)\n   uid   facilitator role1 experience experience2 grades location region country\n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  \n 1 1     0           libm…          1 6 to 10     secon… VA       South  US     \n 2 2     0           clas…          1 6 to 10     secon… FL       South  US     \n 3 3     0           dist…          2 11 to 20    gener… PA       North… US     \n 4 4     0           clas…          2 11 to 20    middle NC       South  US     \n 5 5     0           othe…          3 20+         gener… AL       South  US     \n 6 6     0           clas…          1 4 to 5      gener… AL       South  US     \n 7 7     0           inst…          2 11 to 20    gener… SD       Midwe… US     \n 8 8     0           spec…          1 6 to 10     secon… BE       Inter… BE     \n 9 9     0           clas…          1 6 to 10     middle NC       South  US     \n10 10    0           scho…          2 11 to 20    middle NC       South  US     \n# ℹ 435 more rows\n# ℹ 4 more variables: group &lt;chr&gt;, gender &lt;chr&gt;, expert &lt;chr&gt;, connect &lt;chr&gt;\n#\n# Edge Data: 2,529 × 9\n   from    to timestamp    discussion_title  discussion_category parent_category\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;             &lt;chr&gt;               &lt;chr&gt;          \n1   360   444 4/4/13 16:32 Most important c… Group N             Units 1-3 Disc…\n2   356   444 4/4/13 18:45 Most important c… Group D-L           Units 1-3 Disc…\n3   356   444 4/4/13 18:47 DLT Resources—Co… Group D-L           Units 1-3 Disc…\n# ℹ 2,526 more rows\n# ℹ 3 more variables: discussion_identifier &lt;chr&gt;, comment_id &lt;chr&gt;,\n#   discussion_id &lt;chr&gt;\n\n# Output: 445 nodes, 2529 edges, 4 components (directed multigraph)\n\n\n\nStep 4: Basic Visualization\n\n# Quick overview plot (stress layout by default)\nautograph(dlt1_network)\n\n\n\n\n\n\n\n# Custom visualization with colors and centrality\nggraph(dlt1_network, layout = \"fr\") +\n  geom_edge_link(alpha = .2) +\n  geom_node_point(aes(color = role1, size = local_size())) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\n\n\n\nStep 5: Network Size and Centralization\n\n# Number of nodes and edges\ngorder(dlt1_network)   # 445\n\n[1] 445\n\ngsize(dlt1_network)    # 2529\n\n[1] 2529\n\n# Degree centrality (all, in, out)\ndeg_all &lt;- centr_degree(dlt1_network, mode = \"all\")$res\ndeg_in  &lt;- centr_degree(dlt1_network, mode = \"in\")$res\ndeg_out &lt;- centr_degree(dlt1_network, mode = \"out\")$res\n# Centralization\ncentr_degree(dlt1_network, mode = \"all\")$centralization  # 0.64\n\n[1] 0.6429242\n\ncentr_degree(dlt1_network, mode = \"in\")$centralization   # 1.06\n\n[1] 1.05702\n\ncentr_degree(dlt1_network, mode = \"out\")$centralization  # 0.23\n\n[1] 0.2259389\n\n\n\n\nStep 6: Attach and Visualize Node Centrality\n\n# Add in-degree centrality as node attribute\ndlt1_network &lt;- dlt1_network |&gt;\n  activate(nodes) |&gt;\n  mutate(in_degree = centrality_degree(mode = \"in\"))\n\n# Plot, sizing nodes by in-degree\nggraph(dlt1_network) +\n  geom_edge_link(alpha = .2) +\n  geom_node_point(aes(size = in_degree, color = role1)) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\n\n\n\nStep 7: Network Density, Reciprocity, Clustering, Distance\n\n# Density\nedge_density(dlt1_network)      # 0.013 (sparse network)\n\n[1] 0.01279988\n\n# Reciprocity\nreciprocity(dlt1_network)       # 0.20 (20% of ties are reciprocated)\n\n[1] 0.1997544\n\n# Add reciprocated edge attribute and plot\ndlt1_network &lt;- dlt1_network |&gt;\n  activate(edges) |&gt;\n  mutate(reciprocated = edge_is_mutual())\nggraph(dlt1_network) +\n  geom_node_point(aes(size = in_degree)) +\n  geom_edge_link(aes(color = reciprocated), alpha = .2) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\n# Clustering (transitivity/global)\ntransitivity(dlt1_network)      # 0.089\n\n[1] 0.08880774\n\n# Network diameter (longest shortest path) & average distance\ndiameter(dlt1_network)          # 8\n\n[1] 8\n\nmean_distance(dlt1_network)     # 3.03\n\n[1] 3.030694\n\n\n\n\nStep 8:Repeat for DLT2\n\n# Step 8: Repeat for DLT2\n\n# 1. Load the DLT2 edge and node data\ndlt2_ties &lt;- read_csv(\"data/dlt2-edges.csv\", \n  col_types = cols(Sender = col_character(), \n                   Receiver = col_character(), \n                   `Category Text` = col_skip(), \n                   `Comment ID` = col_character(), \n                   `Discussion ID` = col_character())) |&gt; \n  clean_names()\n\ndlt2_actors &lt;- read_csv(\"data/dlt2-nodes.csv\", \n  col_types = cols(UID = col_character(), \n                   Facilitator = col_character(), \n                   expert = col_character(), \n                   connect = col_character())) |&gt; \n  clean_names()\n\n# 2. Construct the directed network\ndlt2_network &lt;- tbl_graph(\n  edges = dlt2_ties,\n  nodes = dlt2_actors,\n  node_key = \"uid\",\n  directed = TRUE\n)\n\n# 3. Basic network properties\nnum_nodes &lt;- gorder(dlt2_network)   # Number of nodes\nnum_edges &lt;- gsize(dlt2_network)    # Number of edges\n\n# 4. Degree centrality (overall, in, out)\ndeg_all &lt;- centr_degree(dlt2_network, mode = \"all\")$res\ndeg_in  &lt;- centr_degree(dlt2_network, mode = \"in\")$res\ndeg_out &lt;- centr_degree(dlt2_network, mode = \"out\")$res\n\n# Centralization values\ncentr_all &lt;- centr_degree(dlt2_network, mode = \"all\")$centralization\ncentr_in  &lt;- centr_degree(dlt2_network, mode = \"in\")$centralization\ncentr_out &lt;- centr_degree(dlt2_network, mode = \"out\")$centralization\n\n# 5. Attach centrality as a node attribute\ndlt2_network &lt;- dlt2_network |&gt;\n  activate(nodes) |&gt;\n  mutate(in_degree = centrality_degree(mode = \"in\"))\n\n# 6. Visualize the network\nggraph(dlt2_network, layout = \"fr\") +\n  geom_edge_link(alpha = .2) +\n  geom_node_point(aes(color = role, size = local_size())) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\n# 7. Density, reciprocity, clustering, distances\ndensity &lt;- edge_density(dlt2_network)\nrecip   &lt;- reciprocity(dlt2_network)\ndlt2_network &lt;- dlt2_network |&gt;\n  activate(edges) |&gt;\n  mutate(reciprocated = edge_is_mutual())\nggraph(dlt2_network) +\n  geom_node_point(aes(size = in_degree)) +\n  geom_edge_link(aes(color = reciprocated), alpha = .2) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\ntrans   &lt;- transitivity(dlt2_network)\ndiam    &lt;- diameter(dlt2_network)\nmean_d  &lt;- mean_distance(dlt2_network)\n\n# 8. Print summary statistics\ncat(\"DLT2 Network Stats:\\n\")\n\nDLT2 Network Stats:\n\ncat(\"Nodes:\", num_nodes, \"Edges:\", num_edges, \"\\n\")\n\nNodes: 492 Edges: 2584 \n\ncat(\"Degree Centralization (all/in/out):\", centr_all, centr_in, centr_out, \"\\n\")\n\nDegree Centralization (all/in/out): 0.5311161 0.8650671 0.3273889 \n\ncat(\"Density:\", density, \"Reciprocity:\", recip, \"\\n\")\n\nDensity: 0.0106966 Reciprocity: 0.2500977 \n\ncat(\"Transitivity:\", trans, \"Diameter:\", diam, \"Mean Distance:\", mean_d, \"\\n\")\n\nTransitivity: 0.1248291 Diameter: 8 Mean Distance: 3.03815 \n\n\n\n\n\n3.3.4 Results and Discussion\n\nRQ1: What is the overall structure of interaction in each cohort?\n\nDLT1 consists of 445 nodes (participants) and 2529 edges (directed interactions).\nDLT2 has 492 nodes and 2584 edges.\nBoth networks are large and sparse:\n\nDensity: DLT1 = 0.013, DLT2 = 0.011\nInterpretation: Only about 1–1.3% of all possible connections exist—typical for online discussion networks where not every participant interacts with every other.\n\nBoth networks are multi-component (several disconnected groups), but most participants are included in the main giant component.\nThe diameter (longest shortest path) is 8 for both cohorts, and the average shortest path length is about 3.03 (DLT1) and 3.04 (DLT2), indicating that on average, any participant is just 3 steps away from any other in the largest group.\nInterpretation: Information or discussion threads can reach most participants with only a few hops, but overall engagement is selective rather than comprehensive.\n\n\n\nRQ2: Who are the most central or influential actors?\n\nCentrality (degree, in-degree, out-degree) analyses show a right-skewed distribution: most participants have low centrality, but a small subset are highly connected.\nIn both DLT1 and DLT2, facilitators and a handful of highly active participants emerge as hubs—they initiate and/or receive a disproportionate number of interactions.\n\nFor DLT1, degree centralization (all): 0.64 (in: 1.06, out: 0.23)\nFor DLT2, degree centralization (all): 0.53 (in: 0.87, out: 0.33)\n\nVisualization: Network plots with node size proportional to in-degree clearly highlight these central actors.\nInterpretation: These key individuals (often facilitators) play critical roles in steering discussion, providing feedback, and potentially keeping less active members engaged.\n\n\n\nRQ3: Are ties reciprocated?\n\nReciprocity (proportion of mutual connections):\n\nDLT1: 0.20 (20% of ties are reciprocated)\nDLT2: 0.25 (25% reciprocated)\n\nInterpretation: Most interactions are one-way (e.g., a reply that does not receive a response), but a substantial fraction are mutual—possibly reflecting peer-to-peer conversations or ongoing exchanges. In online learning contexts, this suggests a mix of broadcasting (one-to-many) and genuine dialog (two-way).\n\n\n\nRQ4: How cohesive are the networks?\n\nTransitivity/Clustering coefficient (probability that two connected nodes’ neighbors are also connected):\n\nDLT1: 0.089\nDLT2: 0.125\n\nInterpretation: Triadic closure is low—there are few closed triangles, so close-knit groups (where “my friend is also your friend”) are rare. The network structure is more “hub-and-spoke” than “cliquish.”\nDiameter: 8 for both, showing that even the furthest nodes can be reached in 8 steps.\nMean distance: ~3.0, so participants are relatively close to each other in the main component.\n\nComparison Across Cohorts\n\nDLT2 is slightly larger (more participants and interactions), but the structural properties—density, centralization, reciprocity, clustering, and path lengths—are all quite similar.\nMinor variations (e.g., higher reciprocity and clustering in DLT2) could reflect differences in facilitation style, cohort engagement, or participant composition.\nInterpretation: Both cohorts exhibit classic patterns for large-scale online educational discussions—a small number of central actors drive most of the interaction, the networks are sparse but efficiently connected, and genuine dialogue is present but not universal.\n\nEducational Implications\n\nFor educators and instructional designers:\nThese findings suggest that a small group of highly active facilitators or students are critical to fostering interaction. Encouraging more distributed engagement (for example, through peer response requirements or rotating leadership) may enhance network cohesion and learning opportunities.\nFor researchers:\nUnderstanding who occupies central positions and the overall structure of discussion networks can inform interventions to support isolated participants, promote reciprocity, and create more connected learning communities.\n\nSummary:\nThrough these SNA measures, we have shown how to reconstruct, visualize, and interpret the structure of large-scale online discussion networks. The approach enables identification of core communicators, understanding of participation patterns, and empirical comparison across cohorts or interventions. This “cookbook” can be adapted to other online learning or collaborative contexts.\n&gt; Note: This analysis is based on real-world data from online professional development courses. The methods and findings can be generalized to other educational settings where social networks play a role in learning and collaboration.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Networks Data</span>"
    ]
  },
  {
    "objectID": "chapter-3.html#case-study-hashtag-common-core",
    "href": "chapter-3.html#case-study-hashtag-common-core",
    "title": "Networks Data",
    "section": "3.4 Case Study: Hashtag Common Core",
    "text": "3.4 Case Study: Hashtag Common Core\n\n3.4.1 Purpose & Case\nThe purpose of this case study is to demonstrate the application of social network analysis (SNA) in a real-world policy context: the heated national debate over the Common Core State Standards (CCSS) as it played out on Twitter. Drawing on the work of Supovitz, Daly, del Fresno, and Kolouch, the #COMMONCORE Project provides a vivid example of how social media-enabled networks shape educational discourse and policy.\nThis case focuses on: - Identifying key actors (“transmitters,” “transceivers,” and “transcenders”) and measuring their influence, - Detecting subgroups/factions within the conversation, - Exploring how sentiment about the Common Core varies across network positions, - Demonstrating network wrangling, visualization, and analysis using real tweet data.\n\nData Source\nData was collected from Twitter’s public API using keywords/hashtags related to the Common Core (e.g., #commoncore, ccss, stopcommoncore). The dataset includes user names, tweets, mentions, retweets, and relevant timestamps from a sample week. Only public tweets are included, and user privacy is respected.\n\n\n\n3.4.2 Sample Research Questions\n\nRQ1: Who are the “transmitters,” “transceivers,” and “transcenders” in the Common Core Twitter network?\nRQ2: What subgroups or factions exist within the network, and how are they structured?\nRQ3: How does sentiment about the Common Core vary across actors and subgroups?\nRQ4: What other patterns of communication (e.g., centrality, clique formation, isolates) characterize this network?\n\n\n\n3.4.3 Analysis\n\nStep 1: Load Required Packages\n\nlibrary(tidyverse) \nlibrary(tidygraph) \nlibrary(ggraph) \nlibrary(skimr) \nlibrary(igraph) \nlibrary(tidytext) \nlibrary(vader)\n\n\n\nStep 2: Data Import and Wrangling\n\n# Import tweet data (edgelist format: sender, receiver, timestamp, text)\nccss_tweets &lt;- read_csv(\"data/ccss-tweets.csv\")\n\n# Prepare the edgelist (extract sender, mentioned users, and tweet text)\nties_1 &lt;- ccss_tweets %&gt;%\n  relocate(sender = screen_name, target = mentions_screen_name) %&gt;%\n  select(sender, target, created_at, text)\n\n# Unnest receiver to handle multiple mentions per tweet\nties_2 &lt;- ties_1 %&gt;%\n  unnest_tokens(input = target,\n                output = receiver,\n                to_lower = FALSE) %&gt;%\n  relocate(sender, receiver)\n\n# Remove tweets without mentions to focus on direct connections\nties &lt;- ties_2 %&gt;%\n  drop_na(receiver)\n\n# Save for reproducibility\nwrite_csv(ties, \"data/ccss-edgelist.csv\")\n\n# Build nodelist\nactors_1 &lt;- ties %&gt;%\n  select(sender, receiver) %&gt;%\n  pivot_longer(cols = c(sender,receiver))\n\nactors &lt;- actors_1 %&gt;%\n  select(value) %&gt;%\n  rename(actors = value) %&gt;%\n  distinct()\n\nwrite_csv(actors, \"data/ccss-nodelist.csv\")\n\n\n\nStep 3: Create Network Object\n\nccss_network &lt;- tbl_graph(edges = ties,\n                          nodes = actors,\n                          directed = TRUE)\nccss_network\n\n# A tbl_graph: 46 nodes and 42 edges\n#\n# A directed multigraph with 14 components\n#\n# Node Data: 46 × 1 (active)\n   actors        \n   &lt;chr&gt;         \n 1 DistanceLrnBot\n 2 k12movieguides\n 3 WEquilSchool  \n 4 JoeWEquil     \n 5 SumayLu       \n 6 fluttbot      \n 7 BodShameless  \n 8 Math          \n 9 ozsultan      \n10 sfchronicle   \n# ℹ 36 more rows\n#\n# Edge Data: 42 × 4\n   from    to created_at          text                                          \n  &lt;int&gt; &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;                                         \n1     1     2 2021-06-28 09:53:54 \"#Luca Movie Guide | Worksheet | Questions | …\n2     3     4 2021-06-28 02:32:59 \"Why public schools should focus more on buil…\n3     3     3 2021-06-28 02:32:59 \"Why public schools should focus more on buil…\n# ℹ 39 more rows\n\n\n\n\nStep 4: Network Structure – Components, Cliques, and Communities\n\nComponents\n\nIdentify weak and strong components (connected subgroups):\n\n\n\n    ccss_network &lt;- ccss_network |&gt;\n      activate(nodes) |&gt;\n      mutate(weak_component = group_components(type = \"weak\"),\n             strong_component = group_components(type = \"strong\"))\n    # View component sizes\n    ccss_network |&gt;\n      as_tibble() |&gt;\n      group_by(weak_component) |&gt;\n      summarise(size = n()) |&gt;\n      arrange(desc(size))\n\n# A tibble: 14 × 2\n   weak_component  size\n            &lt;int&gt; &lt;int&gt;\n 1              1    14\n 2              2     6\n 3              3     4\n 4              4     3\n 5              5     3\n 6              6     2\n 7              7     2\n 8              8     2\n 9              9     2\n10             10     2\n11             11     2\n12             12     2\n13             13     1\n14             14     1\n\n\n\nCliques\n\nIdentify fully connected subgroups (if any):\n\n\n\n    clique_num(ccss_network)\n\n[1] 4\n\n    cliques(ccss_network, min = 3)\n\n[[1]]\n+ 3/46 vertices, from 0e5da57:\n[1] 4 5 6\n\n[[2]]\n+ 3/46 vertices, from 0e5da57:\n[1] 39 40 41\n\n[[3]]\n+ 3/46 vertices, from 0e5da57:\n[1] 3 4 6\n\n[[4]]\n+ 4/46 vertices, from 0e5da57:\n[1] 3 4 5 6\n\n[[5]]\n+ 3/46 vertices, from 0e5da57:\n[1] 3 4 5\n\n[[6]]\n+ 3/46 vertices, from 0e5da57:\n[1] 3 5 6\n\n\n\nCommunities\n\nDetect densely connected communities using edge betweenness:\n\n\n\n    ccss_network &lt;- ccss_network |&gt;\n      morph(to_undirected) |&gt;\n      activate(nodes) |&gt;\n      mutate(sub_group = group_edge_betweenness()) |&gt;\n      unmorph()\n    ccss_network |&gt;\n      as_tibble() |&gt;\n      group_by(sub_group) |&gt;\n      summarise(size = n()) |&gt;\n      arrange(desc(size))\n\n# A tibble: 16 × 2\n   sub_group  size\n       &lt;int&gt; &lt;int&gt;\n 1         1    10\n 2         2     6\n 3         3     4\n 4         4     3\n 5         5     3\n 6         6     2\n 7         7     2\n 8         8     2\n 9         9     2\n10        10     2\n11        11     2\n12        12     2\n13        13     2\n14        14     2\n15        15     1\n16        16     1\n\n\n\n\nStep 5: Egocentric Analysis – Centrality & Key Actors\n\nccss_network &lt;- ccss_network |&gt;\n  activate(nodes) |&gt;\n  mutate(\n    size = local_size(),\n    in_degree = centrality_degree(mode = \"in\"),\n    out_degree = centrality_degree(mode = \"out\"),\n    closeness = centrality_closeness(),\n    betweenness = centrality_betweenness()\n  )\n\n# Identify top actors by out_degree (transmitters), in_degree (transceivers), and both (transcenders)\ntop_transmitters &lt;- ccss_network %&gt;% as_tibble() %&gt;% arrange(desc(out_degree)) %&gt;% head(5)\ntop_transceivers &lt;- ccss_network %&gt;% as_tibble() %&gt;% arrange(desc(in_degree)) %&gt;% head(5)\ntop_transcenders &lt;- ccss_network %&gt;% as_tibble() %&gt;%\n  filter(out_degree &gt; quantile(out_degree, 0.9) & in_degree &gt; quantile(in_degree, 0.9))\n\n\n\nStep 6: Visualize the Network\n\nggraph(ccss_network, layout = \"fr\") +\n  geom_node_point(aes(size = out_degree, color = out_degree)) +\n  geom_edge_link(alpha = .2) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\n\n\n\nStep 7: Sentiment Analysis (Optional)\nIf you want to analyze sentiment as in the original #COMMONCORE study:\n\nlibrary(vader)\nvader_ccss &lt;- vader_df(ccss_tweets$text)\n mean(vader_ccss$compound)\n\n[1] 0.08668182\n\n vader_ccss_summary &lt;- vader_ccss %&gt;%\n   mutate(sentiment = case_when(\n     compound &gt;= 0.05 ~ \"positive\",\n     compound &lt;= -0.05 ~ \"negative\",\n     TRUE ~ \"neutral\"\n   )) %&gt;%\n   count(sentiment)\n\n\n\n\n3.4.4 Results and Discussion\n\nRQ1: Who are the “transmitters,” “transceivers,” and “transcenders” in the Common Core Twitter network?\n\nTransmitters (high out-degree):\nThe user SumayLu stands out as the top transmitter, initiating 8 outgoing ties (mentions/retweets), followed by DouglasHolt... (5), WEquilSchool (3), fluttbot (3), and JoeWEquil (2). These users are the most active in broadcasting or mentioning others within the network.\nTransceivers (high in-degree):\nThe most-mentioned users are WEquilSchool and SumayLu (in-degree = 3), JoeWEquil (2), Tech4Learni... (2), and LASER_Insti... (2). These individuals receive the most attention from other actors—potential focal points in conversations.\nTranscenders (high in-degree and out-degree):\nOnly two users—WEquilSchool (in-degree = 3, out-degree = 3) and SumayLu (in-degree = 3, out-degree = 8)—simultaneously act as hubs for both sending and receiving communication. These “bridging” actors may serve as key facilitators or connectors in the discourse.\n\n\n\nRQ2: What subgroups or factions exist in the network?\n\nComponent analysis shows a fragmented network:\n\nThere are 14 weakly connected components, the largest containing 14 users, and several small groups or dyads (many with just 2–3 members).\nThis fragmentation suggests limited overall cohesion, with multiple parallel or isolated conversations occurring.\n\nClique analysis reveals:\n\nFour cliques (fully connected subgroups) of size 3 or 4—e.g., one 4-person clique involving nodes 3, 4, 5, and 6, and several overlapping 3-person cliques. This indicates pockets of tight-knit interaction, but such groups are rare relative to the size of the network.\n\nCommunity detection using edge betweenness identifies 16 subgroups, generally aligning with the component structure. The largest subgroup has 10 members, with most others much smaller.\n\n\n\nRQ3: What is the overall sentiment in the network?\n\nVADER sentiment analysis of tweet content yields:\n\nAn average sentiment score (compound) of 0.09 (slightly positive), indicating that, despite the policy controversy, the sampled tweets were, on balance, more positive than negative.\nWhen tweets are classified into categories:\n\nA mix of positive, neutral, and negative tweets is observed, with positive tweets slightly outnumbering negatives.\n\nThis suggests the debate, at least in this time slice, included advocacy and constructive dialogue, not only criticism or negativity.\n\n\n\n\nRQ4: What other patterns of communication (e.g., centrality, clique formation, isolates) characterize this network?\n\nCentrality Patterns:\nThe network displays a classic “star” structure in its largest component. Two users, SumayLu and WEquilSchool, stand out with high out-degree and in-degree centrality, respectively. Most other users have very low degree values (often 0 or 1), meaning they are peripheral, engaging in few interactions.\n\nTransmitters (high out-degree): e.g., SumayLu (8 outgoing ties), DouglasHolt... (5).\nTransceivers (high in-degree): e.g., WEquilSchool, SumayLu (both in-degree = 3).\nTranscenders (both high in- and out-degree): rare—only WEquilSchool and SumayLu meet this criterion in this sample.\n\nClique Formation:\nClique analysis revealed 4 cliques (fully connected subgroups) of size 3 or more, with one larger clique (size 4) and several overlapping smaller cliques. However, cliques are rare and limited in size—most communication occurs outside of dense subgroups.\nIsolates and Components:\nThe network has 14 weak components—many of them tiny. Several users are isolates or part of isolated dyads and triads, meaning they are disconnected from the main conversation or only loosely connected. This points to a lack of broad, network-wide cohesion.\nCommunity Structure:\nEdge betweenness community detection found 16 subgroups, typically matching up with the component structure: most subgroups are very small (2–3 nodes), while the largest subgroup consists of 10 users.\nSummary:\nCommunication in this network is characterized by:\n\nStrong centralization around a small number of users (hubs);\nSparse and fragmented structure with many small, disconnected components;\nLimited clique formation—pockets of tightly connected users exist but are rare;\nNumerous isolates—users who are only weakly or not at all connected to the core discussion.\n\n\n\n\nDiscussion\nThis analysis of the Common Core Twitter conversation reveals a sparse and fragmented network structure. The debate is distributed across many small subgroups, with only one moderately sized component (14 members). Within this landscape:\n\nKey actors such as SumayLu and WEquilSchool serve as both broadcasters and focal points of attention (“transcenders”), but most users are peripheral, interacting minimally.\nCliques and communities are few and small, underscoring the lack of broad cohesion. Most interactions happen within micro-groups rather than across the entire network.\nSentiment is, perhaps surprisingly, slightly positive on average. This may reflect the presence of advocacy groups, promotional messaging, or simply a lack of highly negative engagement during the observed period.\n\nImplications:\nThe findings illustrate classic social network phenomena in online policy debate: - Most users are only lightly involved, and only a select few drive discussion or receive significant attention. - Communication is siloed, with many small isolated groups and minimal bridging between them. - Sentiment analysis offers nuance: while public debates may be assumed to be contentious, the prevailing tone can still be balanced or even positive in certain time slices.\nFor researchers and practitioners, this means that: - Identifying and engaging “transcenders” is essential for bridging subgroups and spreading information. - Interventions or outreach should consider the network’s fragmentation—broader influence may require engaging multiple small groups individually rather than targeting a single “core.” - Combining SNA with text/sentiment analysis gives a fuller picture: not just who is talking, but how and with what tone.\nFuture analysis could track changes in sentiment and connectivity over time, or compare subgroups for differences in message tone and network position.\nReferences\n\nSupovitz, J., Daly, A.J., del Fresno, M., & Kolouch, C. (2017). #commoncore Project. Retrieved from http://www.hashtagcommoncore.com\nCarolan, B.V. (2014). Social Network Analysis and Education: Theory, Methods & Applications. Sage.\nSilge, J., & Robinson, D. (2017). Text Mining with R: A Tidy Approach. O’Reilly.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Networks Data</span>"
    ]
  },
  {
    "objectID": "chapter-4.html",
    "href": "chapter-4.html",
    "title": "Numeric Data",
    "section": "",
    "text": "4.1 Overview\nAbstract: This section reviews how to access data that is primarily numeric/quantitative in nature, but from a different source and of a different nature than the data typically used by social scientists. Example data sets include international or national large-scale assessments (e.g., PISA, NAEP，IPEDS) and data from digital technologies (e.g., log-trace data from Open University Learning Analytics Dataset (OULAD)).\nIn social science research, data is traditionally sourced from small-scale surveys, experiments, or qualitative studies. However, the rise of big data offers researchers opportunities to explore numeric and quantitative datasets of unprecedented scale and variety. This chapter discusses how to access and analyze large-scale datasets like international assessments (e.g., PISA, NAEP) and digital log-trace data (e.g., Open University Learning Analytics Dataset (OULAD)). These secondary data sources enable novel research questions and methods, particularly when paired with machine learning and statistical modeling approaches.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Numeric Data</span>"
    ]
  },
  {
    "objectID": "chapter-4.html#accessing-big-data-broadening-the-horizon",
    "href": "chapter-4.html#accessing-big-data-broadening-the-horizon",
    "title": "Numeric Data",
    "section": "4.2 Accessing Big data (Broadening the Horizon)",
    "text": "4.2 Accessing Big data (Broadening the Horizon)\n\n4.2.1 Big Data\n\nAccessing PISA Data\nThe Programme for International Student Assessment (PISA) is a widely used dataset for large-scale educational research. It assesses 15-year-old students’ knowledge and skills in reading, mathematics, and science across multiple countries. Researchers can access PISA data through various methods:\n\n1. Direct Download from the Official Website\nThe OECD provides direct access to PISA data files via its official website. Researchers can download data for specific years and cycles. Data files are typically provided in .csv or .sav (SPSS) formats, along with detailed documentation.\n\nSteps to Access PISA Data from the OECD Website:\n\nVisit the OECD PISA website.\nNavigate to the “Data” section.\nSelect the desired assessment year (e.g., 2022).\nDownload the data and accompanying codebooks.\n\n\n\n\n2. Using the OECD R Package\nThe OECD R package provides a direct interface to download and explore datasets published by the OECD, including PISA.\n\nSteps to Use the OECD Package:\n\nInstall and load the OECD package.\nUse the getOECD() function to fetch PISA data.\n\n\n\n# Install and load the OECD package\ninstall.packages(\"OECD\")\nlibrary(OECD)\n\n# Fetch PISA data for the 2018 cycle\npisa_data &lt;- getOECD(\"pisa\", years = \"2022\")\n\n# Display a summary of the data\nsummary(pisa_data)\n\n\n\n3. Using the Edsurvey R Package\nThe Edsurvey package is designed specifically for analyzing large-scale assessment data, including PISA. It allows for complex statistical modeling and supports handling weights and replicate weights used in PISA.\n\nSteps to Use the Edsurvey Package:\n\nInstall and load the Edsurvey package.\nDownload the PISA data from the OECD website and provide the path to the .sav files.\nLoad the data into R using readPISA().\n\n\n\n# Install and load the Edsurvey package\ninstall.packages(\"Edsurvey\")\nlibrary(Edsurvey)\n\n# Read PISA data from a local file\npisa_data &lt;- readPISA(\"path/to/PISA2022Student.sav\")\n\n# Display the structure of the dataset\nstr(pisa_data)\n\n\n\nComparison of Methods\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nDisadvantages\n\n\n\n\nDirect Download\nFull access to all raw data and documentation.\nRequires manual processing and cleaning.\n\n\nOECD Package\nEasy to use for downloading specific datasets.\nLimited to OECD-published formats.\n\n\nEdsurvey Package\nSupports advanced statistical analysis and weights.\nRequires additional setup and dependencies.\n\n\n\n\n\n\nAccessing IPEDS Data\nThe Integrated Postsecondary Education Data System (IPEDS) is a comprehensive source of data on U.S. colleges, universities, and technical and vocational institutions. It provides data on enrollments, completions, graduation rates, faculty, finances, and more. Researchers and policymakers widely use IPEDS data to analyze trends in higher education.\nThere are several ways to access IPEDS data, depending on the user’s needs and technical proficiency.\n\n1. Direct Download from the NCES Website\nThe most straightforward way to access IPEDS data is by downloading it directly from the National Center for Education Statistics (NCES) website.\n\n\nSteps to Access IPEDS Data:\n\nVisit the IPEDS Data Center.\nClick on “Use the Data” and navigate to the “Download IPEDS Data Files” section.\nSelect the desired data year and survey component (e.g., Fall Enrollment, Graduation Rates).\nDownload the data files, typically provided in .csv or .xls format, along with accompanying codebooks.\n\n\n\n2. Using the ipeds R Package\nThe ipeds R package simplifies downloading and analyzing IPEDS data directly from R by connecting to the NCES data repository.\n\n\nSteps to Use the ipeds Package:\n\nInstall and load the ipeds package.\nUse the download_ipeds() function to fetch data for specific survey components and years.\n\n\n# Install and load the ipeds package\ninstall.packages(\"ipeds\")\nlibrary(ipeds)\n\n# Download IPEDS data for completions in 2021\nipeds_data &lt;- download_ipeds(\"C\", year = 2021)\n\n# View the structure of the downloaded data\nstr(ipeds_data)\n\n\n\n3. Using the tidycensus R Package\nThe tidycensus package, while primarily designed for Census data, can access specific IPEDS data linked to educational institutions.\n\n\nSteps to Use the tidycensus Package:\n\nInstall and load the tidycensus package.\nSet up a Census API key to access the data.\nQuery IPEDS data for specific institution-level information.\n\n\n# Install and load the tidycensus package\ninstall.packages(\"tidycensus\")\nlibrary(tidycensus)\n\n# Set Census API key (replace with your actual key)\ncensus_api_key(\"your_census_api_key\")\n\n# Fetch IPEDS-related data (e.g., institution information)\nipeds_institutions &lt;- get_acs(\n  geography = \"place\",\n  variables = \"B14002_003\",\n  year = 2021,\n  survey = \"acs5\"\n)\n\n# View the first few rows\nhead(ipeds_institutions)\n\n\n\n4. Using Online Tools\nIPEDS provides several online tools for querying and visualizing data without requiring programming skills.\n\n\nCommon Tools:\n\nIPEDS Data Explorer: Enables users to query and export customized datasets.\nTrend Generator: Allows users to visualize trends in key metrics over time.\nIPEDS Use the Data: Simplified tool for accessing pre-compiled datasets.\n\n\n\nSteps to Use the IPEDS Data Explorer:\n\nVisit the IPEDS Data Explorer.\nSelect variables of interest, such as institution type, enrollment size, or location.\nFilter results by years, institution categories, or other criteria.\nExport the results as a .csv or .xlsx file.\n\n\n\nComparison of Methods\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nDisadvantages\n\n\n\n\nDirect Download\nFull access to raw data and documentation.\nRequires manual data preparation and cleaning.\n\n\nipeds Package\nAutomated access to specific components.\nLimited flexibility for customized queries.\n\n\ntidycensus Package\nAllows integration with Census and ACS data.\nRequires API setup and advanced R skills.\n\n\nOnline Tools\nUser-friendly and suitable for non-coders.\nLimited to predefined queries and exports.\n\n\n\n\n\n\nAccessing Open University Learning Analytics Dataset (OULAD)\nThe Open University Learning Analytics Dataset (OULAD) is a publicly available dataset designed to support research in educational data mining and learning analytics. It includes student demographics, module information, interactions with the virtual learning environment (VLE), and assessment scores.\n\n\nSteps to Access OULAD Data\n\nVisit the OULAD Repository**\nThe dataset is hosted on the Open University’s Analytics Project. To access the data: 1. Navigate to the website. 2. Download the dataset as a .zip file. 3. Extract the .zip file to a local directory.\nThe dataset contains multiple CSV files: - studentInfo.csv: Student demographics and performance data. - studentVle.csv: Interactions with the VLE. - vle.csv: Details of learning resources. - studentAssessment.csv: Assessment scores.\n\n\nLoading OULAD Data in R\nOnce the data is downloaded and extracted, follow these steps to load and access it in R:\n\n\nStep 1: Install Required Packages\n\n# Install necessary packages\ninstall.packages(c(\"readr\", \"dplyr\"))\n\n\n\nStep 2: Load Data\nUse the readr package to read the CSV files into R.\n\n# Load required libraries\nlibrary(readr)\n\n# Define the path to the OULAD data\ndata_path &lt;- \"path/to/OULAD/\"\n\n# Load individual CSV files\nstudent_info &lt;- read_csv(file.path(data_path, \"studentInfo.csv\"))\nstudent_vle &lt;- read_csv(file.path(data_path, \"studentVle.csv\"))\nvle &lt;- read_csv(file.path(data_path, \"vle.csv\"))\nstudent_assessment &lt;- read_csv(file.path(data_path, \"studentAssessment.csv\"))\n\n\n\nStep 3: Preview the Data\nInspect the structure and contents of the datasets.\n\n# View the first few rows of student info\nhead(student_info)\n\n# Check the structure of the student VLE data\nstr(student_vle)\n\n\n\n\n\n4.2.2 Learning Analytics\n\nWhat is Learning Analytics?\nLearning Analytics (LA) refers to the measurement, collection, analysis, and reporting of data about learners and their contexts. The primary goal of LA is to understand and improve learning processes by identifying patterns, predicting outcomes, and providing actionable insights to educators, institutions, and learners.\nKey features of LA include: - Data Collection: Gathering information from digital platforms such as learning management systems (LMS) or external assessments. - Analysis: Using machine learning, statistical methods, or visualization tools to reveal trends and patterns. - Applications: Supporting personalized learning, enhancing institutional decision-making, and improving curriculum design.\n\n\nApplications of Learning Analytics in Big Data\nLearning analytics can be applied to large-scale educational datasets like PISA, IPEDS, and OULAD to uncover trends, predict outcomes, and guide interventions.\n\n1. PISA Data and Learning Analytics\n\nWhat it offers: Insights into international student performance in reading, math, and science, combined with contextual variables (e.g., socio-economic status).\nLA Applications:\n\nIdentifying key factors influencing performance across countries.\nPredicting the impact of ICT use on student achievement.\nSegmenting students into performance clusters for targeted interventions.\n\n\n\n\n2. IPEDS Data and Learning Analytics\n\nWhat it offers: U.S. institutional-level data on enrollment, graduation rates, tuition, and financial aid.\nLA Applications:\n\nAnalyzing trends in student demographics across institutions.\nPredicting enrollment patterns based on historical data.\nBenchmarking institutions to inform policymaking and funding decisions.\n\n\n\n\n3. OULAD and Learning Analytics\n\nWhat it offers: Rich data on student engagement with virtual learning environments (VLE), assessment scores, and demographic information.\nLA Applications:\n\nTracking student interactions with learning resources to predict course completion.\nModeling the relationship between VLE usage and final grades.\nDetecting early warning signs for at-risk students based on engagement metrics.\n\n\n\n\n\nWhy Learning Analytics Matters\nThe integration of Learning Analytics with big data enables researchers and practitioners to: - Personalize Learning: Tailor educational experiences to meet individual needs. - Improve Retention: Identify at-risk learners and implement timely interventions. - Enhance Decision-Making: Provide evidence-based recommendations for curriculum and policy adjustments.\nBy leveraging datasets like PISA, IPEDS, and OULAD, learning analytics can help bridge the gap between raw data and actionable insights, fostering a more equitable and effective educational landscape.\n\n\nSupervised Learning in Learning Analytics\nMachine Learning, particularly Supervised Learning, has become a cornerstone of Learning Analytics. Supervised learning models are trained on labeled datasets, where input features are mapped to known outcomes, enabling the prediction of new, unseen data.\n\nKey Concepts in Supervised Learning\n\nDefinition\nSupervised Learning is a subset of Machine Learning focused on learning a mapping between input variables (features) and output variables (labels or outcomes). Models trained on labeled data can predict outcomes for new data points.\nCommon Algorithms\n\nLinear Regression\nLogistic Regression\nDecision Trees and Random Forests\nNeural Networks\n\nApplications in Education\nSupervised learning is particularly effective in Learning Analytics for predicting:\n\nStudent performance\nDropout risks\nEnrollment trends\nCourse completion rates\n\n\n\n\n\nApplications of Supervised Learning with Big Data\n\n1. PISA Data and Supervised Learning\n\nGoal: Use demographic and contextual features to predict student performance in mathematics, reading, or science.\nExample: Train a linear regression model to identify the relationship between socioeconomic status and test scores.\n\n\n\n2. IPEDS Data and Supervised Learning\n\nGoal: Develop models to predict institutional enrollment rates based on financial aid, demographics, and program offerings.\nExample: Use logistic regression to forecast whether a student is likely to enroll based on financial aid eligibility.\n\n\n\n3. OULAD Data and Supervised Learning\n\nGoal: Predict student outcomes (e.g., pass/fail) based on engagement metrics like forum participation and assignment submissions.\nExample: Train a random forest model to classify students as “at-risk” or “not at-risk” based on weekly interaction data.\n\n\n\n\nChoosing the Right Supervised Learning Approach\nWhen applying supervised learning in Learning Analytics: 1. Define the Goal: Clearly articulate the outcome you want to predict (e.g., performance, enrollment, or engagement). 2. Select an Algorithm: Choose an appropriate model based on the data and prediction task. - For continuous outcomes, use regression models. - For categorical outcomes, use classification models like logistic regression or random forests. 3. Feature Engineering: Select and preprocess relevant features (e.g., attendance, demographics, assignment scores) to improve model accuracy. 4. Evaluate Model Performance: Use metrics such as accuracy, precision, recall, or R-squared to assess model effectiveness.\nIntegrating supervised learning techniques into Learning Analytics, researchers and practitioners can leverage big data to make data-driven predictions and decisions, ultimately enhancing educational outcomes.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Numeric Data</span>"
    ]
  },
  {
    "objectID": "chapter-4.html#logistic-regression-ml",
    "href": "chapter-4.html#logistic-regression-ml",
    "title": "Numeric Data",
    "section": "4.3 Logistic Regression ML",
    "text": "4.3 Logistic Regression ML\n\n4.3.1 Purpose + CASE\n\nPurpose\nLogistic regression is a supervised learning technique widely used for binary classification tasks. It models the probability of an event occurring (e.g., success vs. failure) based on a set of predictor variables. Logistic regression is particularly effective in educational research for predicting outcomes such as retention, enrollment, or graduation rates.\n\n\nCASE: Predicting Graduation Rates\nThis case study is based on IPEDS data and inspired by Zong and Davis (2022). We predict graduation rates as a binary outcome (good_grad_rate) using institutional features such as total enrollment, admission rate, tuition fees, and average instructional staff salary.\n\n\n\n4.3.2 Sample Research Questions (RQs)\n\nRQ A: What institutional factors are associated with high graduation rates in U.S. four-year universities?\nRQ B: How accurately can we predict high graduation rates using institutional features with supervised machine learning?\n\n\n\n4.3.3 Analysis\n\nLoading Required Packages\nWe load necessary R packages for data wrangling, cleaning, and modeling.\n\n# Load necessary libraries for data cleaning, wrangling, and modeling\nlibrary(tidyverse)  # For data manipulation and visualization\nlibrary(tidymodels) # For machine learning workflows\nlibrary(janitor)    # For cleaning variable names\n\n\n\nLoading and Cleaning Data\nWe read the IPEDS dataset and clean column names for easier handling.\n\n# Read in IPEDS data from CSV file\nipeds &lt;- read_csv(\"data/ipeds-all-title-9-2022-data.csv\")\n\n# Clean column names for consistency and usability\nipeds &lt;- janitor::clean_names(ipeds)\n\n\n\nData Wrangling\nSelect relevant variables, filter the dataset, and create the dependent variable good_grad_rate.\n\n# Select and rename key variables; filter relevant institutions\nipeds &lt;- ipeds %&gt;%\n  select(\n    name = institution_name,                  # Institution name\n    total_enroll = drvef2022_total_enrollment, # Total enrollment\n    pct_admitted = drvadm2022_percent_admitted_total, # Admission percentage\n    tuition_fees = drvic2022_tuition_and_fees_2021_22, # Tuition fees\n    grad_rate = drvgr2022_graduation_rate_total_cohort, # Graduation rate\n    percent_fin_aid = sfa2122_percent_of_full_time_first_time_undergraduates_awarded_any_financial_aid, # Financial aid\n    avg_salary = drvhr2022_average_salary_equated_to_9_months_of_full_time_instructional_staff_all_ranks # Staff salary\n  ) %&gt;%\n  filter(!is.na(grad_rate)) %&gt;% # Remove rows with missing graduation rates\n  mutate(\n    # Create binary dependent variable for high graduation rates\n    good_grad_rate = if_else(grad_rate &gt; 62, 1, 0),\n    good_grad_rate = as.factor(good_grad_rate) # Convert to factor\n  )\n\n\n\nExploratory Data Analysis (EDA)\nVisualize the distribution of the graduation rate.\n\n# Plot a histogram of graduation rates\nipeds %&gt;%\n  ggplot(aes(x = grad_rate)) +\n  geom_histogram(bins = 20, fill = \"blue\", color = \"white\") +\n  labs(\n    title = \"Distribution of Graduation Rates\",\n    x = \"Graduation Rate\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\n\n\nLogistic Regression Model\nFit a logistic regression model to predict high graduation rates.\n\n# Fit logistic regression model\nm1 &lt;- glm(\n  good_grad_rate ~ total_enroll + pct_admitted + tuition_fees + percent_fin_aid + avg_salary,\n  data = ipeds,\n  family = \"binomial\" # Specify logistic regression for binary outcome\n)\n\n# View model summary\nsummary(m1)\n\n\nCall:\nglm(formula = good_grad_rate ~ total_enroll + pct_admitted + \n    tuition_fees + percent_fin_aid + avg_salary, family = \"binomial\", \n    data = ipeds)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -8.742e-01  6.237e-01  -1.402    0.161    \ntotal_enroll     3.350e-05  7.880e-06   4.251 2.13e-05 ***\npct_admitted    -1.407e-02  3.519e-03  -3.997 6.40e-05 ***\ntuition_fees     6.952e-05  4.965e-06  14.003  &lt; 2e-16 ***\npercent_fin_aid -2.960e-02  5.652e-03  -5.237 1.64e-07 ***\navg_salary       2.996e-05  3.870e-06   7.740 9.91e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2277  on 1706  degrees of freedom\nResidual deviance: 1632  on 1701  degrees of freedom\n  (3621 observations deleted due to missingness)\nAIC: 1644\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nSupervised ML Workflow\nUse the tidymodels framework to build a machine learning model.\n\n# Define recipe for the model (preprocessing steps)\nmy_rec &lt;- recipe(good_grad_rate ~ total_enroll + pct_admitted + tuition_fees + percent_fin_aid + avg_salary, data = ipeds)\n\n# Specify logistic regression model with tidymodels\nmy_mod &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%         # Use glm engine for logistic regression\n  set_mode(\"classification\")    # Specify binary classification task\n\n# Create workflow to connect recipe and model\nmy_wf &lt;- workflow() %&gt;%\n  add_recipe(my_rec) %&gt;%\n  add_model(my_mod)\n\n# Fit the logistic regression model\nfit_model &lt;- fit(my_wf, ipeds)\n\n# Generate predictions on the dataset\npredictions &lt;- predict(fit_model, ipeds) %&gt;%\n  bind_cols(ipeds) # Combine predictions with original data\n\n# Calculate and display accuracy\nmy_accuracy &lt;- predictions %&gt;%\n  metrics(truth = good_grad_rate, estimate = .pred_class) %&gt;%\n  filter(.metric == \"accuracy\")\n\nmy_accuracy\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.800\n\n\n\n\n\n4.3.4 Results and Discussions\n\nLogistic Regression Model (RQ A)\nThe logistic regression model was fitted to predict whether a university achieves a “good” graduation rate (i.e., graduation rate &gt; 62%) based on several institutional features. The model output is summarized below:\n\nCoefficients & Significance:\n\ntotal_enroll: Estimate = 3.35e-05, z = 4.251, p = 2.13e-05\nInterpretation: As total enrollment increases, the probability of a high graduation rate increases.\npct_admitted: Estimate = -1.407e-02, z = -3.997, p = 6.40e-05\nInterpretation: Higher admission percentages are associated with a lower likelihood of achieving a high graduation rate.\ntuition_fees: Estimate = 6.952e-05, z = 14.003, p &lt; 2e-16\nInterpretation: Higher tuition fees are strongly associated with higher graduation rates.\npercent_fin_aid: Estimate = -2.960e-02, z = -5.237, p = 1.64e-07\nInterpretation: A higher percentage of students receiving financial aid is associated with a lower probability of a good graduation rate.\navg_salary: Estimate = 2.996e-05, z = 7.740, p = 9.91e-15\nInterpretation: Higher average salaries for instructional staff are positively associated with high graduation rates.\n\nModel Fit Statistics:\n\nNull Deviance: 2277 (on 1706 degrees of freedom)\nResidual Deviance: 1632 (on 1701 degrees of freedom)\nAIC: 1644\nNote: 3621 observations were deleted due to missing values.\n\n\nOverall, the regression model demonstrates that several institutional factors are statistically significant predictors of graduation rates. In particular, tuition fees and avg_salary have a strong positive effect, while pct_admitted and percent_fin_aid show negative associations.\n\n\nSupervised ML Workflow Results (RQ B)\nUsing the tidymodels framework, we built a logistic regression model as part of a supervised machine learning workflow. The performance metric obtained is as follows:\n\nAccuracy: 80.02%\n\nThis indicates that the machine learning model correctly classified approximately 80% of the institutions as having either a good or not good graduation rate, based on the selected predictors.\n\n\nOverall Discussion\n\nSimilarities between Approaches:\n\nBoth the traditional logistic regression and the tidymodels workflow identified key predictors that influence graduation rates, such as total enrollment, admission percentage, tuition fees, financial aid percentage, and average staff salary.\nEach approach provides valuable insights: the regression model offers detailed coefficient estimates and significance levels, while the tidymodels workflow emphasizes predictive accuracy.\n\nDifferences between Approaches:\n\nInterpretability vs. Predictive Performance: The logistic regression output delivers interpretability through its coefficients and p-values, allowing us to understand the direction and magnitude of the relationships. In contrast, the supervised ML workflow focuses on achieving a robust predictive performance, evidenced by an 80% accuracy.\nHandling of Data: The traditional regression model summarizes the relationship between variables, whereas the ML workflow integrates data pre-processing, modeling, and validation into a cohesive framework.\n\n\nIn summary, our analyses indicate that institutional factors, particularly tuition fees and staff salaries, play a significant role in predicting graduation outcomes. The supervised ML approach, with an accuracy of around 80%, confirms the model’s practical utility in classifying institutions based on graduation performance. Both methods complement each other, providing a comprehensive understanding of the underlying dynamics that drive graduation rates in higher education.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Numeric Data</span>"
    ]
  },
  {
    "objectID": "chapter-4.html#random-forests-ml-on-interactions-data",
    "href": "chapter-4.html#random-forests-ml-on-interactions-data",
    "title": "Numeric Data",
    "section": "4.4 Random Forests ML on Interactions Data",
    "text": "4.4 Random Forests ML on Interactions Data\nIn this section, we explore a more sophisticated supervised learning approach—Random Forests—to model student interactions data from the Open University Learning Analytics Dataset (OULAD). Building on our earlier work with logistic regression and evaluation metrics, this case study examines whether a random forest model can improve predictive performance when leveraging clickstream data from the virtual learning environment (VLE).\n\n4.4.1 Purpose + CASE\n\nPurpose\nRandom Forests is an ensemble learning method that builds multiple decision trees and aggregates their results to improve prediction accuracy and control over-fitting. It is particularly well suited for complex, high-dimensional data such as student interaction (clickstream) data. This approach not only provides robust predictions but also offers insights into variable importance, helping us understand which features most influence student outcomes.\n\n\nCASE\nInspired by research on digital trace data (e.g., Rodriguez et al., 2021; Bosch, 2021), this case study uses pre-processed interactions data from OULAD. In our analysis, we focus on predicting whether a student will pass the course (a binary outcome) based on engineered features derived from clickstream data. These features include the total number of clicks (sum_clicks), summary statistics (mean and standard deviation of clicks), and linear trends over time (slope and intercept from clickstream patterns).\n\n\n\n4.4.2 Sample Research Questions\n\nRQ1: How accurately can a random forest model predict whether a student will pass a course using interactions data from OULAD?\nRQ2: Which interaction-based features (e.g., total clicks, click stream slope) are most important in predicting student outcomes?\nRQ3: How does the use of cross-validation (e.g., v-fold CV) influence the stability and generalizability of the random forest model on interactions data?\n\n\n\n4.4.3 Analysis\n\nLoading Required Packages\n\n# Load necessary libraries for data manipulation and modeling\nlibrary(tidyverse)      # Data wrangling and visualization\nlibrary(janitor)        # Cleaning variable names\nlibrary(tidymodels)     # Modeling workflow\nlibrary(ranger)         # Random forest implementation\nlibrary(vip)            # Variable importance plots\n\n\n\nLoading and Preparing the Data\nWe load the pre-filtered interactions data from OULAD along with a students-and-assessments file, then join them to create a complete dataset for modeling.\n\n# Load the interactions data (filtered for the first one-third of the semester)\ninteractions &lt;- read_csv(\"data/oulad-interactions-filtered.csv\")\n\n# Load the students and assessments data\nstudents_and_assessments &lt;- read_csv(\"data/oulad-students-and-assessments.csv\")\n\n# Create cut-off dates based on assessments data (using first quantile as intervention point)\nassessments &lt;- read_csv(\"data/oulad-assessments.csv\")\n\n# Create cut-off dates based on assessments data using the correct date column 'date_submitted'\ncode_module_dates &lt;- assessments %&gt;% \n    group_by(code_module, code_presentation) %&gt;% \n    summarize(quantile_cutoff_date = quantile(date_submitted, probs = 0.25, na.rm = TRUE), .groups = 'drop')\n\n# Join interactions with the cutoff dates and filter\ninteractions_joined &lt;- interactions %&gt;% \n    left_join(code_module_dates, by = c(\"code_module\", \"code_presentation\"))\n\n\ninteractions_joined &lt;- interactions_joined %&gt;% \n    select(-quantile_cutoff_date.x) %&gt;% \n    rename(quantile_cutoff_date = quantile_cutoff_date.y)\n\n\n\n# Filter interactions to include only those before the cutoff date\ninteractions_filtered &lt;- interactions_joined %&gt;% \n    filter(date &lt; quantile_cutoff_date)\n\n# Summarize interactions: total clicks, mean and standard deviation\ninteractions_summarized &lt;- interactions_filtered %&gt;% \n    group_by(id_student, code_module, code_presentation) %&gt;% \n    summarize(\n      sum_clicks = sum(sum_click),\n      sd_clicks = sd(sum_click), \n      mean_clicks = mean(sum_click)\n    )\n\n# (Optional) Further feature engineering: derive linear slopes from clickstream\nfit_model &lt;- function(data) {\n    tryCatch(\n        { \n            model &lt;- lm(sum_click ~ date, data = data)\n            tidy(model)\n        },\n        error = function(e) { tibble(term = NA, estimate = NA, std.error = NA, statistic = NA, p.value = NA) },\n        warning = function(w) { tibble(term = NA, estimate = NA, std.error = NA, statistic = NA, p.value = NA) }\n    )\n}\n\ninteractions_slopes &lt;- interactions_filtered %&gt;%\n    group_by(id_student, code_module, code_presentation) %&gt;%\n    nest() %&gt;%\n    mutate(model = map(data, fit_model)) %&gt;%\n    unnest(model) %&gt;%\n    ungroup() %&gt;%\n    select(code_module, code_presentation, id_student, term, estimate) %&gt;%\n    filter(!is.na(term)) %&gt;%\n    pivot_wider(names_from = term, values_from = estimate) %&gt;%\n    mutate_if(is.numeric, round, 4) %&gt;%\n    rename(intercept = `(Intercept)`, slope = date)\n\n# Join summarized clicks and slopes features\ninteractions_features &lt;- left_join(interactions_summarized, interactions_slopes, by = c(\"id_student\", \"code_module\", \"code_presentation\"))\n\n# Finally, join with students_and_assessments to get the outcome variable\nstudents_assessments_and_interactions &lt;- left_join(students_and_assessments, interactions_features, by = c(\"id_student\", \"code_module\", \"code_presentation\"))\n\n# Ensure outcome variable 'pass' is a factor\nstudents_assessments_and_interactions &lt;- students_assessments_and_interactions %&gt;% \n    mutate(pass = as.factor(pass))\n\n# Optional: Inspect the final dataset\nstudents_assessments_and_interactions %&gt;% \n    skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n32593\n\n\nNumber of columns\n22\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nfactor\n1\n\n\nnumeric\n13\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncode_module\n0\n1\n3\n3\n0\n7\n0\n\n\ncode_presentation\n0\n1\n5\n5\n0\n4\n0\n\n\ngender\n0\n1\n1\n1\n0\n2\n0\n\n\nregion\n0\n1\n5\n20\n0\n13\n0\n\n\nhighest_education\n0\n1\n15\n27\n0\n5\n0\n\n\nage_band\n0\n1\n4\n5\n0\n3\n0\n\n\ndisability\n0\n1\n1\n1\n0\n2\n0\n\n\nfinal_result\n0\n1\n4\n11\n0\n4\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\npass\n0\n1\nFALSE\n2\n0: 20232, 1: 12361\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nid_student\n0\n1.00\n706687.67\n549167.31\n3733.00\n508573.00\n590310.00\n644453.00\n2716795.00\n▅▇▁▁▁\n\n\nimd_band\n4627\n0.86\n5.62\n2.73\n1.00\n4.00\n6.00\n8.00\n10.00\n▃▇▇▆▆\n\n\nnum_of_prev_attempts\n0\n1.00\n0.16\n0.48\n0.00\n0.00\n0.00\n0.00\n6.00\n▇▁▁▁▁\n\n\nstudied_credits\n0\n1.00\n79.76\n41.07\n30.00\n60.00\n60.00\n120.00\n655.00\n▇▁▁▁▁\n\n\nmodule_presentation_length\n0\n1.00\n256.01\n13.18\n234.00\n241.00\n262.00\n268.00\n269.00\n▇▁▁▅▇\n\n\ndate_registration\n45\n1.00\n-69.41\n49.26\n-322.00\n-100.00\n-57.00\n-29.00\n167.00\n▁▂▇▃▁\n\n\ndate_unregistration\n22521\n0.31\n49.76\n82.46\n-365.00\n-2.00\n27.00\n109.00\n444.00\n▁▁▇▂▁\n\n\nmean_weighted_score\n7958\n0.76\n544.70\n381.39\n0.00\n160.00\n610.00\n875.00\n1512.00\n▇▃▇▅▁\n\n\nsum_clicks\n3495\n0.89\n474.93\n572.89\n1.00\n128.00\n295.50\n604.00\n10712.00\n▇▁▁▁▁\n\n\nsd_clicks\n3753\n0.88\n4.91\n5.51\n0.00\n2.37\n3.72\n6.44\n560.24\n▇▁▁▁▁\n\n\nmean_clicks\n3495\n0.89\n3.19\n1.30\n1.00\n2.33\n2.95\n3.82\n47.12\n▇▁▁▁▁\n\n\nintercept\n3640\n0.89\n3.04\n4.61\n-585.59\n2.15\n2.80\n3.66\n130.83\n▁▁▁▁▇\n\n\nslope\n4441\n0.86\n0.01\n0.22\n-12.17\n-0.01\n0.01\n0.03\n20.12\n▁▇▁▁▁\n\n\n\n\n\n\n\nCreating the Model Recipe\nWe build a recipe that includes the engineered features from interactions data along with other predictors from the students data.\n\nmy_rec2 &lt;- recipe(pass ~ disability +\n                     date_registration + \n                     gender +\n                     code_module +\n                     mean_weighted_score +\n                     sum_clicks + sd_clicks + mean_clicks + \n                     intercept + slope, \n                 data = students_assessments_and_interactions) %&gt;% \n    step_dummy(disability) %&gt;% \n    step_dummy(gender) %&gt;%  \n    step_dummy(code_module) %&gt;% \n    step_impute_knn(mean_weighted_score) %&gt;% \n    step_impute_knn(sum_clicks) %&gt;% \n    step_impute_knn(sd_clicks) %&gt;% \n    step_impute_knn(mean_clicks) %&gt;% \n    step_impute_knn(intercept) %&gt;% \n    step_impute_knn(slope) %&gt;% \n    step_impute_knn(date_registration) %&gt;% \n    step_normalize(all_numeric_predictors())\n\n\n\nSpecifying the Model and Workflow\nWe use a random forest model via the ranger engine and set up our workflow.\n\n# Specify random forest model\nmy_mod2 &lt;- rand_forest() %&gt;% \n    set_engine(\"ranger\", importance = \"impurity\") %&gt;% \n    set_mode(\"classification\")\n\n# Create workflow to bundle the recipe and model\nmy_wf2 &lt;- workflow() %&gt;% \n    add_recipe(my_rec2) %&gt;% \n    add_model(my_mod2)\n\n\n\nResampling and Model Fitting\nWe perform cross-validation (v-fold CV) to estimate model performance.\n\n# Create 4-fold cross-validation on training data\nvfcv &lt;- vfold_cv(data = students_assessments_and_interactions, v = 4, strata = pass)\n\n# Specify metrics: accuracy, sensitivity, specificity, ppv, npv, and Cohen's kappa\nclass_metrics &lt;- metric_set(accuracy, sensitivity, specificity, ppv, npv, kap)\n\n# Fit the model using resampling\nfitted_model_resamples &lt;- fit_resamples(my_wf2, resamples = vfcv, metrics = class_metrics)\n\n# Collect and display metrics\ncollect_metrics(fitted_model_resamples)\n\n# A tibble: 6 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.670     4 0.00209 Preprocessor1_Model1\n2 kap         binary     0.260     4 0.00474 Preprocessor1_Model1\n3 npv         binary     0.588     4 0.00382 Preprocessor1_Model1\n4 ppv         binary     0.701     4 0.00151 Preprocessor1_Model1\n5 sensitivity binary     0.816     4 0.00211 Preprocessor1_Model1\n6 specificity binary     0.430     4 0.00352 Preprocessor1_Model1\n\n\n\n\nFinal Model Fit and Evaluation\nFinally, we fit the model on the full training set (using last_fit) and evaluate its predictions on the test set.\n\n# Split data into training and testing sets (e.g., 33% for testing)\nset.seed(20230712)\ntrain_test_split &lt;- initial_split(students_assessments_and_interactions, prop = 0.67, strata = pass)\ndata_train &lt;- training(train_test_split)\ndata_test &lt;- testing(train_test_split)\n\n# Fit final model on the training set and evaluate on the test set\nfinal_fit &lt;- last_fit(my_wf2, train_test_split, metrics = class_metrics)\n\n# Collect and display final metrics\ncollect_metrics(final_fit)\n\n# A tibble: 6 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.665 Preprocessor1_Model1\n2 sensitivity binary         0.815 Preprocessor1_Model1\n3 specificity binary         0.419 Preprocessor1_Model1\n4 ppv         binary         0.697 Preprocessor1_Model1\n5 npv         binary         0.580 Preprocessor1_Model1\n6 kap         binary         0.247 Preprocessor1_Model1\n\n# Generate and display a confusion matrix for final predictions\ncollect_predictions(final_fit) %&gt;% \n    conf_mat(.pred_class, pass)\n\n          Truth\nPrediction    0    1\n         0 5439 1238\n         1 2370 1710\n\n# Extract the fitted model from the final workflow and plot variable importance\nfinal_fit %&gt;% \n  pluck(\".workflow\", 1) %&gt;%   # Extract the workflow object from the final fit\n  extract_fit_parsnip() %&gt;%   # Retrieve the fitted model from the workflow\n  vip(num_features = 10)      # Plot the top 10 important features\n\n\n\n\n\n\n\n# Extract the fitted model from the workflow\nfinal_model &lt;- final_fit %&gt;% \n  pluck(\".workflow\", 1) %&gt;% \n  extract_fit_parsnip()\n# Extract the variable importance values from the fitted model\nimportance_values &lt;- final_model$fit$variable.importance\n\n# Print the variable importance values\nprint(importance_values)\n\n  date_registration mean_weighted_score          sum_clicks           sd_clicks \n          652.14806           838.06052          1062.49141           778.63848 \n        mean_clicks           intercept               slope        disability_Y \n          737.79417           729.52344           739.85682            60.93367 \n           gender_M     code_module_BBB     code_module_CCC     code_module_DDD \n           79.62440            77.18477            70.72006            46.68243 \n    code_module_EEE     code_module_FFF     code_module_GGG \n           28.26595            74.31310            35.27026 \n\n\n\n\n\n4.4.4 Results and Discussions\n\nResearch Question 1 (RQ1):\nHow accurately can a random forest model predict whether a student will pass a course using interactions data from OULAD?\nResponse:\nUsing 4-fold cross-validation, our random forest model yielded an average accuracy of approximately 67.0% (mean accuracy from resamples: 0.670) with a Cohen’s Kappa of 0.261, suggesting moderate agreement beyond chance. When fitted on the entire training set and evaluated on the test set, the final model showed an accuracy of 66.5% along with: - Sensitivity: 81.5% – indicating the model correctly identifies a high proportion of students who pass. - Specificity: 41.9% – suggesting that the model is less effective at correctly identifying students who do not pass. - Positive Predictive Value (PPV): 69.7% - Negative Predictive Value (NPV): 58.0%\nThe confusion matrix shows: - True Negatives (TN): 5439 - False Negatives (FN): 1238 - False Positives (FP): 2370 - True Positives (TP): 1710\nOverall, these metrics indicate that while the model performs well in detecting positive outcomes (high sensitivity), its lower specificity means that it tends to misclassify a relatively higher proportion of non-passing students.\n\n\nResearch Question 2 (RQ2):\nWhich interaction-based features are most important in predicting student outcomes?\nResponse:\nThe variable importance analysis, extracted from the final random forest model using the vip() function, highlights the following key predictors (with their respective importance scores):\n\nsum_clicks: 1062.49 – This is the most influential feature, indicating that the total number of clicks (i.e., student engagement) in the VLE is a strong predictor of student success.\nmean_weighted_score: 838.06 – Reflecting academic performance as measured by weighted assessment scores.\nmean_clicks: 737.79, slope: 739.86, and intercept: 729.52 – These engineered features representing the central tendency and trend of click behavior further underline the importance of digital engagement patterns.\ndate_registration: 652.15 – The registration date also plays a significant role.\nOther categorical variables (e.g., dummy-coded disability, gender, and code_module levels) generally show lower importance scores, with values typically under 80, indicating that while they do contribute, engagement and performance metrics dominate.\n\nThese results suggest that both the intensity and the temporal trend of student interactions with the learning environment are critical in predicting whether a student will pass.\n\n\nResearch Question 3 (RQ3):\nHow does the use of cross-validation impact the stability and generalizability of the random forest model on interactions data?\nResponse:\nThe use of 4-fold cross-validation (via vfold_cv) allowed us to assess the model’s performance across multiple subsets of the data, mitigating the risk of overfitting. The resampling results are relatively consistent (with accuracy around 67%, sensitivity at 81.6%, and specificity around 43.2%), which supports the model’s robustness and generalizability. Although the final test set performance (accuracy of 66.5%) is slightly lower, the overall consistency of metrics across folds indicates that our model is stable when applied to unseen data.\n\n\nOverall Discussion\nThe random forest model built on interactions data from OULAD demonstrates decent predictive performance with an accuracy of approximately 66.5–67% and high sensitivity (around 81.5%), indicating strong capability in identifying students who will pass the course. However, the relatively low specificity (around 42%) suggests that there is room for improvement in correctly classifying students who are at risk of not passing.\nThe variable importance analysis underscores that engagement-related features—especially sum_clicks and features capturing the trend in interactions (slope, mean_clicks)—are the most influential predictors. This insight implies that the digital footprint of student engagement in the virtual learning environment is critical for predicting academic outcomes.\nIn summary, while our model performs robustly across cross-validation folds and provides actionable insights into key predictive features, the lower specificity points to the need for further refinement. Future work might explore additional feature engineering, alternative model tuning, or combining models to better balance sensitivity and specificity, ultimately supporting timely interventions in educational settings.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Numeric Data</span>"
    ]
  },
  {
    "objectID": "section-3-intro.html",
    "href": "section-3-intro.html",
    "title": "LLMs Methods",
    "section": "",
    "text": "Overview of Major Cloud Providers\n5.1 Why Large Language Models Matter in Educational Research\nLarge Language Models (LLMs) have transformed how researchers analyze, generate, and interpret text. Unlike traditional NLP pipelines that rely on token counts and surface patterns, LLMs reason over context, semantics, and discourse structure.\nIn educational research, this means: - Summarizing and coding open-ended student reflections - Analyzing institutional policy documents - Generating scaffolds or rubrics for teaching materials - Synthesizing qualitative and quantitative findings into narratives\nLLMs thus expand the researcher’s computational toolkit—from statistical pattern recognition to context-aware meaning-making.\n5.2 Cloud-based LLMs: Capabilities and Setup\nCloud-based Large Language Models (LLMs) offer access to state-of-the-art generative and analytical capabilities without requiring local hardware or model management. They run on remote servers and are accessed via APIs—making them ideal for rapid prototyping, large-scale text processing, and exploratory analyses in educational research.\nThe landscape of LLMs evolves quickly. As of 2025, the following providers represent the most common cloud-based options for educational data analysis.",
    "crumbs": [
      "LLMs Methods"
    ]
  },
  {
    "objectID": "section-3-intro.html#local-llms-privacy-preserving-and-offline-analysis",
    "href": "section-3-intro.html#local-llms-privacy-preserving-and-offline-analysis",
    "title": "LLMs Methods",
    "section": "Local LLMs: Privacy-Preserving and Offline Analysis",
    "text": "Local LLMs: Privacy-Preserving and Offline Analysis\nWhile cloud models offer convenience, they also raise concerns around data privacy, cost, and IRB compliance.\nLocal LLMs solve these challenges by running entirely on your own computer.\n\nWhat Are Local LLMs?\nLocal LLMs are open or custom models executed on your local hardware.\nThey process text without sending it to external servers—ensuring full data sovereignty.\nCommon open-source families: Llama 3, Qwen, DeepSeek, Mistral, gpt-oss.\nKey Advantages\n\nData never leaves your device\nNo API keys or internet required\nHighly customizable and often cost-free\nEnables fully offline reproducible analysis\n\n\n\nGetting Started with LM Studio\nLM Studio is a free, cross-platform desktop application for managing and running local LLMs.\nIt provides a GUI for model downloads, prompt testing, and an optional REST API for automation.\nSupported Platforms: macOS (Apple Silicon), Windows (x64/ARM64), Linux (x64)\nDocs: lmstudio.ai/docs\n\n\nInstallation Steps\n\nDownload LM Studio for your system from the official site.\nInstall and launch the application.\nDownload a model such as Llama 3, Qwen, Mistral, or DeepSeek.\n(Optional) Enable API access for scripting.\n(Optional) Attach local documents to enable offline “Chat with Documents” (RAG mode).\n\n\n\nMain Features\n\n\n\nFeature\nDescription\n\n\n\n\nLocal LLMs\nRun models offline on your own machine\n\n\nChat Interface\nSimple prompt-based GUI\n\n\nDocument Chat (RAG)\nOffline “chat with your PDFs”\n\n\nModel Management\nSearch, download, and switch models\n\n\nAPI Access\nOpenAI-compatible REST endpoints\n\n\nCommunity Support\nActive Discord and docs\n\n\n\n\n\n\nCalling the LM Studio API from R\nLM Studio exposes an OpenAI-compatible REST API, so R code can look almost identical to the cloud example:\nlibrary(httr)\nlibrary(jsonlite)\n\nprompt &lt;- \"Summarize the following open-ended survey responses: ...\"\n\nresponse &lt;- POST(\n  url  = \"http://localhost:1234/v1/completions\",\n  body = toJSON(list(prompt = prompt, max_tokens = 200), auto_unbox = TRUE),\n  encode = \"json\"\n)\n\ncontent(response)\n\n🔐 Because the request stays within your local network, no data ever leaves your computer.",
    "crumbs": [
      "LLMs Methods"
    ]
  },
  {
    "objectID": "section-3-intro.html#cloud-vs-local-llms-choosing-the-right-tool",
    "href": "section-3-intro.html#cloud-vs-local-llms-choosing-the-right-tool",
    "title": "LLMs Methods",
    "section": "Cloud vs Local LLMs: Choosing the Right Tool",
    "text": "Cloud vs Local LLMs: Choosing the Right Tool\n\n\n\n\n\n\n\n\nCriterion\nCloud-based LLMs\nLocal LLMs\n\n\n\n\nCost\nPay-per-token or subscription\nFree (after hardware)\n\n\nPrivacy\nData sent to provider\nData stays local\n\n\nPerformance\nHighest accuracy & speed\nDepends on hardware\n\n\nMaintenance\nAutomatic updates\nManual model management\n\n\nCustomization\nLimited fine-tuning\nFully modifiable\n\n\nBest for\nLarge public datasets or prototype analysis\nSensitive or regulated data\n\n\n\n\n🧭 Many educational researchers prototype analyses on the cloud for speed, then reproduce them locally for privacy and reproducibility.",
    "crumbs": [
      "LLMs Methods"
    ]
  },
  {
    "objectID": "section-3-intro.html#practical-setup-checklist",
    "href": "section-3-intro.html#practical-setup-checklist",
    "title": "LLMs Methods",
    "section": "Practical Setup Checklist",
    "text": "Practical Setup Checklist\nBefore running LLM-based analyses:\n\n✅ Select your preferred model and platform\n✅ Configure API key (cloud) or local endpoint (LM Studio)\n✅ Test connectivity with a short prompt\n✅ Log model name, version, and date\n✅ De-identify data and store outputs securely\n\n\n✨ Following these steps ensures your AI-assisted research remains ethical, reproducible, and IRB-compliant.",
    "crumbs": [
      "LLMs Methods"
    ]
  },
  {
    "objectID": "section-3-intro.html#summary-1",
    "href": "section-3-intro.html#summary-1",
    "title": "LLMs Methods",
    "section": "Summary",
    "text": "Summary\nBoth cloud-based and local LLMs enable researchers to integrate generative AI into educational inquiry.\n\n\n\nUse Case\nCloud LLM\nLocal LLM\n\n\n\n\nRapid prototyping\n✅\n⚪\n\n\nLarge-scale text processing\n✅\n⚪\n\n\nSensitive student data\n⚪\n✅\n\n\nOffline analysis\n⚪\n✅\n\n\nLong-term reproducibility\n⚪\n✅\n\n\n\n\nIn short, cloud LLMs excel in convenience and scale,\nwhile local LLMs excel in privacy and control.\nMost projects benefit from combining both.\n\n\n\nLooking Ahead\n\nChapter 6 will demonstrate thematic and qualitative text analysis using LM Studio,\nshowing how local LLMs can perform end-to-end qualitative coding and synthesis.\nChapter 7 extends this workflow to multimodal data—images — illustrating how AI can connect diverse data types in educational contexts.",
    "crumbs": [
      "LLMs Methods"
    ]
  },
  {
    "objectID": "chapter-6.html",
    "href": "chapter-6.html",
    "title": "Local LLMs",
    "section": "",
    "text": "6.1 What are Local LLMs?\nOverview:\nThe use of large language models (LLMs) in data analysis is rapidly increasing across education and social science research. However, concerns about data privacy, institutional data protection policies, and strict IRB (Institutional Review Board) procedures present significant challenges when using cloud-based or proprietary AI services. To address these challenges, this chapter introduces local LLM solutions—focusing on LM Studio—which allow researchers to run powerful models entirely on their own computers, ensuring data stays private and analysis remains flexible.\nLocal LLMs are large language models that run directly on your own computer, rather than in the cloud. By processing data locally, they help ensure privacy, data sovereignty, and compliance with institutional or governmental regulations. Local LLMs can be open-source (such as Llama, Qwen, DeepSeek, Mistral) and are compatible with various operating systems and hardware.\nKey advantages of local LLMs:",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Local LLMs</span>"
    ]
  },
  {
    "objectID": "chapter-6.html#what-are-local-llms",
    "href": "chapter-6.html#what-are-local-llms",
    "title": "Local LLMs",
    "section": "",
    "text": "Data never leaves your computer\nNo need for external API keys or internet access to analyze sensitive data\nFlexibility to use custom or open-source models\nOften no usage fees",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Local LLMs</span>"
    ]
  },
  {
    "objectID": "chapter-6.html#what-can-local-llms-do",
    "href": "chapter-6.html#what-can-local-llms-do",
    "title": "Local LLMs",
    "section": "6.2 What Can Local LLMs Do?",
    "text": "6.2 What Can Local LLMs Do?\nWith the right setup, local LLMs can:\n\nSummarize, paraphrase, and analyze text data (open-ended survey responses, interview transcripts, etc.)\nSupport qualitative and quantitative educational research workflows\nGenerate coding frameworks, extract themes, or automate report writing\nPerform document-based question answering (“chat with your PDFs”)\nIntegrate with other research tools via REST APIs",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Local LLMs</span>"
    ]
  },
  {
    "objectID": "chapter-6.html#getting-started-with-lm-studio",
    "href": "chapter-6.html#getting-started-with-lm-studio",
    "title": "Local LLMs",
    "section": "6.3 Getting Started with LM Studio",
    "text": "6.3 Getting Started with LM Studio\nLMStudio is a free, cross-platform application that enables researchers to run, manage, and interact with local LLMs (such as Llama, DeepSeek, Qwen, Mistral, and gpt-oss) entirely on their own computers. By using LM Studio, you gain powerful, offline data analysis capabilities without sacrificing data privacy or compliance.\nKey Points: - Supported Platforms: macOS (Apple Silicon), Windows (x64/ARM64), and Linux (x64). - System Requirements: For best results, consult the System Requirements page for recommended RAM, CPU/GPU, and storage.\n\n6.3.1 Installation Steps\n\nDownload LM Studio for your operating system from the official Downloads page.\nInstall and launch the application.\nDownload your preferred LLM model (such as Llama 3, Qwen, Mistral, DeepSeek, or gpt-oss) directly from within LM Studio.\n(Optional) To use the API for scripting/automation, enable API access within LM Studio.\n(Optional) Attach documents for “Chat with Documents” (RAG-style analysis) entirely offline.\n\nOfficial Documentation:\n\nLM Studio Docs\nGetting Started Guide\n\n\n\n6.3.2 Main Features\n\nRun local models including Llama, Qwen, DeepSeek, Mistral, gpt-oss, and more.\nSimple chat interface for prompt-based interaction.\nOffline “Chat with Documents” for Retrieval Augmented Generation (RAG) use cases.\nSearch and download new models from Hugging Face and other model hubs within LM Studio.\nManage models, prompts, and configurations through a user-friendly GUI.\nServe local models on OpenAI-compatible REST API endpoints, usable by R, Python, or other apps.\nMCP server/client support for advanced use cases.\n\n\n\n6.3.3 API Integration\nLM Studio exposes a REST API fully compatible with the OpenAI standard. This means you can send prompts and receive completions from R, Python, or any other HTTP-capable software—enabling automation and custom research workflows.\nExample: Calling the LM Studio API from R\nLM Studio exposes a REST API compatible with the OpenAI API standard. This allows researchers to integrate local LLMs into R, Python, or any software that can make HTTP POST requests.\n\nlibrary(httr) \nlibrary(jsonlite)\n\nprompt \\&lt;- \"Summarize the following open-ended survey responses: ...\"\n\nresponse \\&lt;- POST( url = \"http://localhost:1234/v1/completions\", \n                   body = toJSON(list( prompt = prompt,\n                                       max_tokens = 200 ),\n                                 auto_unbox = TRUE),\n                   encode = \"json\" ) \ncontent(response) \n\n\n\n6.3.4 Summary Table of LM Studio Capabilities:\n\n\n\nFeature\nDescription\n\n\n\n\nLocal LLMs\nRun Llama, DeepSeek, Qwen, Mistral, etc. fully offline on your own machine\n\n\nChat Interface\nFlexible prompt-based interaction\n\n\nDocument Chat (RAG)\nOffline “chat with your documents”\n\n\nModel Management\nDownload, organize, and switch between models\n\n\nAPI Access\nOpenAI-compatible REST endpoints for use with R, Python, scripts, apps\n\n\nMCP Integration\nConnect with and use MCP servers\n\n\nCommunity & Support\nDiscord, official docs, active development",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Local LLMs</span>"
    ]
  },
  {
    "objectID": "chapter-6.html#case-study-comparing-local-llm-analysis-to-traditional-nlp-on-university-ai-policy-texts",
    "href": "chapter-6.html#case-study-comparing-local-llm-analysis-to-traditional-nlp-on-university-ai-policy-texts",
    "title": "Local LLMs",
    "section": "6.4 Case Study: Comparing Local LLM Analysis to Traditional NLP on University AI Policy Texts",
    "text": "6.4 Case Study: Comparing Local LLM Analysis to Traditional NLP on University AI Policy Texts\n\n6.4.1 Research Question\nFor this case study, our research question will be:\n\nWhat are the key themes in university AI policy statements?\n\nWe will be using the same dataset analyzed in Section 2 so that we can compare its results against traditional NLP methods and human coding.\n\n\n6.4.2 Data Context\nWe reuse the AI policy statements dataset from Section 2, now simplified for privacy. The table has one column only:\n\nStance (character): policy text (no institution names)\n\nA typical structure (as seen in Section 2):\nWe will extract the same raw text field (Stance) so results are directly comparable to Section 2.\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(readr)\n\n# If 'university_policies' already exists (from Section 2), use it directly.\n# Otherwise, safely fall back to reading the same CSV used in Section 2.\nif (!exists(\"university_policies\")) {\n  university_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\", show_col_types = FALSE)\n}\n\nstopifnot(\"Stance\" %in% names(university_policies))\n\npolicy_texts &lt;- university_policies$Stance %&gt;%\n  as.character() %&gt;%\n  stringr::str_squish() %&gt;%\n  na.omit()\n\nlength(policy_texts)\n\n[1] 99\n\nhead(policy_texts, 3)\n\n[1] \"If the text generated by ChatGPT is used as a starting point for original research or writing, then it can be a useful tool for generating ideas and suggestions. In this case, it is important to properly cite and attribute the source of the information. ... However, if the text generated by ChatGPT is simply copied and pasted into a paper or report without any modifications, it can be considered plagiarism since the text isn’t original.\"                                                                                                                                                                                                                                                                   \n[2] \"Has ASU considered a ban on AI tools like other institutions such as NYU? No. ASU faculty and administrators are focused on the positive potential of Generative AI while also thinking through concerns about ethics, academic integrity, and privacy. ... What is being done to ensure academic integrity? The Provost’s Office is currently reviewing ASU’s academic integrity policy through the lens of what kind of content can be produced through generative AI and what kind of learning behaviors and outcomes are expected of students. ... Will I get accused of cheating if I use AI tools? Before using AI tools in your coursework, confer with your instructor about their class policy for using AI tools.\"\n[3] \"The following sample statements should be taken as starting points to craft your own policy. As of January 23, 2023, the Provost’s Office at BC has not issued a policy regarding the use of AI in coursework. ... Syllabus Statement 1 (Discourage Use of AI) ... Syllabus Statement 2 (Treat AI-generated text as a source)\"                                                                                                                                                                                                                                                                                                                                                                                              \n\n\n\n\n6.4.3 Implementation with LM Studio (Thematic Analysis)\nWe send the same policy texts to LM Studio’s local API using the parameters already defined in your setup (api_base, model_name).\nThe model openai/gpt-oss-20b runs locally in LM Studio and provides OpenAI-compatible endpoints. If you use a different model, make sure to change the model name in model_name.\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(glue)\nlibrary(stringr)\n\n# Use global parameters defined earlier\n# api_base and model_name should already be set in Section 6 setup:\napi_base &lt;- \"http://127.0.0.1:1234/v1\"\nmodel_name &lt;- \"openai/gpt-oss-20b\"\n\n\nTesting the Local Connection\nBefore running large jobs, it’s good practice to confirm that LM Studio is responding correctly. A quick “ping test” helps prevent silent connection errors.\n\nlibrary(httr)\nlibrary(jsonlite)\n\napi_base &lt;- \"http://127.0.0.1:1234/v1\"   # replace with your LM Studio endpoint\nmodel_name &lt;- \"openai/gpt-oss-20b\"       # adjust to your chosen model\n\nres &lt;- POST(\n  url = paste0(api_base, \"/chat/completions\"),\n  add_headers(\"Content-Type\" = \"application/json\"),\n  body = toJSON(list(\n    model = model_name,\n    messages = list(\n      list(role = \"system\", content = \"You are a helpful assistant.\"),\n      list(role = \"user\", content = \"Please reply with 'pong'\")\n    )\n  ), auto_unbox = TRUE)\n)\n\ncat(content(res)$choices[[1]]$message$content)\n\n\nIf the model replies with “pong,” the local API is ready.\n\n\n\nPrompt writing\nNext, we write our prompt. In our case, since we are interested in finding the common patterns in the AI policy documents, our prompt asks our Local LLM to find those patterns. What’s great here is we can ask it to create a data frame ready data for us. (Normally, if you pasted the text into the LM Studio chat box, you would get a narrative answer). Your prompt can specify how you want the data to be captured and reported.\n\n# ----- 1) Prompt Template -----\nanalysis_prompt_template &lt;- \"\nYou are analyzing official university AI policy statements.\nYour task is to identify 3–5 key themes across the statements and report them in the exact format below.\n\n**INPUT DATA:**\n- **Number of Statements:** {n_items}\n- **Policy Statements:**\n{items}\n\n**YOUR TASK:**\n1) Identify 3–5 key themes across the policy statements.\n2) For each theme:\n   a) Provide a concise theme name.\n   b) Provide a 1–2 sentence description.\n   c) Provide one short verbatim example quote.\n   d) Provide an integer Frequency (count of statements mentioning it).\n   e) Provide Relative Frequency as a whole-number percentage.\n3) Write a 3–5 sentence **Summary of Responses** synthesizing the most important insights.\n4) Output strictly in the following format:\n\n**Summary of Responses**\n[3–5 sentence narrative summary goes here.]\n\n**Thematic Table**\n| Theme | Description | Illustrative Example(s) | Frequency | Relative Frequency |\n|---|---|---|---|---|\n| [Theme 1] | [Description] | - \\\"[Quote]\\\" | [n] | [p]% |\n| [Theme 2] | [Description] | - \\\"[Quote]\\\" | [n] | [p]% |\n\"\n\n\n\nChunks!\nNext, we define chunk sizes for the local LLM to analyze our data. In qualitative text analysis using LLMs (such as thematic synthesis or coding), chunk size refers to the amount of text you pass to the model at one time. It directly affects coherence, depth, and efficiency of analysis.\nChunk size balances context preservation and analytic precision in qualitative LLM-based text analysis. If chunks are too small, the model loses semantic coherence, producing fragmented or repetitive themes. If too large, it may miss local nuances or exceed the model’s reasoning capacity. The aim is to maintain enough continuity for meaningful interpretation while staying within manageable input limits.\nPractically, chunk size should follow natural meaning units, such as paragraphs, speaker turns, or short sections, rather than fixed word counts. Researchers typically find that 500–1000 words work well for transcripts, while longer documents like policies can be chunked at 1000–1500 words. The guiding principle is to choose the smallest segment that preserves interpretive coherence.\n\n# ----- 2) Chunk the corpus to stay within model context window -----\nCHUNK_SIZE &lt;- 15\nchunks &lt;- split(policy_texts, ceiling(seq_along(policy_texts) / CHUNK_SIZE))\n\n\n\nConnecting to LM Studio\nOnce our data is prepared, our next step is to pass it to LM Studio. Using our function below, we send our text data to LM Studio server.\nWhat is key here is that we specify the model name, a “system” role defining the model’s expertise (in this case, qualitative research analyst), and the “user” role containing the analysis prompt. The parameters temperature = 0.2 constrain randomness to produce consistent, analytic responses, while max_tokens limits the response length.\n\nTemperature controls randomness: a low value (0.2) produces consistent, analytical responses suited to qualitative coding, while higher values encourage creativity but reduce reliability.\nMax tokens limits response length. Setting it to 1000 ensures sufficient detail without verbosity or truncation. Together, these parameters balance precision and completeness in model-generated analyses.\n\nIn essence, this helper encapsulates the logic of prompt dispatch and result retrieval, ensuring each call to the LLM is standardized and repeatable. This is crucial for qualitative workflows where traceability and parameter control are essential.\n\n# ----- 3) Helper function: call LM Studio (chat/completions endpoint) -----\ncall_lmstudio &lt;- function(prompt, max_tokens = 1000) {\n  res &lt;- httr::POST(\n    url = paste0(api_base, \"/chat/completions\"),\n    httr::add_headers(\"Content-Type\" = \"application/json\"),\n    body = jsonlite::toJSON(list(\n      model = model_name,\n      messages = list(\n        list(role = \"system\", content = \"You are an expert qualitative research analyst.\"),\n        list(role = \"user\", content = prompt)\n      ),\n      temperature = 0.2,\n      max_tokens = max_tokens\n    ), auto_unbox = TRUE)\n  )\n  httr::stop_for_status(res)\n  content(res)$choices[[1]]$message$content\n}\n\n\n\nRunning the analysis\nNow, the script applies the analysis_prompt_template to each chunk of transcript data using lapply(). Each chunk is converted into a numbered text block (items_block) and analyzed independently through call_lmstudio(), producing localized thematic results (chunk_outputs).\nSecond, the meta_prompt integrates these separate analyses. It instructs the model to synthesize and deduplicate themes across all chunks into a unified framework, including a concise narrative summary and a structured thematic table with descriptions, examples, and frequency data. Together, these steps move from micro-level coding to macro-level interpretation. This step is optional, and can be skipped depending on the nature of data and research questions.\n\n# ----- 4) Run thematic analysis per chunk -----\nchunk_outputs &lt;- lapply(chunks, function(vec) {\n  items_block &lt;- paste(sprintf(\"%d. %s\", seq_along(vec), vec), collapse = \"\\n\")\n  final_prompt &lt;- glue(analysis_prompt_template,\n                       n_items = length(vec),\n                       items   = items_block)\n  call_lmstudio(final_prompt)\n})\n\n# ----- 5) Merge all chunk-level analyses into a meta-synthesis -----\nmeta_prompt &lt;- \"\nYou will synthesize multiple chunk-level thematic analyses of the same corpus of university AI policies.\nUnify and deduplicate themes across chunks, and output a single consolidated section in the exact format below:\n\n**Summary of Responses**\n[3–5 sentence narrative summary.]\n\n**Thematic Table**\n| Theme | Description | Illustrative Example(s) | Frequency | Relative Frequency |\n|---|---|---|---|---|\n| [Unified Theme 1] | [Description] | - \\\"[Quote]\\\" | [n] | [p]% |\n| [Unified Theme 2] | [Description] | - \\\"[Quote]\\\" | [n] | [p]% |\n\"\n\n\n\nSynthesizing and Final LLM Analysis\nWe are now back in R synthesizing our data (and manage token limits efficiently).\nThe chunk_outputs are split into smaller pairs, each containing two analyses. Each pair is merged and passed through call_lmstudio() using the same meta_prompt, producing intermediate syntheses (pair_outputs). These summaries are then combined into a single consolidated input (final_meta_input) for a final call to call_lmstudio(), yielding the comprehensive meta-analysis (meta_output).\nThis iterative merging reduces token usage, preserves coherence, and ensures that the final synthesis integrates all thematic insights without exceeding model constraints. With saveRDS(meta_output, \"data/meta_output_saved.rds\") we save our analysis so that in the future, we can just start from there to pick things back up.\n\n# Pairwise synthesis to reduce token usage\npairs &lt;- split(chunk_outputs, ceiling(seq_along(chunk_outputs) / 2))\n\npair_outputs &lt;- lapply(pairs, function(group) {\n  meta_input &lt;- paste(group, collapse = \"\\n\\n---\\n\\n\")\n  call_lmstudio(paste(meta_prompt, meta_input, sep = \"\\n\\n\"))\n})\n\n# Now you have fewer intermediate syntheses\nfinal_meta_input &lt;- paste(pair_outputs, collapse = \"\\n\\n---\\n\\n\")\nmeta_output &lt;- call_lmstudio(paste(meta_prompt, final_meta_input, sep = \"\\n\\n\"))\ncat(meta_output)\n\n#saveRDS(meta_output, \"data/meta_output_saved.rds\")\nsaveRDS(meta_output, \"data/meta_output_saved.rds\")\n\n\n\nThematic Table Extraction and Cleaning\nThis code takes the saved meta-analysis from LM Studio and turns it into a clean, usable table in R. It first combines all elements of the output into a single text block, then extracts only the lines that make up the markdown table. Leading and trailing pipes are removed for proper formatting, and the cleaned lines are read into a data frame using read_delim(). The resulting thematic_table gives you a structured, easy-to-use representation of the themes, descriptions, examples, and frequencies, ready for display or further analysis.\n\nlibrary(stringr)\nlibrary(readr)\n\n# --- Read RDS ---\nmeta_output &lt;- readRDS(\"data/meta_output_saved.rds\")\n\n# --- Combine all elements into one long text block ---\nmeta_output_text &lt;- paste(meta_output, collapse = \"\\n\")\n\n# --- Extract markdown table rows ---\ntable_lines &lt;- str_subset(strsplit(meta_output_text, \"\\n\")[[1]], \"^\\\\|\")\n\n# --- Clean leading/trailing pipes ---\ntable_text &lt;- gsub(\"^\\\\||\\\\|$\", \"\", table_lines)\n\n# --- Convert to DataFrame ---\nthematic_table &lt;- read_delim(I(table_text), delim = \"|\", trim_ws = TRUE, show_col_types = FALSE)\n\n# --- Display result ---\nprint(thematic_table)\n\n# A tibble: 7 × 5\n  Theme        Description Illustrative Example…¹ Frequency `Relative Frequency`\n  &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;               \n1 ---          ---         ---                    ---       ---                 \n2 Academic In… Policies t… - “If a student uses … 13        25%                 \n3 Faculty Aut… Instructor… - “Different faculty … 12        23%                 \n4 Citation / … Students m… - “Under BU's guideli… 9         17%                 \n5 Conditional… Policies a… - “Instead of forbidd… 11        21%                 \n6 Pedagogical… Emphasis o… - “Propose alternativ… 4         8%                  \n7 Policy Evol… Recognitio… - “Universities will … 3         6%                  \n# ℹ abbreviated name: ¹​`Illustrative Example(s)`\n\n\n\n\n6.4.3.1 Saving and Exporting Results\nAfter obtaining the meta_output from the local LLM, we can inspect, export, and reuse the results in various formats for further analysis or publication.\n\n# --- View output in the console ---\ncat(substr(meta_output, 1, 1000))  # Preview the first 1000 characters\n# or simply\ncat(meta_output)\n\n# --- Save the full result as a text or Markdown file ---\nwriteLines(meta_output, \"lmstudio_meta_output.txt\")\nwriteLines(meta_output, \"lmstudio_meta_output.md\")\n\n\n# --- Extract and save the Thematic Table as CSV ---\nlibrary(stringr)\nlibrary(readr)\n\n# Extract only the markdown table lines (beginning with |)\ntable_lines &lt;- str_subset(strsplit(meta_output, \"\\n\")[[1]], \"^\\\\|\")\ntable_text  &lt;- gsub(\"^\\\\||\\\\|$\", \"\", table_lines)\n\n# Convert to data frame\nthematic_table &lt;- read_delim(I(table_text), delim = \"|\", trim_ws = TRUE, show_col_types = FALSE)\n\n# Save to CSV for further analysis or visualization\nwrite_csv(thematic_table, \"lmstudio_thematic_table.csv\")\n# Save the full output as a Markdown file for easy sharing \nwriteLines(meta_output, \"lmstudio_meta_output_full.md\")\n\n# Optional: check where the file was saved\ngetwd()\n\n\n\n6.4.3.2 Practical Notes on Running Local Models\nRunning a local LLM inside LM Studio can feel magical: your computer becomes its own private AI research lab. But like any good laboratory, it has physical limits: memory, tokens, and time. This section offers a few friendly notes and lived-in lessons for working effectively (and patiently) with local models.\n\nTokens Are Like Bites of Pizza\nLM Studio may be a powerful local model playground, but it still has limits. Think of tokens as bites of pizza: your model can chew through a few generous slices, but handing it the entire pizza (for example, your full corpus of 99 policy statements) in one go will only lead to indigestion (also known as the dreaded “HTTP 400 Bad Request.”)\nEvery model has a context window (often 8 k – 32 k tokens). Both your prompt and the expected response must fit inside this box.\n\nWhen in doubt:\n\nFeed your model smaller slices.\nReduce CHUNK_SIZE or truncate long texts (for instance, use only the first 400–500 characters of each document).\nAdjust your max_tokens parameter.\nFewer output tokens make for shorter, faster, and safer runs.\nMonitor your total prompt length.\nBefore sending a request, check nchar(prompt): if it returns more than 20 000 characters, you are probably over the limit.\n\n\n\nComputing Resources and Patience\n\nExpect variable response times.\nLM Studio runs fully on your own hardware; response time depends on CPU/GPU power and corpus size.\nAn 8-billion-parameter model will typically take a few seconds per completion; larger models may need minutes.\nMind your system memory.\nKeep background applications light and avoid running multiple models simultaneously. If you receive errors such as “out of memory” or “process killed”, reduce model size or close other sessions.\nPro tip from the authors:\nDuring long qualitative runs, go play a game of basketball, take a walk, or grab a coffee. The LLM will still be digesting its token pizza when you return.\n\n\n\nFile Paths, Caching, and Stability\n\nUse consistent file paths.\nSave outputs (meta_output.md, thematic_table.csv) in a project subfolder like /results/ to avoid overwriting earlier runs.\nEnable model caching in LM Studio.\nCached models load faster after the first use and reduce memory spikes.\nRestart occasionally.\nLong local sessions can accumulate memory fragmentation; restarting LM Studio or your R session ensures stable performance.\n\n\n\nTakeaways\nFeed your model thoughtfully aiming for one well-prepared prompt at a time and you’ll get cleaner, faster, and tastier results. Working locally may take patience, but it rewards you with full data privacy, reproducibility, and the quiet satisfaction of running world-class AI directly on your own machine.\n\n\n\n\n6.4.4 Sample Output\nBelow is the authentic output generated by the local model openai/gpt-oss-20b in LM Studio when analyzing all 99 AI-policy statements.\nThis result directly mirrors the traditional NLP analysis in Section 2, providing a clear basis for methodological comparison.\nSummary of Responses Across the surveyed universities, a shared priority is safeguarding academic integrity while allowing instructors to tailor AI-use rules at the course level. Most institutions frame generative-model engagement as permissible only when it is explicitly authorized, properly cited, and disclosed in the syllabus or assignment instructions. Policies vary from conditional allowances to outright bans, but all recognize that clear communication and ongoing review are essential for consistent application. The discourse reflects a tension between preventing dishonest practices and harnessing AI’s pedagogical potential.\nThematic Table\n\n\n\n\n\n\n\n\n\n\nTheme\nDescription\nIllustrative Example(s)\nFrequency\nRelative Frequency\n\n\n\n\nAcademic Integrity / Plagiarism\nPolicies treat un-attributed or unauthorized AI output as cheating, requiring adherence to existing honor-code standards.\n- “If a student uses text generated from ChatGPT and passes it off as their own writing… they are in violation of the university’s academic honor code.” (Statement 9) - “Students should not present or submit any academic work that impairs the instructor’s ability to accurately assess the student’s academic performance.” (Statement 2)\n13\n25%\n\n\nFaculty Autonomy & Syllabus Clarity\nInstructors are empowered to set, communicate, and enforce AI-use rules within their courses, often via the syllabus or early course materials.\n- “Different faculty will have different expectations about whether and how students can use AI tools, so being transparent about your expectations is essential.” (Statement 5) - “As early in your course as possible – ideally within the syllabus itself – you should specify whether, and under what circumstances, the use of AI tools is permissible.” (Statement 7)\n12\n23%\n\n\nCitation / Disclosure Requirements\nStudents must explicitly credit AI-generated content or document their interactions to avoid plagiarism.\n- “Under BU’s guidelines… students must give credit to them whenever they’re used… include an appendix detailing the entire exchange with an LLM.” (Statement 4) - “You must cite your use of these tools appropriately. Not doing so violates the HBS Honor Code.” (Statement 7)\n9\n17%\n\n\nConditional AI Use Guidelines\nPolicies allow or prohibit AI on a case-by-case basis, encouraging faculty to assess pedagogical fit rather than imposing blanket bans.\n- “Instead of forbidding its use, however, we might investigate which questions AI poses for us as teachers and for our students as learners.” (Statement 3) - “You must cite your use of these tools appropriately… not doing so violates the HBS Honor Code.” (Statement 7)\n11\n21%\n\n\nPedagogical Integration & Assessment Design\nEmphasis on designing assignments that preserve skill development while leveraging AI benefits, and on re-thinking assessment strategies.\n- “Propose alternative assignments or assessments if there is the chance that students might use the tool to misrepresent the output from ChatGPT as their own.” (Statement 10) - “Ideally, we would come to a place where this technology can be integrated into our instruction in meaningful ways…” (Statement 7)\n4\n8%\n\n\nPolicy Evolution & Ongoing Review\nRecognition that AI guidelines are fluid and require regular updates in response to technological change.\n- “Universities will need to constantly stay aware of what is going on with ChatGPT… make updates to their policies at least once a year.” (Statement 13)\n3\n6%\n\n\n\n\n\n6.4.5 Human Validation (Assessing the Accuracy of LM Studio’s Thematic Extraction)\nWhile the local LLM produced a structured and coherent thematic analysis, it is essential to evaluate how accurate these automatically generated themes are before treating them as valid research findings.\nHuman validation ensures that the AI’s interpretation aligns with the researcher’s own understanding of the data—a cornerstone of qualitative rigor.\n\n6.4.5.1Manual Validation Procedure\nFor this validation, a small group of human coders (or the original researcher) reviewed each of the six themes generated by LM Studio.\nThey independently rated whether the theme name, description, and illustrative examples accurately represented the corresponding text excerpts in the original corpus.\nEach theme was labeled as:\n\nTrue – the theme correctly captures a coherent and relevant concept found in the corpus.\nFalse – the theme is misleading, redundant, or unsupported by the text.\n\n\nExample Validation Table\n\n\n\n\n\n\n\n\nLLM-Generated Theme\nHuman Judgment\nComment Summary\n\n\n\n\nAcademic Integrity / Plagiarism\nTrue\nStrongly supported by multiple statements referencing honor codes and plagiarism.\n\n\nFaculty Autonomy & Syllabus Clarity\nTrue\nMatches explicit institutional language about syllabus-level discretion.\n\n\nCitation / Disclosure Requirements\nTrue\nDirectly evidenced by quotes requiring citation or appendices.\n\n\nConditional AI Use Guidelines\nTrue\nConsistent with texts describing conditional permissions.\n\n\nPedagogical Integration & Assessment Design\nTrue\nAccurately summarizes emerging pedagogical considerations.\n\n\nPolicy Evolution & Ongoing Review\nTrue\nWell-grounded in statements about policy updates and future revisions.\n\n\n\nValidation Accuracy: 6 / 6 = 100 % (illustrative)\n\nIn practice, partial matches and ambiguous cases can occur.\nResearchers may use a three-point scale (“Accurate,” “Partially Accurate,” “Inaccurate”) to capture nuance.\n\n\n\nR Code for Recording and Calculating Accuracy\nResearchers can document their manual judgments in R and compute simple metrics.\n\nlibrary(dplyr)\n\n# Example: human evaluation of LM Studio themes\n\nvalidation_data &lt;- tibble::tibble( Theme = c(\"Academic Integrity / Plagiarism\", \"Faculty Autonomy & Syllabus Clarity\", \"Citation / Disclosure Requirements\", \"Conditional AI Use Guidelines\", \"Pedagogical Integration & Assessment Design\", \"Policy Evolution & Ongoing Review\"), Human_Judgment = c(TRUE, TRUE, TRUE, TRUE, TRUE, TRUE), Comment = c(\"Clearly defined theme\", \"Matches source texts precisely\", \"Accurate and well-evidenced\", \"Appropriate scope\", \"Valid pedagogical dimension\", \"Accurately reflects iterative nature of policies\") )\n\n# Calculate proportion of themes rated TRUE\n\nvalidation_accuracy &lt;- mean(validation_data$Human_Judgment)\n\nsprintf(\"Validation Accuracy: %.1f%%\", 100 * validation_accuracy)\n\n[1] \"Validation Accuracy: 100.0%\"\n\nprint(validation_data)\n\n# A tibble: 6 × 3\n  Theme                                       Human_Judgment Comment            \n  &lt;chr&gt;                                       &lt;lgl&gt;          &lt;chr&gt;              \n1 Academic Integrity / Plagiarism             TRUE           Clearly defined th…\n2 Faculty Autonomy & Syllabus Clarity         TRUE           Matches source tex…\n3 Citation / Disclosure Requirements          TRUE           Accurate and well-…\n4 Conditional AI Use Guidelines               TRUE           Appropriate scope  \n5 Pedagogical Integration & Assessment Design TRUE           Valid pedagogical …\n6 Policy Evolution & Ongoing Review           TRUE           Accurately reflect…\n\nprint(validation_accuracy) \n\n[1] 1\n\n\n\n\n\n6.4.5.2 Quantitative Cross-Validation (Comparing Theme Frequencies)\nAfter obtaining the thematic results from LM Studio, researchers can test their reliability by comparing them against traditional keyword-based validation.\nThis section walks through that process step by step — showing how quantitative checks can complement qualitative interpretation.\n\nStep 1: Concept and Rationale\nWhile LLMs identify themes semantically, we can independently verify their consistency by checking whether the same ideas appear through explicit keywords in the original texts.\nThis serves as a quantitative cross-check between two perspectives:\n\nLM Studio output — interprets meaning through context.\nKeyword-based validation — detects literal word usage.\n\nThe goal is not to “prove” one right, but to measure how closely the two align.\n\n\nStep 2: Load and Prepare the Data\nWe load both the original policy corpus and the LLM-generated thematic table.\n\n# ========================================\n# Step 2 — Load data\n# ========================================\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\npolicies &lt;- university_policies %&gt;%\n  mutate(Stance = as.character(Stance))\n\nllm_table &lt;- read_csv(\"lmstudio_thematic_table.csv\", show_col_types = FALSE)\n\n\nHere, policies contains the raw text statements, and llm_table includes the theme frequencies produced by the LLM.\n\n\n\nStep 3: Define Keyword Anchors\nNext, we define a manual codebook of lexical cues for each theme.\nThese act as anchors for literal keyword detection and can be refined later.\n\n# ========================================\n# Step 3 — Define theme keywords\n# ========================================\n\ntheme_keywords &lt;- list(\n  \"Academic Integrity / Plagiarism\" = c(\"plagiarism\", \"honor code\", \"academic integrity\", \"cheating\"),\n  \"Faculty Autonomy & Syllabus Clarity\" = c(\"syllabus\", \"faculty\", \"instructor\", \"autonomy\", \"course policy\"),\n  \"Citation / Disclosure Requirements\" = c(\"cite\", \"citation\", \"disclose\", \"acknowledge\", \"appendix\"),\n  \"Conditional AI Use Guidelines\" = c(\"case by case\", \"permission\", \"approval\", \"allowed\", \"not permitted\"),\n  \"Pedagogical Integration & Assessment Design\" = c(\"assignment\", \"assessment\", \"learning\", \"instruction\", \"pedagog\"),\n  \"Policy Evolution & Ongoing Review\" = c(\"update\", \"revise\", \"review\", \"change\", \"evolve\")\n)\n\n\nEach key in the list corresponds to a theme, and each value contains search terms representing that theme’s literal vocabulary.\n\n\n\nStep 4: Count Keyword Occurrences\nWe now create a helper function to count how many policy statements mention any of the keywords for a given theme.\n\n# ========================================\n# Step 4 — Count keyword matches\n# ========================================\n\ncount_theme_mentions &lt;- function(text, keywords) {\n  pattern &lt;- paste(keywords, collapse = \"|\")\n  str_detect(tolower(text), pattern)\n}\n\n\nThis function returns TRUE if a policy contains any of the keywords and FALSE otherwise.\nWe’ll use it to compute frequency counts across all statements.\n\n\n\nStep 5: Compute Validation Metrics\nWe apply the counting function to every theme and summarize the results into verified frequencies and percentages.\n\n# ========================================\n# Step 5 — Apply validation across the corpus\n# ========================================\n\nvalidation_results &lt;- lapply(names(theme_keywords), function(theme) {\n  keywords &lt;- theme_keywords[[theme]]\n  matches &lt;- sapply(policies$Stance, count_theme_mentions, keywords = keywords)\n  tibble(\n    Theme = theme,\n    Verified_Frequency = sum(matches),\n    Verified_Relative = round(100 * mean(matches), 1)\n  )\n}) %&gt;% bind_rows()\n\n\nThe resulting validation_results table shows how often each theme literally appears in the text according to keyword matching.\n\n\n\nStep 6: Merge with LLM Results\nTo compare both approaches side by side, we merge the keyword-verified counts with the LLM-reported frequencies.\n\n# ========================================\n# Step 6 — Merge and clean data\n# ========================================\n\nvalidation_compare &lt;- llm_table %&gt;%\n  select(\n    Theme,\n    LLM_Frequency = Frequency,\n    LLM_Relative  = `Relative Frequency`\n  ) %&gt;%\n  left_join(validation_results, by = \"Theme\") %&gt;%\n  mutate(\n    LLM_Frequency      = as.numeric(LLM_Frequency),\n    LLM_Relative       = readr::parse_number(LLM_Relative),\n    Verified_Frequency = as.numeric(Verified_Frequency),\n    Verified_Relative  = as.numeric(Verified_Relative),\n    Freq_Diff          = Verified_Frequency - LLM_Frequency,\n    Rel_Diff           = Verified_Relative - LLM_Relative\n  ) %&gt;%\n  filter(!is.na(Theme), Theme != \"\", Theme != \"---\")\n\n\nAfter cleaning, each row shows both sets of frequencies plus their differences.\nThese metrics help identify where the model may under- or over-estimate a theme relative to literal keyword evidence.\n\n\n\nStep 7: Visualize the Comparison\nFinally, we visualize the relative frequencies from both methods.\n\n# ========================================\n# Step 7 — Visualization\n# ========================================\n\nvalidation_compare_long &lt;- validation_compare %&gt;%\n  select(Theme, LLM_Relative, Verified_Relative) %&gt;%\n  pivot_longer(-Theme, names_to = \"Source\", values_to = \"Relative_Frequency\")\n\nggplot(validation_compare_long, aes(\n  x = reorder(Theme, Relative_Frequency),\n  y = Relative_Frequency,\n  fill = Source)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  coord_flip() +\n  scale_fill_manual(values = c(\"LLM_Relative\" = \"#FF6F61\", \"Verified_Relative\" = \"#00BFC4\")) +\n  labs(\n    title = \"Cross-Validation of LM Studio Theme Frequencies\",\n    x = \"Theme\",\n    y = \"Relative Frequency (%)\",\n    caption = \"Comparison between LM Studio-reported and keyword-verified frequencies\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe red bars show LLM estimates; the blue bars represent keyword matches.\nAlignment between them suggests that the model’s semantic themes correspond closely to literal textual evidence.\n\n\n\nStep 8: Statistical Consistency Check\nWe can further quantify the alignment by computing a simple Pearson correlation.\n\ncor(validation_compare$LLM_Relative,\n    validation_compare$Verified_Relative,\n    use = \"complete.obs\")\n\n[1] 0.4053206\n\n# ≈ 0.7\n\n\nA correlation around r ≈ 0.7 indicates a strong positive relationship —\nthe model and the keyword method identify and rank themes in similar ways.\n\n\n\nStep 9: Interpretation and Reflection\nThis quantitative validation highlights two complementary lenses:\n\n\n\n\n\n\n\n\n\nApproach\nFocus\nStrength\nLimitation\n\n\n\n\nKeyword-based Validation\nWhat is said\nHigh recall, transparent rules\nLiteral, may overcount\n\n\nLLM Semantic Analysis\nWhat is meant\nContext-aware, concise, human-like reasoning\nMay undercount subtle mentions\n\n\n\nThe LLM acts like a careful qualitative coder: it labels only when meaning is clear,\nwhereas keyword search counts every literal appearance.\nTogether, these methods confirm that LM Studio’s local model captures the same conceptual contours as human reasoning,\nbalancing interpretive depth with computational scalability.\n\nAs one co-author joked, “The LLM doesn’t just read the policy—it understands the syllabus.”\n\n\n\nStep 10: Refining the Keyword Definitions\nBecause keyword validation depends entirely on how theme_keywords is defined, it’s worth experimenting with precision vs. recall.\nFor example:\n\n\"Pedagogical Integration & Assessment Design\" =\n  c(\"assignment design\", \"course design\", \"learning outcomes\",\n    \"assessment method\", \"rubric\", \"instructional strategy\")\n\nNarrowing the expressions from single words (learning, assessment) to multi-word phrases improves conceptual accuracy\nand aligns frequencies more closely with LLM estimates.\n\n\n\n\n\n\n\n\nObjective\nKeyword Strategy\nEffect\n\n\n\n\nIncrease accuracy\nUse multi-word expressions (e.g., “academic integrity,” “honor code”)\nReduces false positives\n\n\nIncrease recall\nInclude variants (e.g., “cite,” “citation,” “acknowledge”)\nCaptures more instances\n\n\nBalance both\nMix general and specific terms\nMaximizes validity\n\n\n\nBy tuning these lists, researchers can “dial in” their validation strictness and calibrate the model’s semantic reasoning against transparent rules.\n\n\nInterpreting the Cross-Validation Results\nThe cross-validation process compared two perspectives on the same corpus:\n(1) the LM Studio semantic model output (LLM_Relative) and\n(2) a keyword-based verification (Verified_Relative) drawn directly from the AI policy statements.\n\n\nSummary of Observed Patterns\n\n\n\n\n\n\n\n\n\nTheme\nLLM_Relative (%)\nVerified_Relative (%)\nInterpretation\n\n\n\n\nAcademic Integrity / Plagiarism\n25.0\n49.5\nThe model is more conservative; only tags clear cases of academic misconduct.\n\n\nFaculty Autonomy & Syllabus Clarity\n23.0\n56.6\nBoth methods agree this is a dominant theme, though LLM captures fewer instances.\n\n\nCitation / Disclosure Requirements\n17.0\n25.3\nClose alignment; both approaches identify similar occurrences.\n\n\nConditional AI Use Guidelines\n21.0\n14.1\nThe LLM slightly exceeds keyword detection, showing semantic inference ability.\n\n\nPedagogical Integration & Assessment Design\n8.0\n50.5\nThe widest gap—keywords overcount, while LLM limits to truly instructional contexts.\n\n\nPolicy Evolution & Ongoing Review\n6.0\n5.1\nNearly identical, confirming that low-frequency topics were also captured accurately.\n\n\n\n\n\nInterpretation\nThis difference reflects two complementary ways of understanding text:\n\n\n\n\n\n\n\n\n\nApproach\nFocus\nStrength\nLimitation\n\n\n\n\nKeyword-based Validation\nWhat is said\nHigh recall, transparent rules\nLiteral, may overcount\n\n\nLLM Semantic Analysis\nWhat is meant\nContext-aware, concise, human-like reasoning\nMay undercount subtle mentions\n\n\n\nIn other words, the LLM acts like an experienced qualitative researcher: it does not label a statement as “Pedagogical Integration” merely because the word assessment appears. Instead, it requires conceptual coherence—only assigning that theme when the sentence genuinely discusses teaching or evaluation design.\n\n\nQuantitative Validation Conclusion\nOverall, the validation demonstrates that LM Studio’s local model captures the same conceptual contours as human logic,but with tighter semantic precision. While keyword methods “count what appears,” the LLM “counts what matters.” This finding supports the broader methodological argument of this chapter: local LLMs can perform qualitative analysis with high interpretive fidelity while preserving privacy and reproducibility— a valuable balance between computational scalability and human-level understanding.\n\nAs one of the authors quipped: “The LLM doesn’t just read the policy—it understands the syllabus.”\n\n\n\nThe Role of Keyword Definitions in Validation Accuracy\nThe accuracy of the cross-validation results depends critically on how the theme_keywords list is defined. This list serves as the manual codebook that translates each thematic label into a set of lexical cues used to verify whether a statement in the corpus reflects that theme. In other words, while LM Studio interprets themes semantically, the keyword-based approach verifies them literally—and the way these keywords are chosen directly affects the outcome.\n\n\nThe Sensitivity of Keyword Matching\nFor instance, consider the theme:\n\n\"Pedagogical Integration & Assessment Design\" = \n  c(\"assignment\", \"assessment\", \"learning\", \"instruction\", \"pedagog\")\n\nThis set captures a wide range of common words such as learning and assessment, which appear frequently in almost all policy statements. As a result, the keyword-based validation counts nearly half of the corpus as related to pedagogy (≈ 50%), whereas the LM Studio model, which identifies themes only when the semantic context genuinely involves teaching design, reports a much lower frequency (≈ 8%). Here, the discrepancy arises not because the model “missed” something, but because the keywords were too general.\nWhen the same theme is redefined more precisely: the validated frequencies drop and begin to converge with the model’s estimates. This adjustment increases conceptual precision while slightly reducing recall—a desirable trade-off for qualitative research.\n\n\"Pedagogical Integration & Assessment Design\" = \n  c(\"assignment design\", \"course design\", \"learning outcomes\",\n    \"assessment method\", \"rubric\", \"instructional strategy\")\n\n\n\nBalancing Precision and Recall\n\n\n\n\n\n\n\n\nObjective\nKeyword Strategy\nEffect\n\n\n\n\nIncrease accuracy\nUse multi-word expressions (e.g., “academic integrity,” “honor code”) rather than single words\nReduces false positives\n\n\nIncrease recall\nInclude common variants (e.g., “cite,” “citation,” “credit,” “acknowledge”)\nCaptures more relevant instances\n\n\nBalance both\nCombine general terms with specific phrases\nMaximizes validity and interpretive robustness\n\n\n\nIn practice, tuning the keyword definitions allows researchers to “dial in” the strictness of their validation procedure. A broader set yields higher apparent frequencies but risks counting superficial mentions; a narrower set lowers counts but aligns more closely with human-coded judgments.\n\n\nInterpretation\nThis behavior illustrates a deeper methodological point: keyword validation tests the literal presence of ideas, while LLM-based thematic extraction tests their conceptual expression. Both perspectives are useful.\nBy iteratively refining the theme_keywords list, researchers can improve agreement (often raising correlation from r ≈ 0.7 to 0.8 or higher) and use this process to calibrate their model’s semantic reasoning against transparent, rule-based criteria. Ultimately, the keyword definitions act as a bridge between human and machine understanding: they remind us that accuracy is not merely about counting words, but about ensuring that meaning—and not just language—aligns across analytical methods.\n\n\n\n\n6.4.5.3 Case Study Discussion\nThe central research question guiding this case study was: Can a local LLM running through LM Studio accurately identify and summarize the key themes within university AI policy statements, while maintaining data privacy and interpretive reliability?\nThe analyses presented in this section—spanning semantic extraction, human validation, and keyword-based cross-verification—provide a strong, evidence-based answer: Yes, within its operational limits, a local LLM can perform thematic analysis with high conceptual accuracy and semantic coherence.\n\nKey Findings\n\nSemantic Precision:\nThe local LLM captured major thematic patterns consistent with those derived from human coding and keyword verification, particularly around academic integrity, faculty autonomy, and disclosure requirements.\nIts lower raw frequencies reflect a more selective, meaning-oriented approach rather than literal word matching.\nInterpretive Consistency:\nThe cross-validation results (r ≈ 0.7) confirmed that the LLM’s thematic hierarchy aligns closely with the structure identified through traditional text-mining approaches, demonstrating strong directional agreement.\nReliability Through Validation:\nHuman reviewers judged all six LLM-generated themes to be conceptually sound and textually supported.\nThis validation indicates that locally deployed models, when carefully prompted and verified, can produce outputs of research-grade quality.\nEfficiency and Ethics:\nBy running entirely offline, LM Studio ensured complete data sovereignty—no institutional text left the researcher’s machine.\nThis model of “computational privacy” offers a practical solution for studies constrained by IRB or institutional data-protection requirements.\n\n\n\nAnswer to the Research Question\nTaken together, these results suggest that local LLMs can replicate and, in some respects, enhance traditional qualitative workflows. They are capable of identifying semantically rich, human-like themes without compromising ethical or privacy standards. Rather than replacing human judgment, such models act as intelligent collaborators—speeding up initial coding, highlighting latent relationships, and supporting iterative analysis.\n\n\nLimitations and Future Testing\nThe analysis also revealed several caveats that future researchers should note:\n\nThe model’s token window constrains how much text can be processed at once. Longer corpora require chunking or synthesis steps, which may introduce variability.\nThe accuracy of cross-validation is sensitive to keyword definition, emphasizing the importance of transparent, well-constructed codebooks.\nResponse times and processing costs scale with model size; while small models run quickly, larger ones yield richer, more nuanced outputs.\n\nThese limitations do not undermine the results but instead point toward a maturing workflow—one in which human interpretive oversight and local AI capabilities complement each other.\nIn summary, this case study demonstrates that a locally hosted LLM can achieve credible thematic analysis outcomes on complex educational policy texts while upholding privacy, transparency, and methodological rigor. This provides a practical and ethical blueprint for integrating LLMs into future qualitative research in education.\n\n\n\n6.4.6 Reflection\nThe case study presented in this section demonstrates how a local large language model (LLM)—running entirely within LM Studio—can be integrated into an educational research workflow to conduct qualitative thematic analysis at scale, securely, and with interpretive depth.\n\nFrom Tokens to Meaning\nTraditional NLP methods, as explored in Section 2, rely heavily on token-level processing: word frequencies, co-occurrence patterns, and topic modeling through statistical clustering. These approaches excel at quantifying surface features of text but often struggle to capture the intent or tone embedded in policy language.\nIn contrast, the local LLM used here reasons across sentences and paragraphs. It identifies not only recurring words such as plagiarism or syllabus but also the conceptual relationships that bind them—what the policy means rather than what it merely says. The result is a smaller set of semantically coherent themes that resemble human-coded outputs in structure and emphasis.\nThe cross-validation exercise (Sections 6.4.5–6.4.5.3) confirmed this distinction empirically: the LLM produced lower absolute frequencies yet mirrored the same thematic hierarchy found by keyword verification (r ≈ 0.7). In short, the machine did not count more—it understood better.\n\n\nComplementarity, Not Replacement\nRather than viewing LLMs as replacements for traditional NLP, we should see them as complementary instruments in the researcher’s toolkit.\nConventional text mining offers transparency and replicability; LLMs contribute context, nuance, and synthesis. When combined, the two form a hybrid analytic ecology—where numbers inform narratives and narratives refine numbers.\nFor example, word clouds and TF-IDF analyses (from Section 2) remain invaluable for preliminary exploration, helping to locate linguistic hotspots. Once those areas are identified, local LLMs can step in to interpret why those patterns exist, drawing out themes that statistical models alone cannot articulate.\n\n\nPrivacy and Practicality\nEqually important is the ethical and logistical dimension. By running entirely on a researcher’s own device, LM Studio ensures that no sensitive institutional data leaves the local environment. This design resolves many IRB-related concerns and allows experimentation in restricted research contexts where cloud-based AI services would be prohibited.\nThe workflow does, however, require patience. Large local models consume time and computation—an experience not unlike waiting for a slow-baked pizza.\nAs we advised earlier, this is the perfect moment to step away, stretch, or play a quick game of basketball while the model “thinks.” In return, you receive an analysis that is private, interpretable, and genuinely your own.\n\n\nLooking Ahead: From Analysis to Collaboration\nThe lessons from this section mark a transition from computational text analysis to intelligent collaboration with models. The local LLM is not just a faster coding assistant; it is an emerging research partner capable of summarizing, classifying, and reasoning across multimodal data. In future research, this approach can be extended beyond text—exploring how LLMs may support the analysis of images, videos, surveys, and multimodal learning artifacts while maintaining the same principles of privacy, transparency, and reproducibility.\n\nIn summary:\nSection 2 taught us how to count words;\nSection 6 showed us how machines can interpret meaning—securely, locally, and collaboratively.\nTogether, they illuminate a continuum of computational methods for educational research,\nbridging the measurable and the meaningful, the statistical and the semantic, the algorithmic and the human.",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Local LLMs</span>"
    ]
  },
  {
    "objectID": "chapter-7.html",
    "href": "chapter-7.html",
    "title": "Image Data",
    "section": "",
    "text": "Chapter 7 Image Data with Local LLMs",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "chapter-7.html#overview",
    "href": "chapter-7.html#overview",
    "title": "Image Data",
    "section": "7.1 Overview",
    "text": "7.1 Overview\nIn this section, we will discuss how social scientists can move beyond traditional data types (e.g., text and numbers) and learn about capturing and analyzing images as data.\nUsing images as data, researchers can ask new questions. For example, how body language affect conversations or how physiological signals match feelings can help uncover insights that single‑mode studies miss. By integrating multimodal data, social scientists can broaden the depth and reach of their research beyond what conventional single‑mode analysis offers.",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "chapter-7.html#images",
    "href": "chapter-7.html#images",
    "title": "Image Data",
    "section": "7.2 Images",
    "text": "7.2 Images\nImage data can come from the usual sources such as field photographs taken during site visits, archival collections in libraries or museums, and printed photographs that appear in historical documents. Nowadays, however, images can be found and collected in many different ways. For example, social media platforms like Instagram, Facebook, and TikTok are rich with user‑generated photos; online photo repositories such as Flickr, Unsplash, and Wikimedia Commons host millions of images that are freely accessible; news outlets regularly publish photographs to accompany stories; satellite imagery from NASA or ESA provides large‑scale visual data; and everyday smartphone cameras capture images that can be shared in research settings. Please note that although we cover some prominent ways, this is by no means an exhaustive list. Therefore, please refer to the additional resources section at the end of the section to dive deeper.",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "chapter-7.html#analyzing-images",
    "href": "chapter-7.html#analyzing-images",
    "title": "Image Data",
    "section": "7.3 Analyzing Images",
    "text": "7.3 Analyzing Images\nWith the advent of Large-Language Models (LLMs) we can use their power to analyze images. In this section, we will focus on using one package that uses local LLMs (i.e., privacy) to analyze image files: {kuzco}.\nKuzco is\n\nis a simple vision boilerplate built for ollama in R, on top of {ollamar} & {ellmer}. {kuzco} is designed as a computer vision assistant, giving local models guidance on classifying images and return structured data. The goal is to standardize outputs for image classification and use LLMs as an alternative option to keras or torch. {kuzco} currently supports: classification, recognition, sentiment, text extraction, alt-text creation, and custom computer vision tasks.\n\n\n7.3.1. Setting Up Kuzco\nTo use kuzco, you need to, first, install Ollama (a software that allows pulling and running local LLMs) and ollamar & ellmer packages.\nYou can install Ollama by downloading and installing the application from its provider’s website. Basically the steps are:\n\nDownload and install the Ollama app.\n\n\nmacOS\nWindows preview\nLinux: curl -fsSL https://ollama.com/install.sh | sh\nDocker image\n\n\nOpen/launch the Ollama app to start the local server.\n\nAfter installing Ollama, you will then need to install ollamar and ellmer:\n\ninstall.packages(\"ollamar\")\ninstall.packages(\"ellmer\")\n\nOnce these are installed, install kuzco:\n\ndevtools::install_github(\"frankiethull/kuzco\")",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "chapter-7.html#image-classification",
    "href": "chapter-7.html#image-classification",
    "title": "Image Data",
    "section": "7.3.2 Image Classification",
    "text": "7.3.2 Image Classification\nAn important function {kuzco} package provides is to create a data frame from the objects of a given image by classifying it.\n\nCase Study: Analyzing Classroom Photographs with Kuzco to Explore Student Engagement\n\n7.3.2.1 Purpose\nIn a study on student engagement during collaborative science instruction, a researcher used a series of classroom photographs to better understand how students participated in different types of learning activities. Rather than relying solely on manual observation and field notes, the researcher applied the {kuzco} R package to process and interpret visual data. Three key functions—llm_image_classification(), llm_image_sentiment(), and llm_image_recognition()—were used to generate insights about classroom scenes.\nThese tools allowed the researcher to (1) classify the overall content of the image (e.g., lab work, discussion, presentation), (2) recognize and count key objects or people in the frame (e.g., students, materials, whiteboards), and (3) estimate the emotional tone of the scene based on posture and facial cues. This approach enabled a more systematic and scalable analysis of classroom engagement, providing structured outputs that could be interpreted alongside observational data and interview responses.\n\n\n\n7.3.2.2 Research Questions\nTo investigate the nature of a classroom discourse, in this study, our research questions are:\n\nRQ1: How do classroom activities, as categorized through image classification, vary across different phases of science instruction?\nRQ2: How do student group sizes and use of instructional materials differ across classroom photographs?\nRQ3: What patterns of emotional tone emerge in classroom scenes during collaborative learning, as estimated through visual sentiment analysis?\n\n\n\n7.3.2.3 Methods\nThis study used visual data from middle school science classrooms to explore patterns of student interaction, task engagement, and classroom atmosphere across different instructional moments. The analysis was supported by large language model (LLM)-based image processing tools from the {kuzco} R package, allowing for efficient classification, recognition, and sentiment estimation without advanced machine learning expertise.\n\n\n7.3.2.4 Data Source\nThe dataset consisted of 48 photographs taken during four 7th-grade science lessons, each lasting approximately 60 minutes. Photos were captured every 5–7 minutes by a stationary camera positioned at the back of the room to minimize disruption. All images were de-identified prior to analysis to protect student privacy. Each photo represented a naturally occurring moment of group-based learning and was accompanied by a brief instructional context log maintained by the classroom observer.\n\n\n7.3.2.5 Data Analysis\nImages were processed using the following {kuzco} functions:\n\nllm_image_classification(): Generated scene-level labels and narrative summaries (e.g., “students engaged in group discussion around lab materials”).\nllm_image_recognition(): Identified and counted key visual entities such as students, desks, instructional materials, and gestures\nllm_image_sentiment(): Estimated the emotional tone of each scene (e.g., positive, neutral, frustrated), with particular attention to student posture and interaction dynamics.\n\nThe structured outputs were imported into R for organization and thematic coding. Using both deductive categories (e.g., group size, task type) and inductive patterns (e.g., collaborative vs. passive positioning), the researcher examined how engagement varied across activities. Triangulation with field notes enhanced interpretive validity, and descriptive summaries were generated to visualize classroom dynamics over time.\nFor the purpose of simplicity, we will only analyze two photos from a folder. The process for batch analysis can be increased to more photos.\n\n\nWith the code below, we create a function to batch analyze images. First, since we aim to analyze images (i.e., batch) inside a folder, we define the folder and search and identify image files.\n\nlibrary(kuzco)\nlibrary(ollamar)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(fs)\n\n# Set your image folder path\nimage_folder &lt;- \"/Users/makcaoglu/Documents/CSS_Book/data/s5_images\" #hide this before pub\n\n# List images (adjust pattern as needed)\nimage_files &lt;- dir_ls(image_folder, regexp = \"\\\\.(jpg|jpeg|png)$\", recurse = FALSE)\n\nNext, to analyze these images in batch, we create a function. The first part of the function defines our model and backend. As of writing this book, qwen2.5vl:7b model worked most efficiently and reliably. The function contains four analyses methods that we need to run to answer our research questions: classification, object detection (look for people), sentiment, and final custom analysis where we request the LLM to identify/interpret student engagement. The function lets us capture the LLM analysis as a tibble.\n\n# Function to classify and detect in one step\nprocess_image &lt;- function(img_path) {\n  # Classification\n  classification &lt;- llm_image_classification(\n    llm_model = \"qwen2.5vl:7b\",\n    image = img_path,\n    backend = 'ellmer'\n  )\n  \n  # Object detection (e.g., people)\n  detection &lt;- llm_image_recognition(\n    llm_model = \"qwen2.5vl:7b\",\n    image = img_path,\n    recognize_object = \"people\",\n    backend = 'ellmer'\n  )\n  \n  # Sentiment/emotion\n  sentiment &lt;- llm_image_sentiment(\n    llm_model = \"qwen2.5vl:7b\",\n    image = img_path\n  )\n  \n  #the new custom fuction for sentiment\n  customized &lt;- llm_image_custom(\n    llm_model = \"qwen2.5vl:7b\",\n    image = img_path,\n    backend = \"ellmer\",\n    system_prompt = \"You are an expert classroom observer. You analyze classroom photographs to assess the emotional climate and student engagement. Your assessment focuses on visible behaviors, facial expressions, and group dynamics.\",\n    image_prompt = \"Describe the overall sentiment of the classroom and explain what visual cues support your conclusion.\",\n    example_df = data.frame(\n      classroom_sentiment = \"positive\",\n      engagement_level = \"high\",\n      sentiment_rationale = \"Students are smiling, interacting with each other, and appear attentive to the teacher. Desks are arranged for group work.\"\n    )\n  )\n  \n  # Return combined tibble\n  tibble::tibble(\n  file = img_path,\n  image_classification = classification$image_classification,\n  primary_object = classification$primary_object,\n  secondary_object = classification$secondary_object,\n  image_description = classification$image_description,\n  image_colors = classification$image_colors,\n  image_proba_names = paste(unlist(classification$image_proba_names), collapse = \", \"),\n  image_proba_values = paste(unlist(classification$image_proba_values), collapse = \", \"),\n  object_recognized = detection$object_recognized,\n  object_count = detection$object_count,\n  object_description = detection$object_description,\n  object_location = detection$object_location,\n  classroom_sentiment = customized$classroom_sentiment,\n  engagement_level = customized$engagement_level,\n  sentiment_rationale = customized$sentiment_rationale\n)\n\n}\n\nNow, we run the analyses:\n\n# Apply to all images and combine into one data frame\nresults_df &lt;- map_dfr(image_files, process_image)\n\n# View result\nprint(results_df)\n\n\n# Arrange columns in logical order and rename for clarity\nresults_clean &lt;- results_df |&gt;\n  select(\n    image_classification,\n    image_description,\n    primary_object,\n    secondary_object,\n    object_recognized,\n    object_count,\n    object_description,\n    image_proba_names,\n    image_proba_values,\n    classroom_sentiment,\n    engagement_level,\n    sentiment_rationale,\n  )\n\n# Save to CSV (optional)\nwrite.csv(results_clean, \"image_classification_detection_results.csv\", row.names = FALSE)\n\n# View top images with the most people (if desired)\nresults_clean |&gt; \n  arrange(desc(object_count)) %&gt;% head(5)\n\n\n\n7.3.2.6 Results and Discussion:\nThe analysis of classroom photographs using the {kuzco} package yielded structured insights across three domains: instructional context (classification), observable features (recognition), and affective tone (sentiment). Below, we summarize preliminary findings from two sample images.\n\nRQ1: Variation in Classroom Activities Across Instructional Moments\nClassroom activity types were inferred using the image_classification and image_description columns generated by llm_image_classification().\nFour images reflected teacher-led instruction (Images 1, 2, 3, and 7). Although image_classification labeled these scenes generically as classroom, the image_description column emphasized teacher-directed discourse, including phrases such as “a teacher is giving a lesson at the front of the room” (Image 1), “a classroom setting where a person is speaking to students” (Image 3), and “students seated in rows facing the front” (Image 7). These images showed whole-group instructional formats dominated by teacher explanation.\nThree images reflected collaborative or interactive activity (Images 0, 6, and 8). The image_description column explicitly referenced peer interaction behaviors, including “a group of students … sitting together… reading a book” (Image 0), “students raising their hands” (Image 6), and “students actively participating and showing enthusiasm” (Image 8). These entries also aligned with primary_object values centered on “students” rather than instructional tools or teacher presence.\nTwo images depicted individual or independent work (Images 4 and 5). Evidence from image_description highlighted individual task engagement without peer or teacher interaction, such as “students are seated and reading from papers” (Image 4) and “students appear focused and are engaged in individual work” (Image 5).\nThese findings indicate variability in instructional format across images, with teacher-led instruction most frequent (44%), followed by collaborative interaction (33%) and independent work (22%).\n\n\nRQ2: Group Size and Use of Instructional Materials\nGroup size and material use were analyzed using object_count, primary_object, secondary_object, and object_list columns generated by llm_image_recognition().\nThe object_count column suggested observable group sizes ranging from 6 to 18 participants per image, with a median of 11. Teacher-led instruction was associated with larger visible groups (e.g., Images 1 and 3 showed 14–18 detected persons), while collaborative scenes tended to show smaller learning clusters (e.g., Images 0 and 8 with 6–8 persons), consistent with small-group activity structures.\nThe object_list column indicated consistent use of text-based materials (e.g., “books,” “papers,” “notebooks”) across seven images (Images 0, 2, 4, 5, 6, 7, 8). Instructional displays such as “chalkboard,” “whiteboard,” or “projector screen” appeared in five images (Images 1, 2, 3, 6, 7), primarily during teacher-directed instruction. Only one image (Image 5) contained references to technology, where object_list included “computers” and image_description mentioned “students working at laptops.”\n\n\nRQ3: Emotional Tone and Engagement Across Classroom Scenes\nEmotional tone and behavioral participation were interpreted using the classroom_sentiment, engagement_level, and sentiment_rationale columns generated by llm_image_sentiment().\nSentiment was most often coded as neutral (classroom_sentiment = “neutral”; 4 images: 1, 2, 4, 5), followed by positive (3 images: 0, 3, 8) and moderately positive (2 images: 6, 7). However, student engagement varied independently of sentiment labels. The engagement_level column revealed a more nuanced pattern:\n\nHigh engagement (engagement_level = “high”) was observed in Images 0, 3, and 8, all of which also had positive sentiment. The sentiment_rationale referenced overt behavioral participation such as “students… interacting” (Image 0) and “raising hands” (Image 3).\nModerate engagement (engagement_level = “moderate”) appeared in four images (Images 4, 5, 6, 7), even when sentiment was neutral or moderately positive. Rationales included “students appear focused on their work” (Image 5) and “students raising their hands… paying attention” (Image 6).\nLow engagement (engagement_level = “low”) occurred in two images (Images 1 and 2), both of which were whole-class lecture scenes with passive student posture. Rationales noted “students appear disengaged; many do not make eye contact with the teacher.”\n\nTogether, these findings suggest that engagement was more sensitive to instructional structure than sentiment alone. Collaborative scenes showed the highest engagement, teacher-led instruction showed mixed engagement, and independent work produced moderate engagement with limited visible affect.\n\n\nDiscussion\nAcross the nine images, instructional format (RQ1) and classroom structure (RQ2) appeared to shape student participation patterns (RQ3). Collaborative activity was consistently associated with smaller group sizes and higher behavioral engagement. Teacher-led instruction involved larger groups and produced more passive engagement patterns. Independent work reflected focused but emotionally neutral learning states. Sentiment alone provided limited insight; however, the combination of engagement_level and observed activity type offered a more reliable indicator of classroom interaction quality.",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "chapter-8.html",
    "href": "chapter-8.html",
    "title": "Communication",
    "section": "",
    "text": "8.1 Overview: Why Communication Matters\nComputational approaches enable educational researchers to analyze increasingly complex and large-scale data. However, the value of these analyses ultimately depends on how clearly and responsibly findings are communicated. Communication is not a final step added after analysis—it is an integral part of making research meaningful, interpretable, and usable.\nPoor communication can obscure otherwise rigorous analyses, while effective communication can extend the reach of research beyond academic journals to educators, designers, policymakers, and students. In computational educational research, communication also involves technical decisions about formats, platforms, and levels of transparency.\nIn this chapter, we introduce common ways researchers communicate computational work, with particular attention to open, reproducible, and web-based approaches. We then demonstrate how a complete computational analysis—introduced earlier in this book—can be communicated using a Quarto-based project website.",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "chapter-8.html#common-ways-to-communicate-computational-educational-research",
    "href": "chapter-8.html#common-ways-to-communicate-computational-educational-research",
    "title": "Communication",
    "section": "8.2 Common Ways to Communicate Computational Educational Research",
    "text": "8.2 Common Ways to Communicate Computational Educational Research\nEducational researchers use a range of formats and platforms to share computational work. Each approach reflects trade-offs among accessibility, formality, reproducibility, and audience.\n\n8.2.1 Traditional Journal Publications\nPeer-reviewed journal articles remain the dominant mode of scholarly communication in education. Journals provide formal review, editorial oversight, and long-term archiving.\nExamples of education journals that publish computational and data-intensive work include:\n\nEducational Researcher: https://journals.sagepub.com/home/edr\nJournal of Learning Analytics: https://learning-analytics.info/\n\nWhile journal articles are essential for academic recognition, they often impose constraints on length, visualizations, and methodological detail. As a result, many computational decisions—such as preprocessing steps or parameter choices—may be summarized rather than fully documented.\n\n\n8.2.2 Preprints and Open Science Repositories\nPreprint servers allow researchers to share manuscripts publicly before or during the peer-review process. These platforms support rapid dissemination and open feedback.\nCommon platforms include:\n\narXiv: https://arxiv.org/\nPsyArXiv: https://psyarxiv.com/\nOpen Science Framework (OSF): https://osf.io/\n\nPreprints are particularly useful for computational research, where methods evolve quickly and early visibility can support collaboration. Researchers should always confirm that their target journals permit preprint posting.\n\n\n8.2.3 Project Websites and Living Documents\nIncreasingly, computational researchers publish project websites that combine narrative, code, figures, and links to data. These sites allow authors to present analyses without the spatial constraints of traditional articles.\nA common workflow involves:\n\nHosting source files and code on GitHub (https://github.com/)\nPublishing a static website using GitHub Pages (https://pages.github.com/)\nAuthoring documents with Quarto (https://quarto.org/)\n\nThis approach supports transparency, reproducibility, and incremental updates. Project websites are especially well suited for method demonstrations, teaching examples, and analyses involving multiple visualizations.\n\n\n8.2.4 Open Data and Code Archives\nDedicated repositories allow researchers to share datasets, analysis scripts, and documentation in citable form.\nPopular platform such as:\n\nZenodo: https://zenodo.org/\n\nThese services issue persistent identifiers (DOIs), enabling datasets and code to be cited independently of articles. A common practice is to host active development on GitHub while archiving stable releases on Zenodo.",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "chapter-8.html#choosing-an-appropriate-communication-strategy",
    "href": "chapter-8.html#choosing-an-appropriate-communication-strategy",
    "title": "Communication",
    "section": "8.3 Choosing an Appropriate Communication Strategy",
    "text": "8.3 Choosing an Appropriate Communication Strategy\nNo single communication method is optimal for all projects. Researchers should consider their intended audience, goals, and constraints.\nFor example:\n\nJournal articles are essential for scholarly recognition.\nPreprints support rapid and open dissemination.\nProject websites allow rich, transparent presentation of computational workflows.\nData repositories ensure long-term access and citability.\n\nIn practice, many projects use multiple complementary channels, such as a journal article accompanied by a public website and an archived dataset.",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "chapter-8.html#case-study-communicating-a-text-analysis-with-quarto",
    "href": "chapter-8.html#case-study-communicating-a-text-analysis-with-quarto",
    "title": "Communication",
    "section": "8.4 Case Study: Communicating a Text Analysis with Quarto",
    "text": "8.4 Case Study: Communicating a Text Analysis with Quarto\nIn earlier chapters, this book introduced frequency-based text analysis as a method for exploring patterns in large collections of educational texts. In this section, we shift focus from conducting analysis to communicating it. Specifically, we demonstrate how an existing computational study can be shared as a public, reproducible, and accessible research artifact using Quarto.\nThe case study draws on the frequency-based analysis of generative AI (GenAI) usage guidelines from 100 U.S. universities presented in Chapter 2. Rather than reproducing the analytical steps, the goal here is to illustrate how such work can be communicated effectively to a broader audience through a project website.\n\n8.4.1 From Analysis to Communication\nTraditional journal articles often summarize computational workflows due to space limitations. In contrast, web-based formats allow researchers to present narrative explanations, code, visualizations, and documentation in a single, integrated environment.\nFor the GenAI policy analysis, a communication-oriented artifact should allow readers to:\n\nunderstand the research context and questions,\ninspect analytical decisions at a high level,\nview results alongside interpretation,\nand access code and data when appropriate.\n\nQuarto (https://quarto.org/) provides a flexible framework for producing such artifacts, enabling researchers to write plain-text documents that render into polished websites.\n\n\n8.4.2 Overview of the Communication Artifact\nIn this example, the analysis is communicated as a Quarto-based project website hosted via GitHub Pages (https://pages.github.com/). The website includes:\n\na landing page describing the research context,\na dedicated analysis page summarizing methods and findings,\nembedded figures generated from R,\nand links to data and code repositories.\n\nThis format supports transparency and reproducibility while remaining accessible to readers without advanced programming backgrounds.\n\n\n8.4.3 Project Structure\nA minimal project structure for communicating the GenAI policy analysis is shown below:\n\ngenai-policy-analysis/\n\n├── _quarto.yml\n\n├── index.qmd\n\n├── analysis/\n\n│ └── frequency-analysis.qmd\n\n├── data/\n\n│ └── University_GenAI_Policy_Stance.csv\n\n└── README.md\nThis organization separates communication documents from raw data and supports reuse and extension.\n\n\n\n8.4.4 Website Configuration\nThe project is configured as a Quarto website using the _quarto.yml file:\nproject:\n  type: website\n\nwebsite:\n  title: \"GenAI Usage Guidelines in Higher Education\"\n  navbar:\n    right:\n      - text: \"Analysis\"\n        href: analysis/frequency-analysis.html\nThis configuration enables navigation between pages and produces a static website suitable for public hosting.\n\n\n8.4.5 Communicating the Analysis in a Quarto Document\nThe core communication document (frequency-analysis.qmd) integrates narrative text with selected code and visual outputs. Rather than presenting all preprocessing steps, the document emphasizes interpretability and alignment with research questions.\nAn excerpt from such a document is shown below:\n\n\nResearch Context\nAs generative AI writing tools become more prevalent, universities have issued guidelines to clarify acceptable use in academic contexts. This analysis examines 100 publicly available GenAI policy documents from U.S. universities to identify prominent themes and institutional concerns.\n\n\nResearch Questions\n\nWhat words appear most frequently in university GenAI writing usage policies?\nWhich keywords reflect common concerns related to academic integrity and instructional responsibility?\n\n\n\nAnalytical Overview\nText data were tokenized, common stop words were removed, and word frequencies were calculated using R. Full preprocessing and analytical details are documented in Chapter 2.\nFigures such as word clouds and frequency bar charts are rendered directly within the document, allowing readers to view results alongside explanatory text.\n\n\n8.4.6 Presenting Results for Interpretation\nInstead of emphasizing technical implementation, the website focuses on interpretive clarity. For example, a bar chart displaying the most frequent terms highlights the prominence of words such as assignment, student, and writing, emphasizing institutional attention to coursework and academic expectations.\nNarrative text accompanying the visualization explains how these patterns relate to broader concerns about academic integrity, instructor authority, and ethical AI use. This integration of visual and narrative elements helps prevent misinterpretation of descriptive computational results.\n\n\n8.4.7 Publishing and Sharing\nOnce rendered, the Quarto project can be published via GitHub Pages by hosting the repository on GitHub (https://github.com/). This process produces a stable, shareable URL that can be cited in manuscripts, course syllabi, or presentations.\nExamples of publicly available Quarto projects can be found at:\nhttps://quarto.org/examples/\n\n\n8.4.8 Why This Format Matters\nCommunicating the GenAI policy analysis as a web-based artifact offers several advantages:\n\nReaders can engage with results at their own pace.\nMethods and assumptions are documented transparently.\nThe analysis can be updated as policies evolve.\nThe artifact supports both research dissemination and teaching.\n\nImportantly, this approach complements rather than replaces traditional publications. A journal article may present the core findings, while a project website provides extended documentation and resources.\n\n\n8.4.9 Summary\nThis case study illustrates how a frequency-based text analysis can be transformed into a communicative research product using Quarto. By shifting emphasis from computation to interpretation and accessibility, researchers can extend the reach and impact of computational educational research beyond static manuscripts.",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "chapter-8.html#benefits-of-web-based-communication",
    "href": "chapter-8.html#benefits-of-web-based-communication",
    "title": "Communication",
    "section": "8.5 Benefits of Web-Based Communication",
    "text": "8.5 Benefits of Web-Based Communication\nCommunicating computational research through project websites offers several advantages:\n\nTransparency: Readers can inspect code, data, and assumptions.\nReproducibility: Analyses can be rerun and extended.\nFlexibility: Content can be updated as methods or data evolve.\nPedagogical value: Websites serve as learning resources for students and practitioners.\n\nThese benefits complement, rather than replace, traditional academic publications.",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "chapter-8.html#ethical-and-responsible-communication",
    "href": "chapter-8.html#ethical-and-responsible-communication",
    "title": "Communication",
    "section": "8.6 Ethical and Responsible Communication",
    "text": "8.6 Ethical and Responsible Communication\nWhen communicating computational analyses in education, researchers must attend to ethical considerations. These include protecting student privacy, avoiding deficit-oriented narratives, and clearly stating methodological limitations.\nPublic-facing materials should avoid revealing identifiable information and should transparently acknowledge sources of bias or uncertainty. Responsible communication ensures that openness does not come at the expense of ethical research practice.",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "chapter-8.html#summary-and-communication-checklist",
    "href": "chapter-8.html#summary-and-communication-checklist",
    "title": "Communication",
    "section": "8.7 Summary and Communication Checklist",
    "text": "8.7 Summary and Communication Checklist\nThis chapter emphasized that communication is a methodological choice, not merely a presentation task. Computational educational research benefits from communication strategies that align with open science, reproducibility, and audience needs.\nA practical checklist for communicating computational research includes:\n\nResearch questions align with data and methods\nAnalytical decisions are documented transparently\nCode and data are accessible when possible\nVisualizations support interpretation rather than replace explanation\nEthical considerations are explicitly addressed\nCommunication formats match the intended audience\n\nTogether, these practices support rigorous, responsible, and impactful computational educational research.",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Communication</span>"
    ]
  }
]