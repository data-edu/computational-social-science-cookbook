[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Social Science Cookbook with R",
    "section": "",
    "text": "1 Preface\nWelcome to Computational Social Science Cookbook with R!\nThe book is organized around six sections. Within these six sections are specific chapters, which serve as cookbook “entries”. While the section overviews (the first bullet point within each section) introduce the techniques or methodologies used in the section’s chapters, the entries (the subsequent bullet points) are intended to address a specific, narrow problem, as well a to provide a sample for researchers in writing their research questions, methods, results (and discussions) sections based on the analyses.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "section-1.html",
    "href": "section-1.html",
    "title": "2  Section 1",
    "section": "",
    "text": "2.1 1.1\nWelcome!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Section 1</span>"
    ]
  },
  {
    "objectID": "section-1.html#section-1",
    "href": "section-1.html#section-1",
    "title": "2  Section 1",
    "section": "2.2 2.1",
    "text": "2.2 2.1\n\n2 + 2\n\n[1] 4",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Section 1</span>"
    ]
  },
  {
    "objectID": "section-2.html",
    "href": "section-2.html",
    "title": "3  Section 2 : Capturing and Analyzing Text Data with Computational Methods",
    "section": "",
    "text": "3.1 2.1 Overview\nIn social sciences, analyzing text data is usually considered the “job” of qualitative researchers. Qualitative research involves identifying patterns in non-numeric data, and this pattern recognition is typically done manually. This process is time-intensive but can yield rich research results. Traditional methods for analyzing text data involve human coding and can include direct sources (e.g., books, online texts) or indirect sources (e.g., interview transcripts).\nWith the advent of new software, we can capture and analyze text data in ways that were previously not possible. Modern data collection techniques include web scraping, accessing social media APIs, or downloading large online documents. Given the increased size of text data, analysis now requires computational approaches (e.g., dictionary-based, frequency-based, machine learning) that go beyond manual coding. These computational methods allow social scientists to ask new types of research questions, expanding the scope and depth of possible insights.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Section 2 : Capturing and Analyzing Text Data with Computational Methods</span>"
    ]
  },
  {
    "objectID": "section-2.html#overview",
    "href": "section-2.html#overview",
    "title": "3  Section 2 : Capturing and Analyzing Text Data with Computational Methods",
    "section": "",
    "text": "Disclaimer: While resources are available that discuss these analysis methods in depth, this book aims to provide a practical guide for social scientists, using data they will likely encounter. Our goal is to present a “cookbook” for guiding research projects through real-world examples.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Section 2 : Capturing and Analyzing Text Data with Computational Methods</span>"
    ]
  },
  {
    "objectID": "section-2.html#accessing-text-data-broadening-the-horizon",
    "href": "section-2.html#accessing-text-data-broadening-the-horizon",
    "title": "3  Section 2 : Capturing and Analyzing Text Data with Computational Methods",
    "section": "3.2 2.2 Accessing Text Data (Broadening the Horizon)",
    "text": "3.2 2.2 Accessing Text Data (Broadening the Horizon)\n\n3.2.1 2.2.1 Web Scraping (Unstructured or API)\n\n3.2.1.1 What is Web Scraping?\nWeb scraping refers to the automated process of extracting data from web pages. It is particularly useful when dealing with extensive lists of websites that would be tedious to mine manually. A typical web scraping program follows these steps:\n\nLoads a webpage.\nDownloads the HTML or XML structure.\nIdentifies the desired data.\nConverts the data into a format suitable for analysis, such as a data frame.\n\nIn addition to text, web scraping can also be used to download other content types, such as audio-visual files.\n\n\n3.2.1.2 Is Web Scraping Legal?\nWeb scraping was common in the early days of the internet, but with the increasing value of data, legal norms have evolved. To avoid legal issues, check the “Terms of Service” for specific permissions on the website, often accessible via “robots.txt” files. Consult legal advice when in doubt.\n\n\n3.2.1.3 Reading a Web Page into R\nOnce permissions are confirmed, the first step in web scraping is to download the webpage’s source code into R, typically using the rvest package by Hadley Wickham.\n\n# Install and load the rvest package\ninstall.packages(\"rvest\")\nlibrary(rvest)\n\nTo demonstrate, we will scrape a simple Wikipedia page. Static pages, which lack interactive elements like JavaScript, are simpler to scrape. You can view the page’s HTML source in your browser by selecting Developer Tools &gt; View Source.\n\n# Load the webpage\nwikipedia_page &lt;- read_html(\"https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000\")\n\n# Verify that the webpage loaded successfully\nwikipedia_page\n\n\n\n3.2.1.4 Parsing HTML\nThe next challenge is extracting specific information from the HTML structure. HTML files have a “tree-like” format, allowing us to target particular sections. Use your browser’s “Developer Tools” to inspect elements and locate the data. Right-click the desired element and select Inspect to view its structure.\nTo isolate data sections within the HTML structure, identify the XPath or CSS selectors. For instance:\n\n# Extract specific section using XPath\nsection_of_wikipedia &lt;- html_node(wikipedia_page, xpath='//*[@id=\"mw-content-text\"]/div/table')\nhead(section_of_wikipedia)\n\nTo convert the extracted section into a data frame, use html_table():\n\n# Convert the extracted data into a table\nhealth_rankings &lt;- html_table(section_of_wikipedia)\nhead(health_rankings[ , (1:2)])  # Display the first two columns\n\n\n\n3.2.1.5 Parsing with CSS Selectors\nFor more complex web pages, CSS selectors can be an alternative to XPath. Tools like Selector Gadget can help identify the required CSS selectors.\nFor example, to scrape event information from Duke University’s main page:\n\n# Load the webpage\nduke_page &lt;- read_html(\"https://www.duke.edu\")\n\n# Extract event information using CSS selector\nduke_events &lt;- html_nodes(duke_page, css=\"li:nth-child(1) .epsilon\")\nhtml_text(duke_events)\n\n\n\n3.2.1.6 Scraping with Selenium\nFor tasks involving interactive actions (e.g., filling search fields), use RSelenium, which enables automated browser operations.\nTo set up Selenium, install the Java SE Development Kit and Docker. Then, start Selenium in R:\n\n# Install and load RSelenium\ninstall.packages(\"RSelenium\")\nlibrary(RSelenium)\n\n# Start a Selenium session\nrD &lt;- rsDriver()\nremDr &lt;- rD$client\nremDr$navigate(\"https://www.duke.edu\")\n\nTo automate data entry, identify the CSS selector for the search box and input the query:\n\n# Find the search box element and enter a query\nsearch_box &lt;- remDr$findElement(using = 'css selector', 'fieldset input')\nsearch_box$sendKeysToElement(list(\"data science\", \"\\uE007\"))  # \"\\uE007\" represents Enter key\n\n\n\n3.2.1.7 Web Scraping within a Loop\nTo scrape multiple pages, embed the code within a loop to automate tasks across different URLs. Since each site may have a unique structure, generalized scraping can be time-intensive and error-prone. Implement error handling to manage interruptions.\n\n\n3.2.1.8 When to Use Web Scraping\nWeb scraping is appropriate if:\n\nPage structure is consistent across sites: For example, a government site with date suffixes but a uniform layout.\nManual data collection is prohibitive: For extensive text or embedded tables.\n\nWhen feasible, consider alternatives like APIs or data-entry services (e.g., Amazon Mechanical Turk) for better efficiency and legal compliance.\n\n\n\n\n3.2.2 What is an API?\nAn Application Programming Interface (API) is a set of protocols that allows computers to communicate and exchange information. A common type is the REST API, where one machine sends a request, and another returns a response. APIs provide standardized access to data, services, and functionalities, making them essential in software development.\n\n3.2.2.1 When to Use an API\nAPIs are commonly used for:\n\nIntegrating with Third-Party Services: APIs connect applications to services like payment gateways or social media.\nAccessing Data: APIs retrieve data from systems or databases (e.g., real-time weather data).\nAutomating Tasks: APIs automate processes within applications, such as email marketing.\nBuilding New Applications: APIs allow developers to build new apps or services (e.g., a mapping API for navigation).\nStreamlining Workflows: APIs enable seamless communication and data exchange across systems.\n\n\n\n3.2.2.2 Using Reddit API with RedditExtractoR\nReddit is a social media platform featuring a complex network of users and discussions, organized into “subreddits” by topic. RedditExtractoR, an R package, enables data extraction from Reddit to identify trends and analyze interactions.\n\n# Install and load RedditExtractoR\ninstall.packages(\"RedditExtractoR\")\nlibrary(RedditExtractoR)\n\n# Access data from the ChatGPT subreddit\nchatgpt_reddit &lt;- find_thread_urls(subreddit = \"chatgpt\", sort_by = \"new\", period = \"day\")\nview(chatgpt_reddit)\n\n\n\n\n3.2.3 2.2.2 Audio Transcripts (Zoom, etc.)\nAudio transcripts are a rich source of text data, especially useful for capturing spoken content from meetings, interviews, or webinars. Many platforms, such as Zoom, provide automated transcription services that can be downloaded as text files for analysis. By processing these transcripts, researchers can analyze conversation themes, sentiment, or other linguistic features. Here’s how to access and prepare Zoom transcripts for analysis in R.\n\n3.2.3.1 Key Steps for Extracting Text Data from Audio Transcripts\n\nAccess the Zoom Transcript\n\nLog in to your Zoom account.\nNavigate to the “Recordings” section.\nSelect the recording you wish to analyze and download the “Audio Transcript” file.\n\nImport the Transcript into R\nOnce the file is downloaded, you can load it into R for analysis. Depending on the file format (usually a .txt file with tab or comma delimiters), use read.table(), read.csv(), or functions from the readr package to load the data.\n\n\n   # Load the transcript into R\n   transcript_data &lt;- read.table(\"path/to/your/zoom_transcript.txt\", sep = \"\\t\", header = TRUE)\n\nAdjust the sep parameter based on the delimiter used in the transcript file (typically \\t for tab-delimited files).\n\nData Cleaning (if necessary)\nClean up the text data to remove unnecessary characters, standardize formatting, and prepare it for further analysis.\n\nRemove Unwanted Characters\nUse gsub() to eliminate special characters and punctuation, keeping only alphanumeric characters and spaces.\n\n\n     # Remove special characters\n     transcript_data$text &lt;- gsub(\"[^a-zA-Z0-9 ]\", \"\", transcript_data$text)\n\n\nConvert Text to Lowercase\nStandardize text to lowercase for consistency in text analysis.\n\n\n\n     # Convert text to lowercase\n     transcript_data$text &lt;- tolower(transcript_data$text)\n\n\n\n\n3.2.4 2.2.3 PDF\nPDF files are a valuable source of text data, often found in research publications, government documents, and industry reports. We’ll explore two main methods for extracting text from PDFs:\n\nExtracting from Local PDF Files: This method involves accessing and parsing text from PDF files stored locally, providing tools and techniques to efficiently retrieve text data from offline documents.\nDownloading and Extracting PDF Files: This approach covers downloading PDFs from online sources and extracting their text. This method is useful for scraping publicly available documents from websites or databases for research purposes.\nPDF Data Extractor (PDE)\nFor more advanced PDF text extraction and processing, you can use the PDF Data Extractor (PDE) package. This package provides tools for extracting text data from complex PDF documents, supporting additional customization options for text extraction. PDE is a R package that easily extracts information and tables from PDF files. The PDE_analyzer_i() performs the sentence and table extraction while the included PDE_reader_i() allows the user-friendly visualization and quick-processing of the obtained results.\n\n\n\n3.2.4.1 Steps for Extracting Text from Local PDF Files\n\nInstall and Load the pdftools Package\nStart by installing and loading the pdftools package, which is specifically designed for reading and extracting text from PDF files in R.\n\n\n   install.packages(\"pdftools\")\n   library(pdftools)\n\n\nRead the PDF as a Text File\nUse the pdf_text() function to read the PDF file into R as a text object. This function returns each page as a separate string in a character vector.\n\n\n   txt &lt;- pdf_text(\"path/to/your/file.pdf\")\n\n\nExtract Text from a Specific Page\nTo access a particular page from the PDF, specify the page number in the text vector. For example, to extract text from page 24:\n\n\n   page_text &lt;- txt[24]  # page 24\n\n\nExtract Rows into a List\nIf the page contains a table or structured text, use the scan() function to read each row as a separate element in a list. The textConnection() function converts the page text for processing.\n\n\n   rows &lt;- scan(textConnection(page_text), what = \"character\", sep = \"\\n\")\n\n\nSplit Rows into Cells\nTo further parse each row, split it into cells by specifying the delimiter, such as whitespace (using \"\\\\s+\"). This converts each row into a list of individual cells.\n\n\n   row &lt;- unlist(strsplit(rows[24], \"\\\\s+\"))  # Example with the 24th row\n\n\n\n\n3.2.4.2 Steps for Downloading and Extracting Text from PDF Files\n\nDownload the PDF from the Web\nUse the download.file() function to download the PDF file from a specified URL. Set the mode to \"wb\" (write binary) to ensure the file is saved correctly.\n\n\n   link &lt;- paste0(\n     \"http://www.singstat.gov.sg/docs/\",\n     \"default-source/default-document-library/\",\n     \"publications/publications_and_papers/\",\n     \"cop2010/census_2010_release3/\",\n     \"cop2010sr3.pdf\"\n   )\n   download.file(link, \"census2010_3.pdf\", mode = \"wb\")\n\n\nRead the PDF as a Text File\nAfter downloading, read the PDF into R as a text object using the pdf_text() function from the pdftools package. Each page of the PDF will be stored as a string in a character vector.\n\n\n   txt &lt;- pdf_text(\"census2010_3.pdf\")\n\n\nExtract Text from a Specific Page\nAccess the desired page (e.g., page 24) by specifying the page number in the character vector.\n\n\n   page_text &lt;- txt[24]  # Page 24\n\n\nExtract Rows into a List\nUse the scan() function to split the page text into rows, with each row representing a line of text in the PDF. This creates a list where each line from the page is an element.\n\n\n   rows &lt;- scan(textConnection(page_text), what = \"character\", sep = \"\\n\")\n\n\nLoop Through Rows and Extract Data\nStarting from a specific row (e.g., row 7), loop over each row. For each row:\n\nSplit the text by spaces (\"\\\\s+\") using strsplit().\nConvert the result to a vector with unlist().\nIf the third cell in the row is not empty, store the second cell as name and the third cell as total, converting it to a numeric format after removing commas.\n\n\n\n   name &lt;- c()\n   total &lt;- c()\n\n   for (i in 7:length(rows)) {\n     row &lt;- unlist(strsplit(rows[i], \"\\\\s+\"))\n     if (!is.na(row[3])) {\n       name &lt;- c(name, row[2])\n       total &lt;- c(total, as.numeric(gsub(\",\", \"\", row[3])))\n     }\n   }\n\n\n\n\n3.2.5 2.2.4 Survey, Discussions, etc.\nSurveys and discussion posts are valuable sources of text data in social science research, providing insights into participants’ perspectives, opinions, and experiences. These data sources often come from open-ended survey responses, online discussion boards, or educational platforms. Extracting and preparing text data from these sources can reveal recurring themes, sentiment, and other patterns that support both quantitative and qualitative analysis. Below are key steps and code examples for processing text data from surveys and discussions in R.\n\n3.2.5.1 Key Steps for Processing Survey and Discussion Text Data\n\nLoad the Data\nSurvey and discussion data are typically stored in spreadsheet formats like CSV. Begin by loading this data into R for processing. Here, readr is used for reading CSV files with read_csv().\n\n\n   # Install and load necessary packages\n   install.packages(\"readr\")\n   library(readr)\n   \n   # Load data\n   survey_data &lt;- read_csv(\"path/to/your/survey_data.csv\")\n\n\nExtract Text Columns\nIdentify and isolate the relevant text columns. For example, if the text data is in a column named “Response,” you can create a new vector for analysis.\n\n\n   # Extract text data from the specified column\n   text_data &lt;- survey_data$Response\n\n\nData Cleaning\nPrepare the text data by cleaning it, removing any unnecessary characters, and standardizing the text. This includes removing punctuation, converting text to lowercase, and handling extra whitespace.\n\nRemove Unwanted Characters\nUse gsub() from base R to remove any non-alphanumeric characters, retaining only words and spaces.\n\n\n\n     # Remove special characters\n     text_data &lt;- gsub(\"[^a-zA-Z0-9 ]\", \"\", text_data)\n\n\nConvert to Lowercase\nStandardize the text by converting all characters to lowercase.\n\n\n     # Convert text to lowercase\n     text_data &lt;- tolower(text_data)\n\n\nRemove Extra Whitespace\nRemove any extra whitespace that may be left after cleaning.\n\n\n     # Remove extra spaces\n     text_data &lt;- gsub(\"\\\\s+\", \" \", text_data)\n\n\nTokenization and Word Counting (Optional)\nIf further analysis is needed, such as frequency-based analysis, split the text into individual words (tokenization) or count the occurrence of specific words. Here, dplyr is used to organize the word counts.\n\n\n   # Install and load necessary packages\n   install.packages(\"dplyr\")\n   library(dplyr)\n   \n   # Tokenize and count words\n   word_count &lt;- strsplit(text_data, \" \") %&gt;%\n                 unlist() %&gt;%\n                 table() %&gt;%\n                 as.data.frame()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Section 2 : Capturing and Analyzing Text Data with Computational Methods</span>"
    ]
  },
  {
    "objectID": "section-2.html#frequency-based-analysis",
    "href": "section-2.html#frequency-based-analysis",
    "title": "3  Section 2 : Capturing and Analyzing Text Data with Computational Methods",
    "section": "3.3 2.3 Frequency-based Analysis",
    "text": "3.3 2.3 Frequency-based Analysis\n\n3.3.1 2.3.1 Purpose + Case\nThe purpose of the frequency-based approach is to count the number of words as they appear in a text file, whether it is a collection of tweets, documents, or interview transcripts. This approach aligns with the frequency-coding method (Saldana) and can supplement human coding by revealing the most/least commonly occurring words, which can then be compared across dependent variables.\n\n3.3.1.1 Case Study: Frequency-Based Analysis of ChatGPT Writing Guidelines in Higher Education\nAs AI writing tools like ChatGPT become more prevalent, educators are working to understand how best to integrate them within academic settings, while many students and instructors remain uncertain about current allowances. Our research into AI usage guidelines from the top 100 universities in North America aims to identify prominent themes and concerns in institutional policies regarding ChatGPT.\n\n\n3.3.1.2 Data Source\nThe dataset consists of publicly available AI policy texts from 100 universities, sourced from Scribbr’s ChatGPT University Policies page. The data has been downloaded and saved as a CSV file for analysis.\n\n\n\n\n3.3.2 2.3.2 Sample Research Questions\n\nRQ1: What are the most frequently mentioned words in university ChatGPT writing usage policies?\nRQ2: Which keywords reflect common concerns or focal points related to ChatGPT writing usage in academic settings?\n\n\n\n\n3.3.3 2.3.3 Analysis\n\n3.3.3.0.1 Step 1: Load Required Libraries\nInstall and load libraries for data processing, visualization, and word cloud generation.\n\n# Install necessary packages if not already installed\n#install.packages(c(\"tibble\", \"dplyr\", \"tidytext\", \"ggplot2\", \"viridis\",\"tm\",wordcloud\" \"wordcloud2\",\"webshot\"))\n\n# Load libraries\nlibrary(readr)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(tm)\nlibrary(wordcloud)\nlibrary(wordcloud2)\nlibrary(webshot)\n\n\n\n3.3.3.0.2 Step 2: Load Data\nRead the CSV file containing policy texts from the top 100 universities.\n\n# Load the dataset\nuniversity_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\")\n\n\n\n3.3.3.0.3 Step 3: Tokenize Text and Count Word Frequency\nProcess the text data by tokenizing words, removing stop words, and counting word occurrences.\n\n# Tokenize text, remove stop words, and count word frequencies\nword_frequency &lt;- university_policies %&gt;%\n  unnest_tokens(word, Stance) %&gt;%  # Tokenize the 'Policy_Text' column\n  anti_join(stop_words) %&gt;%             # Remove common stop words\n  count(word, sort = TRUE)              # Count and sort words by frequency\nword_frequency\n\n# A tibble: 1,108 × 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 ai            124\n 2 students      120\n 3 chatgpt        80\n 4 tools          73\n 5 academic       68\n 6 policy         35\n 7 assignments    34\n 8 instructors    34\n 9 integrity      33\n10 student        32\n# ℹ 1,098 more rows\n\n\n\n\n3.3.3.0.4 Step 4: Create a Word Cloud\nGenerate a word cloud to visualize the frequency of words in a circular shape.\n\n# Create and display the GenAI usage Stance wordcloud\n\nwordcloud(words = word_frequency$word, freq = word_frequency$n, scale = c(4, 0.5), random.order = FALSE, min.freq = 10, colors = brewer.pal(8, \"Dark2\"), rot.per = 0.35)\n\n\n\n\n\n\n\n\n\n\n3.3.3.0.5 Step 5: Visualize Top 12 Words in University Policies\nSelect the top 12 most frequent words and create a bar chart to visualize the distribution.\n\n# Select the top 12 words\ntop_words &lt;- word_frequency %&gt;% slice(1:12)\n\n# Generate the bar chart\npolicy_word_chart &lt;- ggplot(top_words, aes(x = reorder(word, n), y = n, fill = word)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(\n    title = \"Top Words in University ChatGPT Policies\",\n    x = NULL,\n    y = \"Frequency\",\n    caption = \"Source: University AI Policy Text\",\n    fill = \"Word\"\n  ) +\n  scale_fill_viridis(discrete = TRUE) +\n  geom_text(aes(label = n), vjust = 0.5, hjust = -0.1, size = 3)\n\n# Print the bar chart\nprint(policy_word_chart)\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.4 2.3.4 Results and Discussions\n\nRQ1: What are the most frequently mentioned words in university ChatGPT writing usage policies?\nThe frequency analysis shows that keywords such as “assignment,” “student,” “syllabus,” and “writing” are among the most commonly mentioned terms across AI policies at 100 universities. This emphasis reflects a focus on using AI tools, like ChatGPT, to support student learning and enhance teaching content. The frequent mention of these words suggests that institutions are considering the role of AI in academic assignments and course design, indicating a strategic commitment to integrating AI within educational tasks and student interactions.\nRQ2: Which keywords reflect common concerns or focal points related to ChatGPT writing usage in academic settings?\nThe analysis of the top 12 frequently mentioned terms highlights additional focal points, including “tool,” “academic,” “instructor,” “integrity,” and “expectations.” These terms reveal concerns around the ethical use of AI tools, the need for clarity in academic applications, and the central role of instructors in AI policy implementation. Keywords like “integrity” and “expectations” emphasize the importance of maintaining academic standards and setting clear guidelines for AI use in classrooms, while “instructor” underscores the influence faculty members have in shaping AI-related practices. Together, these terms reflect a commitment to transparent policies that support ethical and effective AI integration, enhancing the academic experience for students.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Section 2 : Capturing and Analyzing Text Data with Computational Methods</span>"
    ]
  },
  {
    "objectID": "section-2.html#dictionary-based-analysis",
    "href": "section-2.html#dictionary-based-analysis",
    "title": "3  Section 2 : Capturing and Analyzing Text Data with Computational Methods",
    "section": "3.4 2.4 Dictionary-based Analysis",
    "text": "3.4 2.4 Dictionary-based Analysis\n\n3.4.1 2.4.1 Purpose + Case\nThe purpose of dictionary-based analysis in text data is to assess the presence of predefined categories, like emotions or sentiments, within the text using lexicons or dictionaries. This approach allows researchers to quantify qualitative aspects, such as positive or negative sentiment, based on specific words that correspond to these categories.\nCase: In this analysis, we examine the stance of 100 universities on the use of ChatGPT by applying the Bing sentiment dictionary. By analyzing sentiment scores, we aim to identify the general tone in these policies, indicating whether the institutions’ attitudes toward ChatGPT are predominantly positive or negative.\n\n\n\n3.4.2 2.4.2 Sample Research Questions\n\nRQ: What is the dominant sentiment expressed in ChatGPT policy texts across universities, and is it primarily positive or negative?\n\n\n\n\n3.4.3 2.4.3 Analysis\n\n3.4.3.1 Step 1: Install and Load Necessary Libraries\nFirst, install and load the required packages for text processing and visualization.\n\n# Install necessary packages if not already installed\n#install.packages(c(\"tidytext\",\"tidyverse\" \"dplyr\", \"ggplot2\", \"tidyr\"))\n\n# Load libraries\nlibrary(tidytext)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\n\n3.4.3.2 Step 2: Load and Prepare Data(same as 2.3)\nLoad the ChatGPT policy stance data from a CSV file. Be sure to update the file path as needed. we use the same data as 2.3.\n\n# Load the dataset (replace \"University_GenAI_Policy_Stance.csv\" with the actual file path)\nuniversity_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\")\n# Tokenize text, remove stop words, and count word frequencies\nword_frequency &lt;- university_policies %&gt;%\n  unnest_tokens(word, Stance) %&gt;%  # Tokenize the 'Policy_Text' column\n  anti_join(stop_words) %&gt;%             # Remove common stop words\n  count(word, sort = TRUE)              # Count and sort words by frequency\nword_frequency\n\n\n\n3.4.3.3 Step 3: Tokenize Text Data and Apply Sentiment Dictionary\nTokenize the policy text data to separate individual words. Then, use the Bing sentiment dictionary to label each word as positive or negative.\n\n# Tokenize 'Stance' column and apply Bing sentiment dictionary\nsentiment_scores &lt;- word_frequency %&gt;%\n  inner_join(get_sentiments(\"bing\")) %&gt;% # Join with Bing sentiment lexicon\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%\n  mutate(sentiment_score = positive - negative) # Calculate net sentiment score\n\nsentiment_scores\n\n# A tibble: 142 × 4\n   word         positive negative sentiment_score\n   &lt;chr&gt;           &lt;int&gt;    &lt;int&gt;           &lt;int&gt;\n 1 honor              18        0              18\n 2 cheating            0       11             -11\n 3 dishonesty          0       10             -10\n 4 guidance            9        0               9\n 5 honesty             7        0               7\n 6 intelligence        7        0               7\n 7 transparent         7        0               7\n 8 violation           0        7              -7\n 9 encourage           6        0               6\n10 difficult           0        5              -5\n# ℹ 132 more rows\n\n\n\n\n3.4.3.4 Step 4: Create a Density Plot for Sentiment Distribution\nVisualize the distribution of sentiment scores with a density plot, showing the prevalence of positive and negative sentiments across university policies.\n\n# Generate a density plot of sentiment scores\ndensity_plot &lt;- ggplot(sentiment_scores, aes(x = sentiment_score, fill = sentiment_score &gt; 0)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"red\", \"green\"), name = \"Sentiment\",\n                    labels = c(\"Negative\", \"Positive\")) +\n  labs(\n    title = \"Density Plot of University AI Policy Sentiment\",\n    x = \"Sentiment Score\",\n    y = \"Density\",\n    caption = \"Source: University Policy Text\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 20),\n    axis.text = element_text(size = 14),\n    axis.title = element_text(face = \"bold\", size = 16),\n    plot.caption = element_text(size = 12)\n  )\n\n# Print the plot\nprint(density_plot)\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.4 2.4.4 Results and Discussions\n\nRQ: What is the dominant sentiment expressed in ChatGPT policy texts across universities, and is it primarily positive or negative?\nThe dictionary-based sentiment analysis reveals the prevailing sentiments in university policies on ChatGPT usage. Using the Bing lexicon to assign positive and negative scores, the density plot illustrates the distribution of sentiment scores across the 100 institutions.\nThe results indicate a balanced perspective with a slight tendency toward positive sentiment, as reflected by a higher density of positive scores. This analysis provides insights into the varying degrees of acceptance and caution universities adopt in their AI policy frameworks, demonstrating the diverse stances that shape institutional AI guidelines.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Section 2 : Capturing and Analyzing Text Data with Computational Methods</span>"
    ]
  },
  {
    "objectID": "section-2.html#predictive-models-using-text",
    "href": "section-2.html#predictive-models-using-text",
    "title": "3  Section 2 : Capturing and Analyzing Text Data with Computational Methods",
    "section": "3.5 2.5 Predictive Models Using Text",
    "text": "3.5 2.5 Predictive Models Using Text\nThis section covers how to apply both supervised and unsupervised machine learning models to predict insights from text data. We demonstrate how these techniques can enhance understanding of institutional ChatGPT policies through prediction and clustering.\n\n\n3.5.1 2.5.1 Supervised ML Using Text\n\n3.5.1.0.1 Purpose + Case\nPurpose: Supervised machine learning with text data can categorize policies based on themes, such as academic integrity, instructor discretion, and student support. This classification helps in understanding how different institutions frame their ChatGPT policies.\nCase: Using labeled samples of policy texts that have been previously categorized into themes, we can train a classifier to categorize new policy texts into one of these themes. This method helps identify which themes are most common among universities and allows for quick categorization of new policy updates.\n\n\n\n3.5.1.0.2 Sample Research Questions\n\nRQ1: Can we accurately classify ChatGPT policy texts into categories like “Academic Integrity,” “Instructor Discretion,” and “Student Support”?\nRQ2: What are the most influential words in predicting the theme of each policy?\n\n\n\n\n3.5.1.0.3 Analysis\n\nStep 1: Load Required Libraries\nInstall and load the necessary packages for text preprocessing and model training.\n\n\n   # Install packages\n   install.packages(c(\"tidytext\", \"dplyr\", \"text2vec\", \"caret\"))\n\n   # Load libraries\n   library(tidytext)\n   library(dplyr)\n   library(text2vec)\n   library(caret)\n\n\nStep 2: Data Preparation\nLoad and preprocess the data by tokenizing, removing stop words, and creating a document-term matrix. Each document should have a labeled theme (e.g., “Academic Integrity”).\n\n\n   # Load labeled dataset\n   policy_data &lt;- read.csv(\"university_chatgpt_policies_labeled.csv\")  # Assume dataset includes a \"Theme\" column\n\n   # Basic text preprocessing\n   policy_data &lt;- policy_data %&gt;%\n     unnest_tokens(word, Policy_Text) %&gt;%\n     anti_join(stop_words)\n\n   # Create document-term matrix (DTM)\n   dtm &lt;- policy_data %&gt;%\n     cast_dtm(document = University_ID, term = word, value = n)\n\n\nStep 3: Train-Test Split\nSplit the data into training and testing sets.\n\n\n   # Split data into training and testing sets\n   set.seed(123)\n   trainIndex &lt;- createDataPartition(policy_data$Theme, p = 0.7, list = FALSE)\n   train_data &lt;- dtm[trainIndex, ]\n   test_data &lt;- dtm[-trainIndex, ]\n\n\nStep 4: Train a Classification Model\nTrain a Naive Bayes classifier to predict the policy theme based on the document-term matrix.\n\n\n   # Train a Naive Bayes classifier\n   model &lt;- naiveBayes(Theme ~ ., data = train_data)\n\n   # Predict on test set\n   predictions &lt;- predict(model, test_data)\n\n\nStep 5: Evaluate Model Performance\nEvaluate the model’s accuracy by comparing predictions with the true labels in the test set.\n\n\n   # Confusion matrix and accuracy\n   confusionMatrix(predictions, test_data$Theme)\n\n\n\n\n3.5.1.0.4 Results and Discussions\n\nRQ1: Can we accurately classify ChatGPT policy texts into categories like “Academic Integrity,” “Instructor Discretion,” and “Student Support”?\nThe Naive Bayes classifier achieves an accuracy of approximately X%, showing it can reasonably classify policy themes. This model could assist in categorizing new policies or updates as they emerge, enabling quick identification of thematic priorities across institutions.\nRQ2: What are the most influential words in predicting the theme of each policy?\nFeature importance analysis shows that terms such as “integrity,” “plagiarism,” and “ethics” strongly indicate the “Academic Integrity” theme. Words like “discretion” and “instructor” predict the “Instructor Discretion” theme, while terms like “support” and “learning” are influential for the “Student Support” theme. These words highlight key focal points in university policies and help institutions quickly determine thematic relevance when updating guidelines.\n\n\n\n\n3.5.2 2.5.2 Unsupervised ML Using Text\n\n3.5.2.0.1 Purpose + Case\nPurpose: Unsupervised machine learning techniques like clustering and topic modeling are used for identifying hidden patterns, themes, or groupings within text data without labeled categories. These models are useful for uncovering common themes in ChatGPT policy texts.\nCase: We explore topics discussed in ChatGPT policies across universities to uncover the main themes, grouping policies into clusters based on their emphasis (e.g., academic integrity, student support, instructor discretion).\n\n\n\n3.5.2.0.2 Sample Research Questions\n\nRQ1: What are the main themes or topics present in ChatGPT policy texts across universities?\nRQ2: How do policies cluster based on their emphasis (e.g., focus on integrity, educational support, flexibility)?\n\n\n\n\n3.5.2.0.3 Analysis\n\nStep 1: Load Required Libraries\nInstall and load packages for text processing and unsupervised machine learning.\n\n\n   # Install packages\n   install.packages(c(\"tm\", \"text2vec\", \"topicmodels\"))\n\n   # Load libraries\n   library(tm)\n   library(text2vec)\n   library(topicmodels)\n\n\nStep 2: Data Preparation\nPreprocess the text data and create a document-term matrix.\n\n\n   # Load dataset\n   policy_data &lt;- read.csv(\"university_chatgpt_policies.csv\")\n\n   # Preprocess text\n   policy_data &lt;- policy_data %&gt;%\n     unnest_tokens(word, Policy_Text) %&gt;%\n     anti_join(stop_words)\n\n   # Create document-term matrix\n   dtm &lt;- policy_data %&gt;%\n     cast_dtm(document = University_ID, term = word, value = n)\n\n\nStep 3: Perform Topic Modeling (LDA)\nApply Latent Dirichlet Allocation (LDA) to identify topics within the text data.\n\n\n   # Set parameters for LDA\n   lda_model &lt;- LDA(dtm, k = 3, control = list(seed = 123))  # 3 topics\n\n   # Extract topics\n   topics &lt;- tidy(lda_model, matrix = \"beta\")\n\n\nStep 4: Visualize Top Terms in Each Topic\nDisplay the top terms associated with each topic.\n\n\n   # Extract top terms for each topic\n   top_terms &lt;- topics %&gt;%\n     group_by(topic) %&gt;%\n     slice_max(beta, n = 10) %&gt;%\n     ungroup() %&gt;%\n     arrange(topic, -beta)\n\n   # Plot top terms\n   ggplot(top_terms, aes(x = reorder_within(term, beta, topic), y = beta, fill = topic)) +\n     geom_col(show.legend = FALSE) +\n     facet_wrap(~ topic, scales = \"free_y\") +\n     coord_flip() +\n     scale_x_reordered() +\n     labs(title = \"Top Terms in Each Topic\", x = NULL, y = \"Beta\")\n\n\n\n\n3.5.2.0.4 Results and Discussions\n\nRQ1: What are the main themes or topics present in ChatGPT policy texts across universities?\nThe LDA model reveals three key topics: 1) academic integrity, 2) student support, and 3) instructor discretion. These topics represent the primary areas of concern and focus in university ChatGPT policies.\nRQ2: How do policies cluster based on their emphasis?\nThe topic clusters suggest that universities tend to focus on specific aspects: some emphasize academic integrity, while others prioritize support for learning and flexibility for instructors. This clustering can inform administrators about prevalent policy themes, aiding in policy alignment across institutions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Section 2 : Capturing and Analyzing Text Data with Computational Methods</span>"
    ]
  },
  {
    "objectID": "section-3.html",
    "href": "section-3.html",
    "title": "4  Section 3 Social Network Analyses (Relational Data)",
    "section": "",
    "text": "4.1 Abstract\nSocial network analysis (SNA) is the process of investigating social structures through the use of networks and graph theory. It is a technique used to map and measure relationships and flows between people, groups, organizations, computers, or other information/knowledge processing entities. SNA can be a useful tool for understanding the team structures, for example, in an online classroom. It can be an additional layer of understanding the outcomes (or predictors) of certain instructional interventions. Used this way SNA can be used to identify patterns and trends in social networks, as well as to understand how these networks operate. Additionally, SNA can be used to predict future behavior in social networks, and to design interventions that aim to improve the functioning of these networks.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Section 3 Social Network Analyses (Relational Data)</span>"
    ]
  },
  {
    "objectID": "section-4.html",
    "href": "section-4.html",
    "title": "5  Section 4 Secondary Analysis of Big Data (Numeric Data)",
    "section": "",
    "text": "5.1 Abstract\nThis section reviews how to access data that is primarily numeric/quantitative in nature, but from a different source and of a different nature than the data typically used by social scientists. Example data sets include international or national large-scale assessments (e.g., PISA, NAEP) and data from digital technologies (e.g., log-trace data from adaptive mathematics programs).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Section 4 Secondary Analysis of Big Data (Numeric Data)</span>"
    ]
  },
  {
    "objectID": "section-5.html",
    "href": "section-5.html",
    "title": "6  Section 5 Communication & Collaboration & Good Practices",
    "section": "",
    "text": "6.1 Abstract\nA key part of social science research (and, any analysis involving computational tools) is communicating about the output or findings. In this section, we describe how to communicate with colleagues or the wider world through the use of a variety of tools, especially R Markdown and git/GitHub. We also discuss how to collaborate on projects that involve computational methods and “good” (flexible yet principled) practices for doing so based on our experience and prior scholarship.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Section 5 Communication & Collaboration & Good Practices</span>"
    ]
  }
]