[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Social Science Cookbook with R",
    "section": "",
    "text": "Preface\nWelcome to Computational Social Science Cookbook with R!\nThe book is organized around six sections. Within these six sections are specific chapters, which serve as cookbook “entries”. While the section overviews (the first bullet point within each section) introduce the techniques or methodologies used in the section’s chapters, the entries (the subsequent bullet points) are intended to address a specific, narrow problem, as well a to provide a sample for researchers in writing their research questions, methods, results (and discussions) sections based on the analyses.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "section-1.html",
    "href": "section-1.html",
    "title": "Section 1 Getting Started",
    "section": "",
    "text": "Abstract: In this section, we will introduce the steps to get started with using R and RStudio. The section will also cover the key concepts in working with R such as packages, R markdown. We will also cover important steps in preparing data, which we will be using frequently in the rest of the book. Since these topics have been covered in so much more detail in other books, or other resources online, we will end the section with a list of key resources on these topics that are easily accessible online.\n\nSection Overview\nUsing R and RStudio\n\nFoundational skills: Data, packages, functions, projects\nReproducibility and R Markdown documents\nBase R: assignment, equality, dollar-sign indexing, xx\nR files\nProject setup within RStudio\n\nCore Data Wrangling Capabilities\n\nSelecting and renaming variables\nCreating new variables\nArranging variables\nJoining data",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Section 1 **Getting Started**</span>"
    ]
  },
  {
    "objectID": "section-2.html",
    "href": "section-2.html",
    "title": "Section 2 Capturing and Analyzing Text Data with Computational Methods",
    "section": "",
    "text": "2.1 Overview\nIn social sciences, analyzing text data is usually considered the “job” of qualitative researchers.Traditionally, qualitative research involves identifying patterns in non-numeric data, and this pattern recognition is typically done manually. This process is time-intensive but can yield rich research results. Traditional methods for analyzing text data involve human coding and can include direct sources (e.g., books, online texts) or indirect sources (e.g., interview transcripts).\nWith the advent of new software, we can capture and analyze text data in ways that were previously not possible. Modern data collection techniques include web scraping, accessing social media APIs, or downloading large online documents. Given the increased size of text data, analysis now requires computational approaches (e.g., dictionary-based, frequency-based, machine learning) that go beyond manual coding. These computational methods allow social scientists to ask new types of research questions, expanding the scope and depth of possible insights.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Section 2 **Capturing and Analyzing Text Data with Computational Methods**</span>"
    ]
  },
  {
    "objectID": "section-2.html#overview",
    "href": "section-2.html#overview",
    "title": "Section 2 Capturing and Analyzing Text Data with Computational Methods",
    "section": "",
    "text": "Disclaimer: While resources are available that discuss these analysis methods in depth, this book aims to provide a practical guide for social scientists, using data they will likely encounter. Our goal is to present a “cookbook” for guiding research projects through real-world examples.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Section 2 **Capturing and Analyzing Text Data with Computational Methods**</span>"
    ]
  },
  {
    "objectID": "section-2.html#accessing-text-data",
    "href": "section-2.html#accessing-text-data",
    "title": "Section 2 Capturing and Analyzing Text Data with Computational Methods",
    "section": "2.2 Accessing Text Data",
    "text": "2.2 Accessing Text Data\nFor conventional qualitative researcher, text data comes from the usual sources such as interview transcripts or existing documents. Nowadays, however, text can be found and collected in many different ways. For example, social media is rich with text data, likewise for faculty who are teaching course (especially online), every student writing can be seen as a piece of text data. In this section, we will cover a few basic ways of accessing text data. Please note that although we cover some prominent ways, this is by no means an exhaustive list. Therefore, please refer to the additional resources section at the end of the section to dive deeper.\n\n2.2.1 Web Scraping (Unstructured or API)\n\nWhat is Web Scraping?\nWeb scraping refers to the automated process of extracting data from web pages. It is particularly useful when dealing with extensive lists of websites that would be tedious to mine manually. A typical web scraping program follows these steps:\n\nLoads a webpage.\nDownloads the HTML or XML structure.\nIdentifies the desired data.\nConverts the data into a format suitable for analysis, such as a data frame.\n\nIn addition to text, web scraping can also be used to download other content types, such as audio-visual files.\n\n\nIs Web Scraping Legal?\nWeb scraping was common in the early days of the internet, but with the increasing value of data, legal norms have evolved. To avoid legal issues, check the “Terms of Service” for specific permissions on the website, often accessible via “robots.txt” files. Consult legal advice when in doubt.\n\n\nReading a Web Page into R\nOnce permissions are confirmed, the first step in web scraping is to download the webpage’s source code into R, typically using the rvest package by Hadley Wickham.\n\n# Install and load the rvest package\ninstall.packages(\"rvest\")\nlibrary(rvest)\n\nTo demonstrate, we will scrape a simple Wikipedia page. Static pages, which lack interactive elements like JavaScript, are simpler to scrape. You can view the page’s HTML source in your browser by selecting Developer Tools &gt; View Source.\n\n# Load the webpage\nwikipedia_page &lt;- read_html(\"https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000\")\n\n# Verify that the webpage loaded successfully\nwikipedia_page\n\n\n\nParsing HTML\nThe next challenge is extracting specific information from the HTML structure. HTML files have a “tree-like” format, allowing us to target particular sections. Use your browser’s “Developer Tools” to inspect elements and locate the data. Right-click the desired element and select Inspect to view its structure.\nTo isolate data sections within the HTML structure, identify the XPath or CSS selectors. For instance:\n\n# Extract specific section using XPath\nsection_of_wikipedia &lt;- html_node(wikipedia_page, xpath='//*[@id=\"mw-content-text\"]/div/table')\nhead(section_of_wikipedia)\n\nTo convert the extracted section into a data frame, use html_table():\n\n# Convert the extracted data into a table\nhealth_rankings &lt;- html_table(section_of_wikipedia)\nhead(health_rankings[ , (1:2)])  # Display the first two columns\n\n\n\nParsing with CSS Selectors\nFor more complex web pages, CSS selectors can be an alternative to XPath. Tools like Selector Gadget can help identify the required CSS selectors.\nFor example, to scrape event information from Duke University’s main page:\n\n# Load the webpage\nduke_page &lt;- read_html(\"https://www.duke.edu\")\n\n# Extract event information using CSS selector\nduke_events &lt;- html_nodes(duke_page, css=\"li:nth-child(1) .epsilon\")\nhtml_text(duke_events)\n\n\n\nScraping with Selenium\nFor tasks involving interactive actions (e.g., filling search fields), use RSelenium, which enables automated browser operations.\nTo set up Selenium, install the Java SE Development Kit and Docker. Then, start Selenium in R:\n\n# Install and load RSelenium\ninstall.packages(\"RSelenium\")\nlibrary(RSelenium)\n\n# Start a Selenium session\nrD &lt;- rsDriver()\nremDr &lt;- rD$client\nremDr$navigate(\"https://www.duke.edu\")\n\nTo automate data entry, identify the CSS selector for the search box and input the query:\n\n# Find the search box element and enter a query\nsearch_box &lt;- remDr$findElement(using = 'css selector', 'fieldset input')\nsearch_box$sendKeysToElement(list(\"data science\", \"\\uE007\"))  # \"\\uE007\" represents Enter key\n\n\n\nWeb Scraping within a Loop\nTo scrape multiple pages, embed the code within a loop to automate tasks across different URLs. Since each site may have a unique structure, generalized scraping can be time-intensive and error-prone. Implement error handling to manage interruptions.\n\n\nWhen to Use Web Scraping\nWeb scraping is appropriate if:\n\nPage structure is consistent across sites: For example, a government site with date suffixes but a uniform layout.\nManual data collection is prohibitive: For extensive text or embedded tables.\n\nWhen feasible, consider alternatives like APIs or data-entry services (e.g., Amazon Mechanical Turk) for better efficiency and legal compliance.\n\n\n\nWhat is an API?\nAn Application Programming Interface (API) is a set of protocols that allows computers to communicate and exchange information. A common type is the REST API, where one machine sends a request, and another returns a response. APIs provide standardized access to data, services, and functionalities, making them essential in software development.\n\nWhen to Use an API\nAPIs are commonly used for:\n\nIntegrating with Third-Party Services: APIs connect applications to services like payment gateways or social media.\nAccessing Data: APIs retrieve data from systems or databases (e.g., real-time weather data).\nAutomating Tasks: APIs automate processes within applications, such as email marketing.\nBuilding New Applications: APIs allow developers to build new apps or services (e.g., a mapping API for navigation).\nStreamlining Workflows: APIs enable seamless communication and data exchange across systems.\n\n\n\nUsing Reddit API with RedditExtractoR\nReddit is a social media platform featuring a complex network of users and discussions, organized into “subreddits” by topic. RedditExtractoR, an R package, enables data extraction from Reddit to identify trends and analyze interactions.\n\n# Install and load RedditExtractoR\ninstall.packages(\"RedditExtractoR\")\nlibrary(RedditExtractoR)\n\n# Access data from the GenAI subreddit\nGenAI_reddit &lt;- find_thread_urls(subreddit = \"GenAI\", sort_by = \"new\", period = \"day\")\nview(GenAI_reddit)\n\n\n\n\n2.2.2 Audio Transcripts (Zoom, etc.)\nAudio transcripts are a rich source of text data, especially useful for capturing spoken content from meetings, interviews, or webinars. Many platforms, such as Zoom, provide automated transcription services that can be downloaded as text files for analysis. By processing these transcripts, researchers can analyze conversation themes, sentiment, or other linguistic features. Here’s how to access and prepare Zoom transcripts for analysis in R.\n\nKey Steps for Extracting Text Data from Audio Transcripts\n\nAccess the Zoom Transcript\n\nLog in to your Zoom account.\nNavigate to the “Recordings” section.\nSelect the recording you wish to analyze and download the “Audio Transcript” file.\n\nImport the Transcript into R\nOnce the file is downloaded, you can load it into R for analysis. Depending on the file format (usually a .txt file with tab or comma delimiters), use read.table(), read.csv(), or functions from the readr package to load the data.\n\n\n   # Load the transcript into R\n   transcript_data &lt;- read.table(\"path/to/your/zoom_transcript.txt\", sep = \"\\t\", header = TRUE)\n\nAdjust the sep parameter based on the delimiter used in the transcript file (typically \\t for tab-delimited files).\n\nData Cleaning (if necessary)\nClean up the text data to remove unnecessary characters, standardize formatting, and prepare it for further analysis.\n\nRemove Unwanted Characters\nUse gsub() to eliminate special characters and punctuation, keeping only alphanumeric characters and spaces.\n\n\n     # Remove special characters\n     transcript_data$text &lt;- gsub(\"[^a-zA-Z0-9 ]\", \"\", transcript_data$text)\n\n\nConvert Text to Lowercase\nStandardize text to lowercase for consistency in text analysis.\n\n\n\n     # Convert text to lowercase\n     transcript_data$text &lt;- tolower(transcript_data$text)\n\n\n\n\n2.2.3 PDF\nPDF files are a valuable source of text data, often found in research publications, government documents, and industry reports. We’ll explore two main methods for extracting text from PDFs:\n\nExtracting from Local PDF Files: This method involves accessing and parsing text from PDF files stored locally, providing tools and techniques to efficiently retrieve text data from offline documents.\nDownloading and Extracting PDF Files: This approach covers downloading PDFs from online sources and extracting their text. This method is useful for scraping publicly available documents from websites or databases for research purposes.\nPDF Data Extractor (PDE)\nFor more advanced PDF text extraction and processing, you can use the PDF Data Extractor (PDE) package. This package provides tools for extracting text data from complex PDF documents, supporting additional customization options for text extraction. PDE is a R package that easily extracts information and tables from PDF files. The PDE_analyzer_i() performs the sentence and table extraction while the included PDE_reader_i() allows the user-friendly visualization and quick-processing of the obtained results.\n\n\nSteps for Extracting Text from Local PDF Files\n\nInstall and Load the pdftools Package\nStart by installing and loading the pdftools package, which is specifically designed for reading and extracting text from PDF files in R.\n\n\n   install.packages(\"pdftools\")\n   library(pdftools)\n\n\nRead the PDF as a Text File\nUse the pdf_text() function to read the PDF file into R as a text object. This function returns each page as a separate string in a character vector.\n\n\n   txt &lt;- pdf_text(\"path/to/your/file.pdf\")\n\n\nExtract Text from a Specific Page\nTo access a particular page from the PDF, specify the page number in the text vector. For example, to extract text from page 24:\n\n\n   page_text &lt;- txt[24]  # page 24\n\n\nExtract Rows into a List\nIf the page contains a table or structured text, use the scan() function to read each row as a separate element in a list. The textConnection() function converts the page text for processing.\n\n\n   rows &lt;- scan(textConnection(page_text), what = \"character\", sep = \"\\n\")\n\n\nSplit Rows into Cells\nTo further parse each row, split it into cells by specifying the delimiter, such as whitespace (using \"\\\\s+\"). This converts each row into a list of individual cells.\n\n\n   row &lt;- unlist(strsplit(rows[24], \"\\\\s+\"))  # Example with the 24th row\n\n\n\nSteps for Downloading and Extracting Text from PDF Files\n\nDownload the PDF from the Web\nUse the download.file() function to download the PDF file from a specified URL. Set the mode to \"wb\" (write binary) to ensure the file is saved correctly.\n\n\n   link &lt;- paste0(\n     \"http://www.singstat.gov.sg/docs/\",\n     \"default-source/default-document-library/\",\n     \"publications/publications_and_papers/\",\n     \"cop2010/census_2010_release3/\",\n     \"cop2010sr3.pdf\"\n   )\n   download.file(link, \"census2010_3.pdf\", mode = \"wb\")\n\n\nRead the PDF as a Text File\nAfter downloading, read the PDF into R as a text object using the pdf_text() function from the pdftools package. Each page of the PDF will be stored as a string in a character vector.\n\n\n   txt &lt;- pdf_text(\"census2010_3.pdf\")\n\n\nExtract Text from a Specific Page\nAccess the desired page (e.g., page 24) by specifying the page number in the character vector.\n\n\n   page_text &lt;- txt[24]  # Page 24\n\n\nExtract Rows into a List\nUse the scan() function to split the page text into rows, with each row representing a line of text in the PDF. This creates a list where each line from the page is an element.\n\n\n   rows &lt;- scan(textConnection(page_text), what = \"character\", sep = \"\\n\")\n\n\nLoop Through Rows and Extract Data\nStarting from a specific row (e.g., row 7), loop over each row. For each row:\n\nSplit the text by spaces (\"\\\\s+\") using strsplit().\nConvert the result to a vector with unlist().\nIf the third cell in the row is not empty, store the second cell as name and the third cell as total, converting it to a numeric format after removing commas.\n\n\n\n   name &lt;- c()\n   total &lt;- c()\n\n   for (i in 7:length(rows)) {\n     row &lt;- unlist(strsplit(rows[i], \"\\\\s+\"))\n     if (!is.na(row[3])) {\n       name &lt;- c(name, row[2])\n       total &lt;- c(total, as.numeric(gsub(\",\", \"\", row[3])))\n     }\n   }\n\n\n\n\n2.2.4 Survey, Discussions, etc.\nSurveys and discussion posts are valuable sources of text data in social science research, providing insights into participants’ perspectives, opinions, and experiences. These data sources often come from open-ended survey responses, online discussion boards, or educational platforms. Extracting and preparing text data from these sources can reveal recurring themes, sentiment, and other patterns that support both quantitative and qualitative analysis. Below are key steps and code examples for processing text data from surveys and discussions in R.\n\nKey Steps for Processing Survey and Discussion Text Data\n\nLoad the Data\nSurvey and discussion data are typically stored in spreadsheet formats like CSV. Begin by loading this data into R for processing. Here, readr is used for reading CSV files with read_csv().\n\n\n   # Install and load necessary packages\n   install.packages(\"readr\")\n   library(readr)\n   \n   # Load data\n   survey_data &lt;- read_csv(\"path/to/your/survey_data.csv\")\n\n\nExtract Text Columns\nIdentify and isolate the relevant text columns. For example, if the text data is in a column named “Response,” you can create a new vector for analysis.\n\n\n   # Extract text data from the specified column\n   text_data &lt;- survey_data$Response\n\n\nData Cleaning\nPrepare the text data by cleaning it, removing any unnecessary characters, and standardizing the text. This includes removing punctuation, converting text to lowercase, and handling extra whitespace.\n\nRemove Unwanted Characters\nUse gsub() from base R to remove any non-alphanumeric characters, retaining only words and spaces.\n\n\n\n     # Remove special characters\n     text_data &lt;- gsub(\"[^a-zA-Z0-9 ]\", \"\", text_data)\n\n\nConvert to Lowercase\nStandardize the text by converting all characters to lowercase.\n\n\n     # Convert text to lowercase\n     text_data &lt;- tolower(text_data)\n\n\nRemove Extra Whitespace\nRemove any extra whitespace that may be left after cleaning.\n\n\n     # Remove extra spaces\n     text_data &lt;- gsub(\"\\\\s+\", \" \", text_data)\n\n\nTokenization and Word Counting (Optional)\nIf further analysis is needed, such as frequency-based analysis, split the text into individual words (tokenization) or count the occurrence of specific words. Here, dplyr is used to organize the word counts.\n\n\n   # Install and load necessary packages\n   install.packages(\"dplyr\")\n   library(dplyr)\n   \n   # Tokenize and count words\n   word_count &lt;- strsplit(text_data, \" \") %&gt;%\n                 unlist() %&gt;%\n                 table() %&gt;%\n                 as.data.frame()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Section 2 **Capturing and Analyzing Text Data with Computational Methods**</span>"
    ]
  },
  {
    "objectID": "section-2.html#frequency-based-analysis",
    "href": "section-2.html#frequency-based-analysis",
    "title": "Section 2 Capturing and Analyzing Text Data with Computational Methods",
    "section": "2.3 Frequency-based Analysis",
    "text": "2.3 Frequency-based Analysis\nIn the following section, we will provide a “recipe” for the social scientist interested in these new methods of analyzing text data to get you from the initial stages of getting the data to running the analyses and the write up. Often left out is also a research question that suits or requires a method. Since we have a data and method-centric approach here, we will backtrack and also provide a research question, so that you can model after it in your own work. Finally, we will provide a sample results and discussions section.\n\n2.3.1 Purpose\nThe purpose of the frequency-based approach is to count the number of words as they appear in a text file, whether it is a collection of tweets, documents, or interview transcripts. This approach aligns with the frequency-coding method (e.g., Saldana) and can supplement human coding by revealing the most/least commonly occurring words, which can then be compared across dependent variables.\n\nCase Study: Frequency-Based Analysis of GenAI USage Guidelines in Higher Education\nAs AI writing tools like ChatGPT become more prevalent, educators are working to understand how best to integrate them within academic settings, while many students and instructors remain uncertain about acceptable use cases. Our research into AI usage guidelines from the top 100 universities in North America aims to identify prominent themes and concerns in institutional policies regarding GenAI.\n\n\n\n2.3.2 Sample Research Questions\nTo investigate the nature of AI use policies within higher education institutions, in this study, our research questions are:\n\nRQ1: What are the most frequently mentioned words in university GenAI writing usage policies?\nRQ2: Which keywords reflect common concerns or focal points related to GenAI writing usage in academic settings?\n\n\n\n2.3.3 Sample Methods\n\nData Source\nThe dataset consists of publicly available AI policy texts from 100 universities(USA), the data has been downloaded and saved as a CSV file for analysis. –we might have to write more here to model how a research should be describing the data from its acquisition to use for research.\n\n\nData Analysis\nIn order to analyze the data we used xyz, —-let’s provide a sample write up for the researcher to adapt.\n\n\n\n\n\n\n2.3.4 Analysis\n\nStep 1: Load Required Libraries\nInstall and load libraries for data processing, visualization, and word cloud generation.\n\n# Install necessary packages if not already installed\n#install.packages(c(\"tibble\", \"dplyr\", \"tidytext\", \"ggplot2\", \"viridis\",\"tm\",wordcloud\" \"wordcloud2\",\"webshot\"))\n\n# Load libraries\nlibrary(readr)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(tm)\nlibrary(wordcloud)\nlibrary(wordcloud2)\nlibrary(webshot)\n\n\n\nStep 2: Load Data\nRead the CSV file containing policy texts from the top 100 universities.\n\n# Load the dataset\nuniversity_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\")\n\n\n\nStep 3: Tokenize Text and Count Word Frequency\nProcess the text data by tokenizing words, removing stop words, and counting word occurrences.\n\n# Tokenize text, remove stop words, and count word frequencies\nword_frequency &lt;- university_policies %&gt;%\n  unnest_tokens(word, Stance) %&gt;%  # Tokenize the 'Policy_Text' column\n  anti_join(stop_words) %&gt;%             # Remove common stop words\n  count(word, sort = TRUE)              # Count and sort words by frequency\nword_frequency\n\n# A tibble: 1,108 × 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 ai            124\n 2 students      120\n 3 chatgpt        80\n 4 tools          73\n 5 academic       68\n 6 policy         35\n 7 assignments    34\n 8 instructors    34\n 9 integrity      33\n10 student        32\n# ℹ 1,098 more rows\n\n\n\n\nStep 4: Create a Word Cloud\nGenerate a word cloud to visualize the frequency of words in a circular shape.\n\n# Create and display the GenAI usage Stance wordcloud\n\nwordcloud(words = word_frequency$word, freq = word_frequency$n, scale = c(4, 0.5), random.order = FALSE, min.freq = 10, colors = brewer.pal(8, \"Dark2\"), rot.per = 0.35)\n\n\n\n\n\n\n\n\n\n\nStep 5: Visualize Top 12 Words in University Policies\nSelect the top 12 most frequent words and create a bar chart to visualize the distribution.\n\n# Select the top 12 words\ntop_words &lt;- word_frequency %&gt;% slice(1:12)\n\n# Generate the bar chart\npolicy_word_chart &lt;- ggplot(top_words, aes(x = reorder(word, n), y = n, fill = word)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(\n    title = \"Top Words in University GenAI Policies\",\n    x = NULL,\n    y = \"Frequency\",\n    caption = \"Source: University AI Policy Text\",\n    fill = \"Word\"\n  ) +\n  scale_fill_viridis(discrete = TRUE) +\n  geom_text(aes(label = n), vjust = 0.5, hjust = -0.1, size = 3)\n\n# Print the bar chart\nprint(policy_word_chart)\n\n\n\n\n\n\n\n\n\n\n\n2.3.5 Results and Discussions\n\nRQ1: What are the most frequently mentioned words in university GenAI writing usage policies?\nThe results of the frequency analysis showed that keywords such as “assignment,” “student,” and “writing” were among the most commonly mentioned terms across AI policies at 100 universities. This emphasis reflects a focus on using AI tools to support student learning and enhance teaching content. The frequent mention of these words suggests that institutions are considering the role of AI in academic assignments and course design, indicating a strategic commitment to integrating AI within educational tasks and student interactions.\nRQ2: Which keywords reflect common concerns or focal points related to GenAI writing usage in academic settings?\nThe analysis of the top 12 frequently mentioned terms highlighted additional focal points, including “tool,” “academic,” “instructor,” “integrity,” and “expectations.” These terms reveal concerns around the ethical use of AI tools, the need for clarity in academic applications, and the central role of instructors in AI policy implementation. Keywords like “integrity” and “expectations” emphasize the importance of maintaining academic standards and setting clear guidelines for AI use in classrooms, while “instructor” underscores the influence faculty members have in shaping AI-related practices. Together, these terms reflect a commitment to transparent policies that support ethical and effective AI integration, enhancing the academic experience for students.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Section 2 **Capturing and Analyzing Text Data with Computational Methods**</span>"
    ]
  },
  {
    "objectID": "section-2.html#dictionary-based-analysis",
    "href": "section-2.html#dictionary-based-analysis",
    "title": "Section 2 Capturing and Analyzing Text Data with Computational Methods",
    "section": "2.4 Dictionary-based Analysis",
    "text": "2.4 Dictionary-based Analysis\n\n2.4.1 Purpose\nThe purpose of dictionary-based analysis in text data is to assess the presence of predefined categories, like emotions or sentiments, within the text using lexicons or dictionaries. This approach allows researchers to quantify qualitative aspects, such as positive or negative sentiment, based on specific words that correspond to these categories.\nCase:\nIn this analysis, we examine the stance of 100 universities on the use of GenAI by applying the Bing sentiment dictionary. By analyzing sentiment scores, we aim to identify the general tone in these policies, indicating whether the institutions’ attitudes toward GenAI are predominantly positive or negative.\n\n\n2.4.2 Sample Research Questions\n\nRQ: What is the dominant sentiment expressed in GenAI policy texts across universities, and is it primarily positive or negative?\n\n\n\n2.4.3 Analysis\n\nStep 1: Install and Load Necessary Libraries\nFirst, install and load the required packages for text processing and visualization.\n\n# Install necessary packages if not already installed\n#install.packages(c(\"tidytext\",\"tidyverse\" \"dplyr\", \"ggplot2\", \"tidyr\"))\n\n# Load libraries\nlibrary(tidytext)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\n\nStep 2: Load and Prepare Data(same as 2.3)\nLoad the GenAI policy stance data from a CSV file. Be sure to update the file path as needed. we use the same data as 2.3.\n\n# Load the dataset (replace \"University_GenAI_Policy_Stance.csv\" with the actual file path)\nuniversity_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\")\n# Tokenize text, remove stop words, and count word frequencies\nword_frequency &lt;- university_policies %&gt;%\n  unnest_tokens(word, Stance) %&gt;%  # Tokenize the 'Policy_Text' column\n  anti_join(stop_words) %&gt;%             # Remove common stop words\n  count(word, sort = TRUE)              # Count and sort words by frequency\nword_frequency\n\n\n\nStep 3: Tokenize Text Data and Apply Sentiment Dictionary\nTokenize the policy text data to separate individual words. Then, use the Bing sentiment dictionary to label each word as positive or negative.\n\n# Tokenize 'Stance' column and apply Bing sentiment dictionary\nsentiment_scores &lt;- word_frequency %&gt;%\n  inner_join(get_sentiments(\"bing\")) %&gt;% # Join with Bing sentiment lexicon\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%\n  mutate(sentiment_score = positive - negative) # Calculate net sentiment score\n\nsentiment_scores\n\n# A tibble: 142 × 4\n   word         positive negative sentiment_score\n   &lt;chr&gt;           &lt;int&gt;    &lt;int&gt;           &lt;int&gt;\n 1 honor              18        0              18\n 2 cheating            0       11             -11\n 3 dishonesty          0       10             -10\n 4 guidance            9        0               9\n 5 honesty             7        0               7\n 6 intelligence        7        0               7\n 7 transparent         7        0               7\n 8 violation           0        7              -7\n 9 encourage           6        0               6\n10 difficult           0        5              -5\n# ℹ 132 more rows\n\n\n\n\nStep 4: Create a Density Plot for Sentiment Distribution\nVisualize the distribution of sentiment scores with a density plot, showing the prevalence of positive and negative sentiments across university policies.\n\n# Generate a density plot of sentiment scores\ndensity_plot &lt;- ggplot(sentiment_scores, aes(x = sentiment_score, fill = sentiment_score &gt; 0)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"red\", \"green\"), name = \"Sentiment\",\n                    labels = c(\"Negative\", \"Positive\")) +\n  labs(\n    title = \"Density Plot of University AI Policy Sentiment\",\n    x = \"Sentiment Score\",\n    y = \"Density\",\n    caption = \"Source: University Policy Text\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 20),\n    axis.text = element_text(size = 14),\n    axis.title = element_text(face = \"bold\", size = 16),\n    plot.caption = element_text(size = 12)\n  )\n\n# Print the plot\nprint(density_plot)\n\n\n\n\n\n\n\n\n\n\n\n2.4.4 Results and Discussions\n\nRQ: What is the dominant sentiment expressed in GenAI policy texts across universities, and is it primarily positive or negative?\nThe dictionary-based sentiment analysis reveals the prevailing sentiments in university policies on GenAI usage. Using the Bing lexicon to assign positive and negative scores, the density plot illustrates the distribution of sentiment scores across the 100 institutions.\nThe results indicate a balanced perspective with a slight tendency toward positive sentiment, as reflected by a higher density of positive scores. This analysis provides insights into the varying degrees of acceptance and caution universities adopt in their AI policy frameworks, demonstrating the diverse stances that shape institutional AI guidelines.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Section 2 **Capturing and Analyzing Text Data with Computational Methods**</span>"
    ]
  },
  {
    "objectID": "section-2.html#clustering-based-analysis",
    "href": "section-2.html#clustering-based-analysis",
    "title": "Section 2 Capturing and Analyzing Text Data with Computational Methods",
    "section": "2.5 Clustering-Based Analysis",
    "text": "2.5 Clustering-Based Analysis\nClustering-based analysis involves grouping similar text documents or text segments into clusters based on their underlying topics or themes. This approach is particularly useful for identifying dominant themes in text data, such as university AI policy documents.\n\n2.5.1 Purpose\nPurpose: The goal of clustering-based analysis is to uncover latent themes in text data using unsupervised machine learning techniques. Topic modeling is one popular method for clustering documents into groups based on their content.\nCase: Using the GenAI policy texts from 100 universities, we apply Latent Dirichlet\nAllocation (LDA) to identify dominant themes in these policy documents. This analysis will help categorize policies into overarching themes, such as academic integrity, student support, and instructor discretion.\n\n\n2.5.2 Sample Research Questions\n\nRQ1: What are the prominent themes present in university policies regarding GenAI usage Stance?\nRQ2: How do these themes reflect the key concerns or opportunities for intergrating GenAI in higher education?\n\n\n\n2.5.3 Analysis\n\nStep 1: Install and Load Necessary Libraries\nInstall and load the required libraries for text processing and topic modeling.\n\n# Install necessary packages\n#install.packages(c(\"dplyr\", \"tidytext\", \"topicmodels\", \"ggplot2\"))\n\n# Load libraries\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(topicmodels)\nlibrary(ggplot2)\n\n\n\nStep 2: Prepare the Data\nLoad the data and create a document-term matrix (DTM) for topic modeling.\n\n# Load the dataset\nuniversity_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\")\n\n# Tokenize text data and count word frequencies\nword_frequency &lt;- university_policies %&gt;%\n  unnest_tokens(word, Stance) %&gt;%  # same as section 2.3\n  anti_join(stop_words) %&gt;%             # Remove common stop words\n  count(word, sort = TRUE)              # Count and sort words by frequency\nword_frequency\n\n# A tibble: 1,108 × 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 ai            124\n 2 students      120\n 3 chatgpt        80\n 4 tools          73\n 5 academic       68\n 6 policy         35\n 7 assignments    34\n 8 instructors    34\n 9 integrity      33\n10 student        32\n# ℹ 1,098 more rows\n\n# Creating Documents - Word Frequency Matrix\ngpt_dtm &lt;- word_frequency %&gt;%\n  group_by(word) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  ungroup() %&gt;%\n  cast_dtm(document = \"id\", term = \"word\", value = \"n\")\n\n\nlibrary(topicmodels)\nlibrary(ggplot2)\n\n# Define range of k values\nk_values &lt;- c(2, 3, 4, 5)\n\n# Initialize a data frame to store perplexities\nperplexities &lt;- data.frame(k = integer(), perplexity = numeric())\n\n# Calculate perplexity for each k\nfor (k in k_values) {\n  lda_model &lt;- LDA(gpt_dtm, k = k, control = list(seed = 1234))  # Fit LDA model\n  perplexity_score &lt;- perplexity(lda_model, gpt_dtm)            # Calculate perplexity\n  perplexities &lt;- rbind(perplexities, data.frame(k = k, perplexity = perplexity_score))\n}\n\n# Plot perplexity vs number of topics\nggplot(perplexities, aes(x = k, y = perplexity)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Perplexity vs Number of Topics\",\n    x = \"Number of Topics (k)\",\n    y = \"Perplexity\"\n  ) +\n  theme_minimal()\n\n\n\nStep 3: Fit the LDA Model\nFit an LDA model with k = 3 topics.\n\n# Converting document-word frequency matrices to sparse matrices\ngpt_dtm_sparse&lt;- as(gpt_dtm, \"matrix\")\n\n# Fit the LDA model\nlda_model &lt;- LDA(gpt_dtm_sparse, k = 3, control = list(seed = 1234))\n\n# View model results\ngpt_policy_topics_k3 &lt;- tidy(lda_model, matrix = \"beta\")\n\nprint(gpt_policy_topics_k3)\n\n# A tibble: 3,324 × 3\n   topic term        beta\n   &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1     1 ai       0.0408 \n 2     2 ai       0.00593\n 3     3 ai       0.0650 \n 4     1 students 0.0435 \n 5     2 students 0.0635 \n 6     3 students 0.0151 \n 7     1 chatgpt  0.0396 \n 8     2 chatgpt  0.0248 \n 9     3 chatgpt  0.0126 \n10     1 tools    0.0197 \n# ℹ 3,314 more rows\n\n\n\n\nStep 4: Visualize Topics\nExtract the top terms for each topic and visualize them.\n\n# Visualizing top terms for each topic\ngpt_policy_ap_top_terms_k3 &lt;- gpt_policy_topics_k3 %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\ngpt_policy_ap_top_terms_k3 %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\n\n\n\n\n\n\n2.5.4 Results and Discussions\n\nResearch Question 1:\nWhat are the prominent themes present in university policies regarding GenAI usage?\nAnswer:\nThe topic modeling analysis revealed three distinct themes in the university GenAI policies:\n\nTheme 1: Student-Centric Guidelines and Ethical Considerations\n\nKey terms: students, integrity, tools, instructors, assignment\nThis theme emphasizes student usage of GenAI in academic settings, with a focus on ethics (integrity) and guidelines for instructors to manage assignments involving AI tools.\n\nTheme 2: Academic Standards and Faculty Expectations\n\nKey terms: students, academic, faculty, honor, expectations\nThis theme focuses on maintaining academic integrity and clarifying expectations for faculty and students regarding GenAI usage in assignments and assessments.\n\nTheme 3: Policy-Level Governance and Technology Integration\n\nKey terms: ai, tools, policy, learning, generative\nThis theme revolves around institutional policies on AI integration, highlighting broader governance strategies and how generative AI (like GenAI) fits into learning environments.\n\n\n\n\nResearch Question 2:\nHow do these themes reflect the key concerns or opportunities for integrating GenAI in higher education?\nAnswer:\nThe identified themes reflect both concerns and opportunities:\n\nConcerns:\nTheme 1: Highlights the ethical challenges, such as ensuring academic integrity when students use AI tools in their coursework. Institutions are keen on setting clear guidelines for both students and instructors to avoid misuse.\nTheme 2: Underlines the potential for conflict between maintaining academic standards (honor, expectations) and leveraging AI to support learning. This shows a cautious approach to integrating AI while upholding traditional values.\nTheme 3: Raises policy-level questions on AI governance, such as whether existing institutional frameworks are adequate to regulate emerging generative AI technologies.\nOpportunities:\nTheme 1: Presents a chance to redefine how students interact with AI tools to foster responsible and innovative usage, particularly for assignments and creative tasks.\nTheme 2: Encourages collaboration between faculty and administration to develop robust expectations and support systems for integrating AI in the classroom.\nTheme 3: Offers a strategic opportunity for universities to lead in AI adoption by establishing comprehensive policies that guide AI’s role in education and research.\n\n\n\nDiscussion:\nThe topic modeling results suggest that universities are navigating a complex landscape of opportunities and challenges as they incorporate GenAI into academic contexts. While student-centric policies aim to balance innovation with ethical considerations, institutional-level themes signal the need for governance frameworks to ensure responsible AI use. These findings indicate that higher education institutions are positioned to play a pivotal role in shaping the future of generative AI in learning, provided they address the ethical, pedagogical, and policy challenges identified in this analysis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Section 2 **Capturing and Analyzing Text Data with Computational Methods**</span>"
    ]
  },
  {
    "objectID": "section-3.html",
    "href": "section-3.html",
    "title": "Section 3 Multimodal Data (Images, Video, Audio)",
    "section": "",
    "text": "Abstract: Conventionally, for us (the social scientists) we start the research process by generating research questions based on our previous knowledge and theories in the field and that is still the way to go about it. But, it is also possible that our observations of the world can guide our research questions. In this section, we will discuss how social scientists can look beyond conventional data types, and learn about capturing and analyzing multimodal data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Section 3 **Multimodal Data (Images, Video, Audio)**</span>"
    ]
  },
  {
    "objectID": "section-4.html",
    "href": "section-4.html",
    "title": "Section 4 Social Network Analyses (Relational Data)",
    "section": "",
    "text": "Abstract: Social network analysis (SNA) is the process of investigating social structures through the use of networks and graph theory. It is a technique used to map and measure relationships and flows between people, groups, organizations, computers, or other information/knowledge processing entities. SNA can be a useful tool for understanding the team structures, for example, in an online classroom. It can be an additional layer of understanding the outcomes (or predictors) of certain instructional interventions. Used this way SNA can be used to identify patterns and trends in social networks, as well as to understand how these networks operate. Additionally, SNA can be used to predict future behavior in social networks, and to design interventions that aim to improve the functioning of these networks.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Section 4 **Social Network Analyses (Relational Data)**</span>"
    ]
  },
  {
    "objectID": "section-5.html",
    "href": "section-5.html",
    "title": "Section 5 Secondary Analysis of Big Data (Numeric Data)",
    "section": "",
    "text": "5.1 Overview\nIn social science research, data is traditionally sourced from small-scale surveys, experiments, or qualitative studies. However, the rise of big data offers researchers opportunities to explore numeric and quantitative datasets of unprecedented scale and variety. This chapter discusses how to access and analyze large-scale datasets like international assessments (e.g., PISA, NAEP) and digital log-trace data (e.g., Open University Learning Analytics Dataset (OULAD)). These secondary data sources enable novel research questions and methods, particularly when paired with machine learning and statistical modeling approaches.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Section 5 **Secondary Analysis of Big Data (Numeric Data)**</span>"
    ]
  },
  {
    "objectID": "section-5.html#accessing-big-data-broadening-the-horizon",
    "href": "section-5.html#accessing-big-data-broadening-the-horizon",
    "title": "Section 5 Secondary Analysis of Big Data (Numeric Data)",
    "section": "5.2 Accessing Big data (Broadening the Horizon)",
    "text": "5.2 Accessing Big data (Broadening the Horizon)\n\n5.2.1 Big Data\n\nAccessing PISA Data\nThe Programme for International Student Assessment (PISA) is a widely used dataset for large-scale educational research. It assesses 15-year-old students’ knowledge and skills in reading, mathematics, and science across multiple countries. Researchers can access PISA data through various methods:\n\n1. Direct Download from the Official Website\nThe OECD provides direct access to PISA data files via its official website. Researchers can download data for specific years and cycles. Data files are typically provided in .csv or .sav (SPSS) formats, along with detailed documentation.\n\nSteps to Access PISA Data from the OECD Website:\n\nVisit the OECD PISA website.\nNavigate to the “Data” section.\nSelect the desired assessment year (e.g., 2022).\nDownload the data and accompanying codebooks.\n\n\n\n\n2. Using the OECD R Package\nThe OECD R package provides a direct interface to download and explore datasets published by the OECD, including PISA.\n\nSteps to Use the OECD Package:\n\nInstall and load the OECD package.\nUse the getOECD() function to fetch PISA data.\n\n\n\n# Install and load the OECD package\ninstall.packages(\"OECD\")\nlibrary(OECD)\n\n# Fetch PISA data for the 2018 cycle\npisa_data &lt;- getOECD(\"pisa\", years = \"2022\")\n\n# Display a summary of the data\nsummary(pisa_data)\n\n\n\n3. Using the Edsurvey R Package\nThe Edsurvey package is designed specifically for analyzing large-scale assessment data, including PISA. It allows for complex statistical modeling and supports handling weights and replicate weights used in PISA.\n\nSteps to Use the Edsurvey Package:\n\nInstall and load the Edsurvey package.\nDownload the PISA data from the OECD website and provide the path to the .sav files.\nLoad the data into R using readPISA().\n\n\n\n# Install and load the Edsurvey package\ninstall.packages(\"Edsurvey\")\nlibrary(Edsurvey)\n\n# Read PISA data from a local file\npisa_data &lt;- readPISA(\"path/to/PISA2022Student.sav\")\n\n# Display the structure of the dataset\nstr(pisa_data)\n\n\n\nComparison of Methods\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nDisadvantages\n\n\n\n\nDirect Download\nFull access to all raw data and documentation.\nRequires manual processing and cleaning.\n\n\nOECD Package\nEasy to use for downloading specific datasets.\nLimited to OECD-published formats.\n\n\nEdsurvey Package\nSupports advanced statistical analysis and weights.\nRequires additional setup and dependencies.\n\n\n\n\n\n\nAccessing IPEDS Data\nThe Integrated Postsecondary Education Data System (IPEDS) is a comprehensive source of data on U.S. colleges, universities, and technical and vocational institutions. It provides data on enrollments, completions, graduation rates, faculty, finances, and more. Researchers and policymakers widely use IPEDS data to analyze trends in higher education.\nThere are several ways to access IPEDS data, depending on the user’s needs and technical proficiency.\n\n1. Direct Download from the NCES Website\nThe most straightforward way to access IPEDS data is by downloading it directly from the National Center for Education Statistics (NCES) website.\n\n\nSteps to Access IPEDS Data:\n\nVisit the IPEDS Data Center.\nClick on “Use the Data” and navigate to the “Download IPEDS Data Files” section.\nSelect the desired data year and survey component (e.g., Fall Enrollment, Graduation Rates).\nDownload the data files, typically provided in .csv or .xls format, along with accompanying codebooks.\n\n\n\n2. Using the ipeds R Package\nThe ipeds R package simplifies downloading and analyzing IPEDS data directly from R by connecting to the NCES data repository.\n\n\nSteps to Use the ipeds Package:\n\nInstall and load the ipeds package.\nUse the download_ipeds() function to fetch data for specific survey components and years.\n\n\n# Install and load the ipeds package\ninstall.packages(\"ipeds\")\nlibrary(ipeds)\n\n# Download IPEDS data for completions in 2021\nipeds_data &lt;- download_ipeds(\"C\", year = 2021)\n\n# View the structure of the downloaded data\nstr(ipeds_data)\n\n\n\n3. Using the tidycensus R Package\nThe tidycensus package, while primarily designed for Census data, can access specific IPEDS data linked to educational institutions.\n\n\nSteps to Use the tidycensus Package:\n\nInstall and load the tidycensus package.\nSet up a Census API key to access the data.\nQuery IPEDS data for specific institution-level information.\n\n\n# Install and load the tidycensus package\ninstall.packages(\"tidycensus\")\nlibrary(tidycensus)\n\n# Set Census API key (replace with your actual key)\ncensus_api_key(\"your_census_api_key\")\n\n# Fetch IPEDS-related data (e.g., institution information)\nipeds_institutions &lt;- get_acs(\n  geography = \"place\",\n  variables = \"B14002_003\",\n  year = 2021,\n  survey = \"acs5\"\n)\n\n# View the first few rows\nhead(ipeds_institutions)\n\n\n\n4. Using Online Tools\nIPEDS provides several online tools for querying and visualizing data without requiring programming skills.\n\n\nCommon Tools:\n\nIPEDS Data Explorer: Enables users to query and export customized datasets.\nTrend Generator: Allows users to visualize trends in key metrics over time.\nIPEDS Use the Data: Simplified tool for accessing pre-compiled datasets.\n\n\n\nSteps to Use the IPEDS Data Explorer:\n\nVisit the IPEDS Data Explorer.\nSelect variables of interest, such as institution type, enrollment size, or location.\nFilter results by years, institution categories, or other criteria.\nExport the results as a .csv or .xlsx file.\n\n\n\nComparison of Methods\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nDisadvantages\n\n\n\n\nDirect Download\nFull access to raw data and documentation.\nRequires manual data preparation and cleaning.\n\n\nipeds Package\nAutomated access to specific components.\nLimited flexibility for customized queries.\n\n\ntidycensus Package\nAllows integration with Census and ACS data.\nRequires API setup and advanced R skills.\n\n\nOnline Tools\nUser-friendly and suitable for non-coders.\nLimited to predefined queries and exports.\n\n\n\n\n\n\nAccessing Open University Learning Analytics Dataset (OULAD)\nThe Open University Learning Analytics Dataset (OULAD) is a publicly available dataset designed to support research in educational data mining and learning analytics. It includes student demographics, module information, interactions with the virtual learning environment (VLE), and assessment scores.\n\n\nSteps to Access OULAD Data\n\nVisit the OULAD Repository**\nThe dataset is hosted on the Open University’s Analytics Project. To access the data: 1. Navigate to the website. 2. Download the dataset as a .zip file. 3. Extract the .zip file to a local directory.\nThe dataset contains multiple CSV files: - studentInfo.csv: Student demographics and performance data. - studentVle.csv: Interactions with the VLE. - vle.csv: Details of learning resources. - studentAssessment.csv: Assessment scores.\n\n\nLoading OULAD Data in R\nOnce the data is downloaded and extracted, follow these steps to load and access it in R:\n\n\nStep 1: Install Required Packages\n\n# Install necessary packages\ninstall.packages(c(\"readr\", \"dplyr\"))\n\n\n\nStep 2: Load Data\nUse the readr package to read the CSV files into R.\n\n# Load required libraries\nlibrary(readr)\n\n# Define the path to the OULAD data\ndata_path &lt;- \"path/to/OULAD/\"\n\n# Load individual CSV files\nstudent_info &lt;- read_csv(file.path(data_path, \"studentInfo.csv\"))\nstudent_vle &lt;- read_csv(file.path(data_path, \"studentVle.csv\"))\nvle &lt;- read_csv(file.path(data_path, \"vle.csv\"))\nstudent_assessment &lt;- read_csv(file.path(data_path, \"studentAssessment.csv\"))\n\n\n\nStep 3: Preview the Data\nInspect the structure and contents of the datasets.\n\n# View the first few rows of student info\nhead(student_info)\n\n# Check the structure of the student VLE data\nstr(student_vle)\n\n\n\n\n\n5.2.2 Learning Analytics\n\nWhat is Learning Analytics?\nLearning Analytics (LA) refers to the measurement, collection, analysis, and reporting of data about learners and their contexts. The primary goal of LA is to understand and improve learning processes by identifying patterns, predicting outcomes, and providing actionable insights to educators, institutions, and learners.\nKey features of LA include: - Data Collection: Gathering information from digital platforms such as learning management systems (LMS) or external assessments. - Analysis: Using machine learning, statistical methods, or visualization tools to reveal trends and patterns. - Applications: Supporting personalized learning, enhancing institutional decision-making, and improving curriculum design.\n\n\nApplications of Learning Analytics in Big Data\nLearning analytics can be applied to large-scale educational datasets like PISA, IPEDS, and OULAD to uncover trends, predict outcomes, and guide interventions.\n\n1. PISA Data and Learning Analytics\n\nWhat it offers: Insights into international student performance in reading, math, and science, combined with contextual variables (e.g., socio-economic status).\nLA Applications:\n\nIdentifying key factors influencing performance across countries.\nPredicting the impact of ICT use on student achievement.\nSegmenting students into performance clusters for targeted interventions.\n\n\n\n\n2. IPEDS Data and Learning Analytics\n\nWhat it offers: U.S. institutional-level data on enrollment, graduation rates, tuition, and financial aid.\nLA Applications:\n\nAnalyzing trends in student demographics across institutions.\nPredicting enrollment patterns based on historical data.\nBenchmarking institutions to inform policymaking and funding decisions.\n\n\n\n\n3. OULAD and Learning Analytics\n\nWhat it offers: Rich data on student engagement with virtual learning environments (VLE), assessment scores, and demographic information.\nLA Applications:\n\nTracking student interactions with learning resources to predict course completion.\nModeling the relationship between VLE usage and final grades.\nDetecting early warning signs for at-risk students based on engagement metrics.\n\n\n\n\n\nWhy Learning Analytics Matters\nThe integration of Learning Analytics with big data enables researchers and practitioners to: - Personalize Learning: Tailor educational experiences to meet individual needs. - Improve Retention: Identify at-risk learners and implement timely interventions. - Enhance Decision-Making: Provide evidence-based recommendations for curriculum and policy adjustments.\nBy leveraging datasets like PISA, IPEDS, and OULAD, learning analytics can help bridge the gap between raw data and actionable insights, fostering a more equitable and effective educational landscape.\n\n\nSupervised Learning in Learning Analytics\nMachine Learning, particularly Supervised Learning, has become a cornerstone of Learning Analytics. Supervised learning models are trained on labeled datasets, where input features are mapped to known outcomes, enabling the prediction of new, unseen data.\n\nKey Concepts in Supervised Learning\n\nDefinition\nSupervised Learning is a subset of Machine Learning focused on learning a mapping between input variables (features) and output variables (labels or outcomes). Models trained on labeled data can predict outcomes for new data points.\nCommon Algorithms\n\nLinear Regression\nLogistic Regression\nDecision Trees and Random Forests\nNeural Networks\n\nApplications in Education\nSupervised learning is particularly effective in Learning Analytics for predicting:\n\nStudent performance\nDropout risks\nEnrollment trends\nCourse completion rates\n\n\n\n\n\nApplications of Supervised Learning with Big Data\n\n1. PISA Data and Supervised Learning\n\nGoal: Use demographic and contextual features to predict student performance in mathematics, reading, or science.\nExample: Train a linear regression model to identify the relationship between socioeconomic status and test scores.\n\n\n\n2. IPEDS Data and Supervised Learning\n\nGoal: Develop models to predict institutional enrollment rates based on financial aid, demographics, and program offerings.\nExample: Use logistic regression to forecast whether a student is likely to enroll based on financial aid eligibility.\n\n\n\n3. OULAD Data and Supervised Learning\n\nGoal: Predict student outcomes (e.g., pass/fail) based on engagement metrics like forum participation and assignment submissions.\nExample: Train a random forest model to classify students as “at-risk” or “not at-risk” based on weekly interaction data.\n\n\n\n\nChoosing the Right Supervised Learning Approach\nWhen applying supervised learning in Learning Analytics: 1. Define the Goal: Clearly articulate the outcome you want to predict (e.g., performance, enrollment, or engagement). 2. Select an Algorithm: Choose an appropriate model based on the data and prediction task. - For continuous outcomes, use regression models. - For categorical outcomes, use classification models like logistic regression or random forests. 3. Feature Engineering: Select and preprocess relevant features (e.g., attendance, demographics, assignment scores) to improve model accuracy. 4. Evaluate Model Performance: Use metrics such as accuracy, precision, recall, or R-squared to assess model effectiveness.\nIntegrating supervised learning techniques into Learning Analytics, researchers and practitioners can leverage big data to make data-driven predictions and decisions, ultimately enhancing educational outcomes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Section 5 **Secondary Analysis of Big Data (Numeric Data)**</span>"
    ]
  },
  {
    "objectID": "section-5.html#logistic-regression-ml",
    "href": "section-5.html#logistic-regression-ml",
    "title": "Section 5 Secondary Analysis of Big Data (Numeric Data)",
    "section": "5.3 Logistic Regression ML",
    "text": "5.3 Logistic Regression ML\n\n5.3.1 Purpose + CASE\n\nPurpose\nLogistic regression is a supervised learning technique widely used for binary classification tasks. It models the probability of an event occurring (e.g., success vs. failure) based on a set of predictor variables. Logistic regression is particularly effective in educational research for predicting outcomes such as retention, enrollment, or graduation rates.\n\n\nCASE: Predicting Graduation Rates\nThis case study is based on IPEDS data and inspired by Zong and Davis (2022). We predict graduation rates as a binary outcome (good_grad_rate) using institutional features such as total enrollment, admission rate, tuition fees, and average instructional staff salary.\n\n\n\n5.3.2 Sample Research Questions (RQs)\n\nRQ A: What institutional factors are associated with high graduation rates in U.S. four-year universities?\nRQ B: How accurately can we predict high graduation rates using institutional features with supervised machine learning?\n\n\n\n5.3.3 Analysis\n\nLoading Required Packages\nWe load necessary R packages for data wrangling, cleaning, and modeling.\n\n# Load necessary libraries for data cleaning, wrangling, and modeling\nlibrary(tidyverse)  # For data manipulation and visualization\nlibrary(tidymodels) # For machine learning workflows\nlibrary(janitor)    # For cleaning variable names\n\n\n\nLoading and Cleaning Data\nWe read the IPEDS dataset and clean column names for easier handling.\n\n# Read in IPEDS data from CSV file\nipeds &lt;- read_csv(\"data/ipeds-all-title-9-2022-data.csv\")\n\n# Clean column names for consistency and usability\nipeds &lt;- janitor::clean_names(ipeds)\n\n\n\nData Wrangling\nSelect relevant variables, filter the dataset, and create the dependent variable good_grad_rate.\n\n# Select and rename key variables; filter relevant institutions\nipeds &lt;- ipeds %&gt;%\n  select(\n    name = institution_name,                  # Institution name\n    total_enroll = drvef2022_total_enrollment, # Total enrollment\n    pct_admitted = drvadm2022_percent_admitted_total, # Admission percentage\n    tuition_fees = drvic2022_tuition_and_fees_2021_22, # Tuition fees\n    grad_rate = drvgr2022_graduation_rate_total_cohort, # Graduation rate\n    percent_fin_aid = sfa2122_percent_of_full_time_first_time_undergraduates_awarded_any_financial_aid, # Financial aid\n    avg_salary = drvhr2022_average_salary_equated_to_9_months_of_full_time_instructional_staff_all_ranks # Staff salary\n  ) %&gt;%\n  filter(!is.na(grad_rate)) %&gt;% # Remove rows with missing graduation rates\n  mutate(\n    # Create binary dependent variable for high graduation rates\n    good_grad_rate = if_else(grad_rate &gt; 62, 1, 0),\n    good_grad_rate = as.factor(good_grad_rate) # Convert to factor\n  )\n\n\n\nExploratory Data Analysis (EDA)\nVisualize the distribution of the graduation rate.\n\n# Plot a histogram of graduation rates\nipeds %&gt;%\n  ggplot(aes(x = grad_rate)) +\n  geom_histogram(bins = 20, fill = \"blue\", color = \"white\") +\n  labs(\n    title = \"Distribution of Graduation Rates\",\n    x = \"Graduation Rate\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\n\n\nLogistic Regression Model\nFit a logistic regression model to predict high graduation rates.\n\n# Fit logistic regression model\nm1 &lt;- glm(\n  good_grad_rate ~ total_enroll + pct_admitted + tuition_fees + percent_fin_aid + avg_salary,\n  data = ipeds,\n  family = \"binomial\" # Specify logistic regression for binary outcome\n)\n\n# View model summary\nsummary(m1)\n\n\nCall:\nglm(formula = good_grad_rate ~ total_enroll + pct_admitted + \n    tuition_fees + percent_fin_aid + avg_salary, family = \"binomial\", \n    data = ipeds)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -8.742e-01  6.237e-01  -1.402    0.161    \ntotal_enroll     3.350e-05  7.880e-06   4.251 2.13e-05 ***\npct_admitted    -1.407e-02  3.519e-03  -3.997 6.40e-05 ***\ntuition_fees     6.952e-05  4.965e-06  14.003  &lt; 2e-16 ***\npercent_fin_aid -2.960e-02  5.652e-03  -5.237 1.64e-07 ***\navg_salary       2.996e-05  3.870e-06   7.740 9.91e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2277  on 1706  degrees of freedom\nResidual deviance: 1632  on 1701  degrees of freedom\n  (3621 observations deleted due to missingness)\nAIC: 1644\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nSupervised ML Workflow\nUse the tidymodels framework to build a machine learning model.\n\n# Define recipe for the model (preprocessing steps)\nmy_rec &lt;- recipe(good_grad_rate ~ total_enroll + pct_admitted + tuition_fees + percent_fin_aid + avg_salary, data = ipeds)\n\n# Specify logistic regression model with tidymodels\nmy_mod &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%         # Use glm engine for logistic regression\n  set_mode(\"classification\")    # Specify binary classification task\n\n# Create workflow to connect recipe and model\nmy_wf &lt;- workflow() %&gt;%\n  add_recipe(my_rec) %&gt;%\n  add_model(my_mod)\n\n# Fit the logistic regression model\nfit_model &lt;- fit(my_wf, ipeds)\n\n# Generate predictions on the dataset\npredictions &lt;- predict(fit_model, ipeds) %&gt;%\n  bind_cols(ipeds) # Combine predictions with original data\n\n# Calculate and display accuracy\nmy_accuracy &lt;- predictions %&gt;%\n  metrics(truth = good_grad_rate, estimate = .pred_class) %&gt;%\n  filter(.metric == \"accuracy\")\n\nmy_accuracy\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.800\n\n\n\n\n\n5.3.4 Results and Discussions\n\nLogistic Regression Model (RQ A)\nThe logistic regression model was fitted to predict whether a university achieves a “good” graduation rate (i.e., graduation rate &gt; 62%) based on several institutional features. The model output is summarized below:\n\nCoefficients & Significance:\n\ntotal_enroll: Estimate = 3.35e-05, z = 4.251, p = 2.13e-05\nInterpretation: As total enrollment increases, the probability of a high graduation rate increases.\npct_admitted: Estimate = -1.407e-02, z = -3.997, p = 6.40e-05\nInterpretation: Higher admission percentages are associated with a lower likelihood of achieving a high graduation rate.\ntuition_fees: Estimate = 6.952e-05, z = 14.003, p &lt; 2e-16\nInterpretation: Higher tuition fees are strongly associated with higher graduation rates.\npercent_fin_aid: Estimate = -2.960e-02, z = -5.237, p = 1.64e-07\nInterpretation: A higher percentage of students receiving financial aid is associated with a lower probability of a good graduation rate.\navg_salary: Estimate = 2.996e-05, z = 7.740, p = 9.91e-15\nInterpretation: Higher average salaries for instructional staff are positively associated with high graduation rates.\n\nModel Fit Statistics:\n\nNull Deviance: 2277 (on 1706 degrees of freedom)\nResidual Deviance: 1632 (on 1701 degrees of freedom)\nAIC: 1644\nNote: 3621 observations were deleted due to missing values.\n\n\nOverall, the regression model demonstrates that several institutional factors are statistically significant predictors of graduation rates. In particular, tuition fees and avg_salary have a strong positive effect, while pct_admitted and percent_fin_aid show negative associations.\n\n\n\nSupervised ML Workflow Results (RQ B)\nUsing the tidymodels framework, we built a logistic regression model as part of a supervised machine learning workflow. The performance metric obtained is as follows:\n\nAccuracy: 80.02%\n\nThis indicates that the machine learning model correctly classified approximately 80% of the institutions as having either a good or not good graduation rate, based on the selected predictors.\n\n\n\nOverall Discussion\n\nSimilarities between Approaches:\n\nBoth the traditional logistic regression and the tidymodels workflow identified key predictors that influence graduation rates, such as total enrollment, admission percentage, tuition fees, financial aid percentage, and average staff salary.\nEach approach provides valuable insights: the regression model offers detailed coefficient estimates and significance levels, while the tidymodels workflow emphasizes predictive accuracy.\n\nDifferences between Approaches:\n\nInterpretability vs. Predictive Performance: The logistic regression output delivers interpretability through its coefficients and p-values, allowing us to understand the direction and magnitude of the relationships. In contrast, the supervised ML workflow focuses on achieving a robust predictive performance, evidenced by an 80% accuracy.\nHandling of Data: The traditional regression model summarizes the relationship between variables, whereas the ML workflow integrates data pre-processing, modeling, and validation into a cohesive framework.\n\n\nIn summary, our analyses indicate that institutional factors, particularly tuition fees and staff salaries, play a significant role in predicting graduation outcomes. The supervised ML approach, with an accuracy of around 80%, confirms the model’s practical utility in classifying institutions based on graduation performance. Both methods complement each other, providing a comprehensive understanding of the underlying dynamics that drive graduation rates in higher education.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Section 5 **Secondary Analysis of Big Data (Numeric Data)**</span>"
    ]
  },
  {
    "objectID": "section-5.html#random-forests-ml-on-interactions-data",
    "href": "section-5.html#random-forests-ml-on-interactions-data",
    "title": "Section 5 Secondary Analysis of Big Data (Numeric Data)",
    "section": "5.4 Random Forests ML on Interactions Data",
    "text": "5.4 Random Forests ML on Interactions Data\nIn this section, we explore a more sophisticated supervised learning approach—Random Forests—to model student interactions data from the Open University Learning Analytics Dataset (OULAD). Building on our earlier work with logistic regression and evaluation metrics, this case study examines whether a random forest model can improve predictive performance when leveraging clickstream data from the virtual learning environment (VLE).\n\n5.4.1 Purpose + CASE\n\nPurpose\nRandom Forests is an ensemble learning method that builds multiple decision trees and aggregates their results to improve prediction accuracy and control over-fitting. It is particularly well suited for complex, high-dimensional data such as student interaction (clickstream) data. This approach not only provides robust predictions but also offers insights into variable importance, helping us understand which features most influence student outcomes.\n\n\nCASE\nInspired by research on digital trace data (e.g., Rodriguez et al., 2021; Bosch, 2021), this case study uses pre-processed interactions data from OULAD. In our analysis, we focus on predicting whether a student will pass the course (a binary outcome) based on engineered features derived from clickstream data. These features include the total number of clicks (sum_clicks), summary statistics (mean and standard deviation of clicks), and linear trends over time (slope and intercept from clickstream patterns).\n\n\n\n\n5.4.2 Sample Research Questions\n\nRQ1: How accurately can a random forest model predict whether a student will pass a course using interactions data from OULAD?\nRQ2: Which interaction-based features (e.g., total clicks, click stream slope) are most important in predicting student outcomes?\nRQ3: How does the use of cross-validation (e.g., v-fold CV) influence the stability and generalizability of the random forest model on interactions data?\n\n\n\n\n5.4.3 Analysis\n\nLoading Required Packages\nBelow we load the necessary packages. (Note: All code chunks are set to eval=FALSE for instructional purposes.)\n\n# Load necessary libraries for data manipulation and modeling\nlibrary(tidyverse)      # Data wrangling and visualization\nlibrary(janitor)        # Cleaning variable names\nlibrary(tidymodels)     # Modeling workflow\nlibrary(ranger)         # Random forest implementation\nlibrary(vip)            # Variable importance plots\n\n\n\nLoading and Preparing the Data\nWe load the pre-filtered interactions data from OULAD along with a students-and-assessments file, then join them to create a complete dataset for modeling.\n\n# Load the interactions data (filtered for the first one-third of the semester)\ninteractions &lt;- read_csv(\"data/oulad-interactions-filtered.csv\")\n\n# Load the students and assessments data\nstudents_and_assessments &lt;- read_csv(\"data/oulad-students-and-assessments.csv\")\n\n# Create cut-off dates based on assessments data (using first quantile as intervention point)\nassessments &lt;- read_csv(\"data/oulad-assessments.csv\")\n\n# Create cut-off dates based on assessments data using the correct date column 'date_submitted'\ncode_module_dates &lt;- assessments %&gt;% \n    group_by(code_module, code_presentation) %&gt;% \n    summarize(quantile_cutoff_date = quantile(date_submitted, probs = 0.25, na.rm = TRUE), .groups = 'drop')\n\n# Join interactions with the cutoff dates and filter\ninteractions_joined &lt;- interactions %&gt;% \n    left_join(code_module_dates, by = c(\"code_module\", \"code_presentation\"))\n\n\ninteractions_joined &lt;- interactions_joined %&gt;% \n    select(-quantile_cutoff_date.x) %&gt;% \n    rename(quantile_cutoff_date = quantile_cutoff_date.y)\n\n\n\n# Filter interactions to include only those before the cutoff date\ninteractions_filtered &lt;- interactions_joined %&gt;% \n    filter(date &lt; quantile_cutoff_date)\n\n# Summarize interactions: total clicks, mean and standard deviation\ninteractions_summarized &lt;- interactions_filtered %&gt;% \n    group_by(id_student, code_module, code_presentation) %&gt;% \n    summarize(\n      sum_clicks = sum(sum_click),\n      sd_clicks = sd(sum_click), \n      mean_clicks = mean(sum_click)\n    )\n\n# (Optional) Further feature engineering: derive linear slopes from clickstream\nfit_model &lt;- function(data) {\n    tryCatch(\n        { \n            model &lt;- lm(sum_click ~ date, data = data)\n            tidy(model)\n        },\n        error = function(e) { tibble(term = NA, estimate = NA, std.error = NA, statistic = NA, p.value = NA) },\n        warning = function(w) { tibble(term = NA, estimate = NA, std.error = NA, statistic = NA, p.value = NA) }\n    )\n}\n\ninteractions_slopes &lt;- interactions_filtered %&gt;%\n    group_by(id_student, code_module, code_presentation) %&gt;%\n    nest() %&gt;%\n    mutate(model = map(data, fit_model)) %&gt;%\n    unnest(model) %&gt;%\n    ungroup() %&gt;%\n    select(code_module, code_presentation, id_student, term, estimate) %&gt;%\n    filter(!is.na(term)) %&gt;%\n    pivot_wider(names_from = term, values_from = estimate) %&gt;%\n    mutate_if(is.numeric, round, 4) %&gt;%\n    rename(intercept = `(Intercept)`, slope = date)\n\n# Join summarized clicks and slopes features\ninteractions_features &lt;- left_join(interactions_summarized, interactions_slopes, by = c(\"id_student\", \"code_module\", \"code_presentation\"))\n\n# Finally, join with students_and_assessments to get the outcome variable\nstudents_assessments_and_interactions &lt;- left_join(students_and_assessments, interactions_features, by = c(\"id_student\", \"code_module\", \"code_presentation\"))\n\n# Ensure outcome variable 'pass' is a factor\nstudents_assessments_and_interactions &lt;- students_assessments_and_interactions %&gt;% \n    mutate(pass = as.factor(pass))\n\n# Optional: Inspect the final dataset\nstudents_assessments_and_interactions %&gt;% \n    skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n32593\n\n\nNumber of columns\n22\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nfactor\n1\n\n\nnumeric\n13\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncode_module\n0\n1\n3\n3\n0\n7\n0\n\n\ncode_presentation\n0\n1\n5\n5\n0\n4\n0\n\n\ngender\n0\n1\n1\n1\n0\n2\n0\n\n\nregion\n0\n1\n5\n20\n0\n13\n0\n\n\nhighest_education\n0\n1\n15\n27\n0\n5\n0\n\n\nage_band\n0\n1\n4\n5\n0\n3\n0\n\n\ndisability\n0\n1\n1\n1\n0\n2\n0\n\n\nfinal_result\n0\n1\n4\n11\n0\n4\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\npass\n0\n1\nFALSE\n2\n0: 20232, 1: 12361\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nid_student\n0\n1.00\n706687.67\n549167.31\n3733.00\n508573.00\n590310.00\n644453.00\n2716795.00\n▅▇▁▁▁\n\n\nimd_band\n4627\n0.86\n5.62\n2.73\n1.00\n4.00\n6.00\n8.00\n10.00\n▃▇▇▆▆\n\n\nnum_of_prev_attempts\n0\n1.00\n0.16\n0.48\n0.00\n0.00\n0.00\n0.00\n6.00\n▇▁▁▁▁\n\n\nstudied_credits\n0\n1.00\n79.76\n41.07\n30.00\n60.00\n60.00\n120.00\n655.00\n▇▁▁▁▁\n\n\nmodule_presentation_length\n0\n1.00\n256.01\n13.18\n234.00\n241.00\n262.00\n268.00\n269.00\n▇▁▁▅▇\n\n\ndate_registration\n45\n1.00\n-69.41\n49.26\n-322.00\n-100.00\n-57.00\n-29.00\n167.00\n▁▂▇▃▁\n\n\ndate_unregistration\n22521\n0.31\n49.76\n82.46\n-365.00\n-2.00\n27.00\n109.00\n444.00\n▁▁▇▂▁\n\n\nmean_weighted_score\n7958\n0.76\n544.70\n381.39\n0.00\n160.00\n610.00\n875.00\n1512.00\n▇▃▇▅▁\n\n\nsum_clicks\n3495\n0.89\n474.93\n572.89\n1.00\n128.00\n295.50\n604.00\n10712.00\n▇▁▁▁▁\n\n\nsd_clicks\n3753\n0.88\n4.91\n5.51\n0.00\n2.37\n3.72\n6.44\n560.24\n▇▁▁▁▁\n\n\nmean_clicks\n3495\n0.89\n3.19\n1.30\n1.00\n2.33\n2.95\n3.82\n47.12\n▇▁▁▁▁\n\n\nintercept\n3640\n0.89\n3.04\n4.61\n-585.59\n2.15\n2.80\n3.66\n130.83\n▁▁▁▁▇\n\n\nslope\n4441\n0.86\n0.01\n0.22\n-12.17\n-0.01\n0.01\n0.03\n20.12\n▁▇▁▁▁\n\n\n\n\n\n\n\nCreating the Model Recipe\nWe build a recipe that includes the engineered features from interactions data along with other predictors from the students data.\n\nmy_rec2 &lt;- recipe(pass ~ disability +\n                     date_registration + \n                     gender +\n                     code_module +\n                     mean_weighted_score +\n                     sum_clicks + sd_clicks + mean_clicks + \n                     intercept + slope, \n                 data = students_assessments_and_interactions) %&gt;% \n    step_dummy(disability) %&gt;% \n    step_dummy(gender) %&gt;%  \n    step_dummy(code_module) %&gt;% \n    step_impute_knn(mean_weighted_score) %&gt;% \n    step_impute_knn(sum_clicks) %&gt;% \n    step_impute_knn(sd_clicks) %&gt;% \n    step_impute_knn(mean_clicks) %&gt;% \n    step_impute_knn(intercept) %&gt;% \n    step_impute_knn(slope) %&gt;% \n    step_impute_knn(date_registration) %&gt;% \n    step_normalize(all_numeric_predictors())\n\n\n\nSpecifying the Model and Workflow\nWe use a random forest model via the ranger engine and set up our workflow.\n\n# Specify random forest model\nmy_mod2 &lt;- rand_forest() %&gt;% \n    set_engine(\"ranger\", importance = \"impurity\") %&gt;% \n    set_mode(\"classification\")\n\n# Create workflow to bundle the recipe and model\nmy_wf2 &lt;- workflow() %&gt;% \n    add_recipe(my_rec2) %&gt;% \n    add_model(my_mod2)\n\n\n\nResampling and Model Fitting\nWe perform cross-validation (v-fold CV) to estimate model performance.\n\n# Create 4-fold cross-validation on training data\nvfcv &lt;- vfold_cv(data = students_assessments_and_interactions, v = 4, strata = pass)\n\n# Specify metrics: accuracy, sensitivity, specificity, ppv, npv, and Cohen's kappa\nclass_metrics &lt;- metric_set(accuracy, sensitivity, specificity, ppv, npv, kap)\n\n# Fit the model using resampling\nfitted_model_resamples &lt;- fit_resamples(my_wf2, resamples = vfcv, metrics = class_metrics)\n\n# Collect and display metrics\ncollect_metrics(fitted_model_resamples)\n\n# A tibble: 6 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.671     4 0.00169 Preprocessor1_Model1\n2 kap         binary     0.263     4 0.00331 Preprocessor1_Model1\n3 npv         binary     0.589     4 0.00371 Preprocessor1_Model1\n4 ppv         binary     0.702     4 0.00101 Preprocessor1_Model1\n5 sensitivity binary     0.815     4 0.00337 Preprocessor1_Model1\n6 specificity binary     0.435     4 0.00336 Preprocessor1_Model1\n\n\n\n\nFinal Model Fit and Evaluation\nFinally, we fit the model on the full training set (using last_fit) and evaluate its predictions on the test set.\n\n# Split data into training and testing sets (e.g., 33% for testing)\nset.seed(20230712)\ntrain_test_split &lt;- initial_split(students_assessments_and_interactions, prop = 0.67, strata = pass)\ndata_train &lt;- training(train_test_split)\ndata_test &lt;- testing(train_test_split)\n\n# Fit final model on the training set and evaluate on the test set\nfinal_fit &lt;- last_fit(my_wf2, train_test_split, metrics = class_metrics)\n\n# Collect and display final metrics\ncollect_metrics(final_fit)\n\n# A tibble: 6 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.665 Preprocessor1_Model1\n2 sensitivity binary         0.815 Preprocessor1_Model1\n3 specificity binary         0.419 Preprocessor1_Model1\n4 ppv         binary         0.697 Preprocessor1_Model1\n5 npv         binary         0.580 Preprocessor1_Model1\n6 kap         binary         0.247 Preprocessor1_Model1\n\n# Generate and display a confusion matrix for final predictions\ncollect_predictions(final_fit) %&gt;% \n    conf_mat(.pred_class, pass)\n\n          Truth\nPrediction    0    1\n         0 5439 1238\n         1 2370 1710\n\n# Extract the fitted model from the final workflow and plot variable importance\nfinal_fit %&gt;% \n  pluck(\".workflow\", 1) %&gt;%   # Extract the workflow object from the final fit\n  extract_fit_parsnip() %&gt;%   # Retrieve the fitted model from the workflow\n  vip(num_features = 10)      # Plot the top 10 important features\n\n\n\n\n\n\n\n# Extract the fitted model from the workflow\nfinal_model &lt;- final_fit %&gt;% \n  pluck(\".workflow\", 1) %&gt;% \n  extract_fit_parsnip()\n# Extract the variable importance values from the fitted model\nimportance_values &lt;- final_model$fit$variable.importance\n\n# Print the variable importance values\nprint(importance_values)\n\n  date_registration mean_weighted_score          sum_clicks           sd_clicks \n          652.14806           838.06052          1062.49141           778.63848 \n        mean_clicks           intercept               slope        disability_Y \n          737.79417           729.52344           739.85682            60.93367 \n           gender_M     code_module_BBB     code_module_CCC     code_module_DDD \n           79.62440            77.18477            70.72006            46.68243 \n    code_module_EEE     code_module_FFF     code_module_GGG \n           28.26595            74.31310            35.27026 \n\n\n\n\n\n5.4.4 Results and Discussions\n\nResearch Question 1 (RQ1):\nHow accurately can a random forest model predict whether a student will pass a course using interactions data from OULAD?\nResponse:\nUsing 4-fold cross-validation, our random forest model yielded an average accuracy of approximately 67.0% (mean accuracy from resamples: 0.670) with a Cohen’s Kappa of 0.261, suggesting moderate agreement beyond chance. When fitted on the entire training set and evaluated on the test set, the final model showed an accuracy of 66.5% along with: - Sensitivity: 81.5% – indicating the model correctly identifies a high proportion of students who pass. - Specificity: 41.9% – suggesting that the model is less effective at correctly identifying students who do not pass. - Positive Predictive Value (PPV): 69.7% - Negative Predictive Value (NPV): 58.0%\nThe confusion matrix shows: - True Negatives (TN): 5439 - False Negatives (FN): 1238 - False Positives (FP): 2370 - True Positives (TP): 1710\nOverall, these metrics indicate that while the model performs well in detecting positive outcomes (high sensitivity), its lower specificity means that it tends to misclassify a relatively higher proportion of non-passing students.\n\n\n\nResearch Question 2 (RQ2):\nWhich interaction-based features are most important in predicting student outcomes?\nResponse:\nThe variable importance analysis, extracted from the final random forest model using the vip() function, highlights the following key predictors (with their respective importance scores):\n\nsum_clicks: 1062.49 – This is the most influential feature, indicating that the total number of clicks (i.e., student engagement) in the VLE is a strong predictor of student success.\nmean_weighted_score: 838.06 – Reflecting academic performance as measured by weighted assessment scores.\nmean_clicks: 737.79, slope: 739.86, and intercept: 729.52 – These engineered features representing the central tendency and trend of click behavior further underline the importance of digital engagement patterns.\ndate_registration: 652.15 – The registration date also plays a significant role.\nOther categorical variables (e.g., dummy-coded disability, gender, and code_module levels) generally show lower importance scores, with values typically under 80, indicating that while they do contribute, engagement and performance metrics dominate.\n\nThese results suggest that both the intensity and the temporal trend of student interactions with the learning environment are critical in predicting whether a student will pass.\n\n\n\nResearch Question 3 (RQ3):\nHow does the use of cross-validation impact the stability and generalizability of the random forest model on interactions data?\nResponse:\nThe use of 4-fold cross-validation (via vfold_cv) allowed us to assess the model’s performance across multiple subsets of the data, mitigating the risk of overfitting. The resampling results are relatively consistent (with accuracy around 67%, sensitivity at 81.6%, and specificity around 43.2%), which supports the model’s robustness and generalizability. Although the final test set performance (accuracy of 66.5%) is slightly lower, the overall consistency of metrics across folds indicates that our model is stable when applied to unseen data.\n\n\n\nOverall Discussion\nThe random forest model built on interactions data from OULAD demonstrates decent predictive performance with an accuracy of approximately 66.5–67% and high sensitivity (around 81.5%), indicating strong capability in identifying students who will pass the course. However, the relatively low specificity (around 42%) suggests that there is room for improvement in correctly classifying students who are at risk of not passing.\nThe variable importance analysis underscores that engagement-related features—especially sum_clicks and features capturing the trend in interactions (slope, mean_clicks)—are the most influential predictors. This insight implies that the digital footprint of student engagement in the virtual learning environment is critical for predicting academic outcomes.\nIn summary, while our model performs robustly across cross-validation folds and provides actionable insights into key predictive features, the lower specificity points to the need for further refinement. Future work might explore additional feature engineering, alternative model tuning, or combining models to better balance sensitivity and specificity, ultimately supporting timely interventions in educational settings.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Section 5 **Secondary Analysis of Big Data (Numeric Data)**</span>"
    ]
  },
  {
    "objectID": "section-6.html",
    "href": "section-6.html",
    "title": "Section 6 Communication Collaboration Practices",
    "section": "",
    "text": "Abstract\nA key part of social science research (and, any analysis involving computational tools) is communicating about the output or findings. In this section, we describe how to communicate with colleagues or the wider world through the use of a variety of tools, especially R Markdown and git/GitHub. We also discuss how to collaborate on projects that involve computational methods and “good” (flexible yet principled) practices for doing so based on our experience and prior scholarship.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Section 6 **Communication Collaboration Practices**</span>"
    ]
  }
]