[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Analysis of Educational Data: A Field Guide Using R",
    "section": "",
    "text": "Preface\nWelcome to Computational Analysis of Educational Data: A Field Guide Using R!\n\nWhy this book?\nConventionally, as social scientists, we start the research process by generating research questions based on our previous knowledge and theories in the field. This is the conventional model of the order of operations in the process, and it is still the normative view of how it should work. However, it is also epistemologically possible that our observations of the world can guide our research questions. The process doesn‚Äôt always proceed in the orderly linear fashion suggested by the conventional model.\nWe are limited in how we see the world: we don‚Äôt know what we don‚Äôt know. For example, for someone who does not know that Twitter posts can be collected and analyzed as data to capture the state of the world, studying Twitter data will most likely not become a topic for a research question. Once you start seeing what can be data in the world, it starts shaping our ideas of what is ‚Äúresearchable‚Äù. Here is a simple model that we propose that shows the reciprocal nature of the research process, which is at the core of our aims in this book:\n\n\n\nFigure 1. Research - data relationship\n\n\nWe are writing this book because in the past few years what we described above has started happening for us. We have published work on Twitter data, Social Network analyses, natural language processing, and machine learning that was only possible after we learned what kind of data already existed around us. We thought other social scientists may benefit from a resource that not only tells them what is available as data but also guides them through concrete examples of going through this research journey along with us. We hope that along this journey you will develop your own research questions, and maybe even replicate some of the studies we imagined in this book.\nThe second most important aspect of this book that does not exist out in the wild is the ‚Äúfield guide‚Äù process where we work through the research design and reporting process. To do that, for each new computational research method, we follow this process:\n\nStart with what makes good data for that analysis (and how to capture it)\nSee what the data looks like (what it **has to **have, and what it can have)\nFormulate sample research questions based on the resources provided by the data and the type of analysis.\nGo through the analyses in R\nProvide a sample write up for Methods\nProvide a sample write up for Results\n\n\n\nWho is this book for?\nThis book will be beginner-friendly but not for the absolute beginner. We will dedicate the initial chapters to take you to resources that will help you get started. But, to make sense of this book, you should have basic research design knowledge, basic statistical knowledge, and a basic understanding of R and RStudio. At the same time, this book will not be for experts or expertise. There are already many great resources that delve into the topics that we cover (e.g., Silge‚Äôs book on using text data for machine learning).\nWe imagine that this book can become a part of doctoral coursework for future researchers, opening the doors for new ways to look at the world for research and data. Likewise, senior and junior academics/researchers would benefit greatly from this book to help them expand their research agendas.\nFor researchers like ourselves, we think this book can serve as a fun summer reading to rejuvenate and get excited about new research. At the same time, we also envision this book as a guidebook to keep on the side and frequently refer to, as researchers write up their work using these new methods.\nThis book will provide new ways to look at the world and formulate RQs. It will guide through the research process for each new method (including, friendly data organization tips, template for writing up and sharing this. We hope that you join us in this journey and this work will help open up new doors/embark on a journey of using computational research methods.\n\n\nOrganization of the books\nThe book is organized around four sections. Within these sections, there are specific chapters, which serve as field guide ‚Äúentries‚Äù or ‚Äúcases‚Äù. While the section overviews (the first chapter within each section) introduce the techniques or methodologies used in the rest of the section‚Äôs chapters, the chapters are intended to address a specific, narrow case, where we provide a sample write-up for researchers in writing their research questions, methods, results (and discussions) sections based on the analyses.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "chapter-1-1.html",
    "href": "chapter-1-1.html",
    "title": "R",
    "section": "",
    "text": "Three Ways to Write R Code\nWe‚Äôre not going to teach you RStudio comprehensively‚Äîexcellent resources already exist for that purpose. We particularly recommend R for Data Science, Data Science in Education Using R, the RStudio primers, and Happy Git and GitHub for the useR.\nInstead, this chapter covers just enough to reproduce our workflows and extend them to your own data science work. Think of this as the minimal viable setup for doing the work in this book.\nRStudio supports multiple file types for writing R code. You‚Äôll encounter all three in data science work:",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>R</span>"
    ]
  },
  {
    "objectID": "chapter-1-1.html#three-ways-to-write-r-code",
    "href": "chapter-1-1.html#three-ways-to-write-r-code",
    "title": "R",
    "section": "",
    "text": "R Scripts (.R)\nPlain text files containing R code, executed line-by-line or all at once. Great for data processing pipelines, functions, and code you‚Äôll reuse.\nCreate one: File ‚Üí New File ‚Üí R Script\n# This is an R script\n# Lines starting with # are comments\ndata &lt;- read.csv(\"mydata.csv\")\nsummary(data)\n\n\nR Markdown (.Rmd)\nDocuments that weave together narrative text (in Markdown) and code chunks. When you ‚Äúknit‚Äù an R Markdown file, it executes the code and generates a formatted document (HTML, PDF, or Word).\nYou should know R Markdown exists because you‚Äôll encounter it in older resources and projects, but we‚Äôre not using it in this book.\n\n\nQuarto (.qmd)\nQuarto is the successor to R Markdown, with better multi-language support (R, Python, Julia) and more consistent syntax. It‚Äôs what we‚Äôll use throughout this book.\nCreate one: File ‚Üí New File ‚Üí Quarto Document\nThe structure looks similar to R Markdown, but the rendering engine is more powerful:\n---\ntitle: \"My Analysis\"\nformat: html\n---\n\n## Introduction\n\nThis is regular text.\n```{r}\n# This is a code chunk\nx &lt;- 1:10\nmean(x)\n```\nClick the ‚ÄúRender‚Äù button (or Ctrl/Cmd + Shift + K) to execute all code and generate your document.\nKey difference for beginners: Think of R scripts as ‚Äújust code‚Äù and Quarto as ‚Äúcode + explanation + output‚Äù in one document. Use scripts for behind-the-scenes work, Quarto for analysis you want to share or understand later.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>R</span>"
    ]
  },
  {
    "objectID": "chapter-1-1.html#console-vs.-source-exploration-vs.-preservation",
    "href": "chapter-1-1.html#console-vs.-source-exploration-vs.-preservation",
    "title": "R",
    "section": "Console vs.¬†Source: Exploration vs.¬†Preservation",
    "text": "Console vs.¬†Source: Exploration vs.¬†Preservation\nA common pattern in data science work:\n\nConsole: Quick exploration, testing ideas, checking output. Code here disappears when you close RStudio.\nSource (scripts/Quarto): Code you want to keep, reproduce, or share. This is your permanent record.\n\nEarly on, you might type everything in the Console. That‚Äôs fine for learning! But as soon as you do something you want to remember, put it in a script or Quarto file.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>R</span>"
    ]
  },
  {
    "objectID": "chapter-1-1.html#when-you-get-stuck",
    "href": "chapter-1-1.html#when-you-get-stuck",
    "title": "R",
    "section": "When You Get Stuck",
    "text": "When You Get Stuck\n\nReading Error Messages\nWhen something goes wrong, R prints an error message in the Console (usually in red). These messages often look cryptic at first, but they‚Äôre trying to help:\nError in mean(x) : object 'x' not found\nThis tells you exactly what went wrong: R couldn‚Äôt find an object called x. Either you misspelled it, or you haven‚Äôt created it yet.\n\n\nGetting Help on Functions\nR has built-in documentation for every function:\n?mean           # Opens help for the mean() function\n??regression    # Searches all help files for \"regression\"\nHelp files include descriptions, arguments, examples, and related functions.\n\n\nWhere Computational Social Scientists Get Help\nWhen you‚Äôre truly stuck, the R community is remarkably helpful:\n\nRStudio Community: Welcoming forum for all R questions\nStack Overflow: Searchable archive of millions of questions\nR for Data Science Online Learning Community: Active Slack workspace\n\nPro tip: Before asking a question, try searching for your error message. Someone has likely encountered it before.\nOf course, turning to AI can also help, as we describe next.\n\n\nLarge Language Models and AI Tools\nIn Section 3, we cover using large language models as part of a research workflow. But LLMs are also useful for getting unstuck when you‚Äôre learning‚Äîand this is increasingly how working data scientists operate.\nTwo approaches work well:\n\nAsk LLMs directly: ChatGPT, Claude, or your preferred model can explain error messages, suggest debugging approaches, and clarify confusing documentation. Copy your error message, describe what you were trying to do, and ask for help.\nUse GitHub Copilot: An AI coding assistant that works inside your editor (setup covered in the next chapter). It can suggest code completions, explain what existing code does, and generate boilerplate. Think of it as an always-available pair programmer.\n\nThe best programmers don‚Äôt memorize every function‚Äîthey know how to find answers quickly. LLMs have become part of that toolkit, alongside documentation, Stack Overflow, and experimentation. Use them when you‚Äôre stuck, but also pay attention to why solutions work. That‚Äôs how you build intuition.\nPlease see section 1.4 for more information about the relationship between you, data, and LLMs.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>R</span>"
    ]
  },
  {
    "objectID": "chapter-1-2.html",
    "href": "chapter-1-2.html",
    "title": "Positron and RStudio",
    "section": "",
    "text": "Why Positron?\nYou need an environment for writing and running R code. We‚Äôre going to focus on Positron, Posit‚Äôs newer IDE built on the foundation of VS Code. If you‚Äôre already comfortable with RStudio, everything in this chapter applies there too‚Äîthe fundamental concepts are identical, and the R code in this book works the same way in both.\nPositron represents where computational social science workflows are heading: multi-language support (R and Python in the same environment), modern editor features inherited from VS Code, and a more extensible architecture. If you‚Äôre coming from VS Code, Positron will feel immediately familiar. If you‚Äôre new to both, you‚Äôre learning a tool that‚Äôs designed for the future of data science.\nThat said, RStudio remains excellent. It‚Äôs mature, widely documented, and has a loyal community. Some researchers prefer its more opinionated interface‚Äîeverything you need is visible and organized for R-first workflows. If you‚Äôre already using RStudio and prefer it, nothing in this book requires switching.\nThe instructions below work in both environments unless otherwise noted. When we say ‚ÄúPositron,‚Äù assume the same applies to RStudio.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Positron and RStudio</span>"
    ]
  },
  {
    "objectID": "chapter-1-2.html#the-four-essential-panes",
    "href": "chapter-1-2.html#the-four-essential-panes",
    "title": "Positron and RStudio",
    "section": "The Four Essential Panes",
    "text": "The Four Essential Panes\nBoth Positron and RStudio organize your workspace into four main areas (though the layout is customizable):\n\nSource/Editor (top-left): Where you write and edit your code files (.R scripts, .qmd documents)\nConsole (bottom-left): Where R actually executes code and shows results in real-time\nEnvironment/Variables (top-right): Shows the objects (data, functions, etc.) currently loaded in memory\nFiles/Plots/Help (bottom-right): Navigate your project files, view visualizations, and access documentation\n\nThe defaults work fine for now. As you get comfortable, you might adjust panel sizes or move things around, but the essential logic stays the same: code on the left, context on the right.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Positron and RStudio</span>"
    ]
  },
  {
    "objectID": "chapter-1-2.html#r-projects-the-foundation-of-reproducible-work",
    "href": "chapter-1-2.html#r-projects-the-foundation-of-reproducible-work",
    "title": "Positron and RStudio",
    "section": "R Projects: The Foundation of Reproducible Work",
    "text": "R Projects: The Foundation of Reproducible Work\nAlways work in an R Project. This is the single most important RStudio habit for reproducibility.\nAn R Project is a folder that contains all files for a particular analysis‚Äîdata, scripts, outputs‚Äîand sets your working directory automatically. This means your code will work on any computer without changing file paths.\nTo create a new project:\n\nFile ‚Üí New Project\nChoose ‚ÄúNew Directory‚Äù or ‚ÄúExisting Directory‚Äù\nName your project (e.g., ‚Äúmy-first-css-analysis‚Äù)\nClick ‚ÄúCreate Project‚Äù\n\nNotice the .Rproj file in your folder? That‚Äôs your project file. Double-click it to open Positron or RStudio with the correct working directory already set.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Positron and RStudio</span>"
    ]
  },
  {
    "objectID": "chapter-1-2.html#version-control-with-git-and-github",
    "href": "chapter-1-2.html#version-control-with-git-and-github",
    "title": "Positron and RStudio",
    "section": "Version Control with Git and GitHub",
    "text": "Version Control with Git and GitHub\nGit is how computational social scientists version, share, and collaborate on their work. GitHub is the most popular platform for hosting Git repositories. If you‚Äôve never used version control before, this section will get you set up with a workflow that integrates directly into your IDE.\n\nWhy Git Matters\nWithout version control, you end up with files like analysis_final.R, analysis_final2.R, analysis_ACTUALLY_final.R. Git tracks every change you make, so you can:\n\nGo back to any previous version without keeping multiple copies\nExperiment freely‚Äîif something breaks, you can always revert\nCollaborate with others without emailing files back and forth\nShare your work publicly for transparency and reproducibility\n\nMost computational social science projects live in Git repositories, and employers expect researchers to be comfortable with basic version control.\n\n\nThe Git Integration in Your IDE\nOnce Git is configured, you‚Äôll see version control features built into your environment:\n\nIn RStudio: A ‚ÄúGit‚Äù tab appears in the Environment pane (top-right)\nIn Positron: Git integration works through the Source Control panel (often on the left sidebar)\n\nEither way, you can stage changes, write commit messages, and push to GitHub without leaving your editor. This is far friendlier than memorizing terminal commands.\n\n\nSetting Up Git and GitHub: The Path of Least Resistance\nGit authentication is notoriously confusing. We‚Äôre going to use an R-native approach that minimizes pain: Personal Access Tokens (PAT) via the usethis package. This keeps you in familiar territory and works reliably for the kind of work you‚Äôll do in this book.\n\nStep 1: Install Required Packages\ninstall.packages(\"usethis\")\ninstall.packages(\"gitcreds\")\n\n\nStep 2: Configure Your Git Identity\nTell Git who you are (one-time setup):\nusethis::use_git_config(\n  user.name = \"Jane Doe\",           # Your actual name\n  user.email = \"jane@example.com\"   # Email associated with GitHub\n)\nImportant: Use the same email address you‚Äôll use (or already use) for your GitHub account.\n\n\nStep 3: Create a GitHub Personal Access Token\nusethis::create_github_token()\nThis opens GitHub in your browser. You‚Äôll need to:\n\nLog in to GitHub if you haven‚Äôt already\nLeave the default scopes checked (at minimum: ‚Äúrepo‚Äù and ‚Äúworkflow‚Äù)\nSet expiration to 90 days (good security practice, though you‚Äôll regenerate when it expires)\nClick ‚ÄúGenerate token‚Äù at the bottom\nCopy the token immediately‚ÄîGitHub only shows it once\n\nThe token looks like: ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n\nStep 4: Store Your Token in R\ngitcreds::gitcreds_set()\nWhen prompted, paste your token and press Enter. R stores it securely for future use.\n\n\nStep 5: Verify Everything Worked\nusethis::git_sitrep()\nThis diagnostic report should show: - Your name and email - ‚ÄúPersonal access token for ‚Äòhttps://github.com‚Äô: ‚Äò&lt;discovered&gt;‚Äô‚Äù\nIf so, you‚Äôre all set. If not, see troubleshooting below.\n\n\n\nCommon Problems & Solutions\nProblem: ‚ÄúI closed the browser before copying my token‚Äù Solution: Tokens are disposable by design. Run usethis::create_github_token() again to generate a new one.\nProblem: ‚ÄúIt worked yesterday, now Git asks for credentials again‚Äù Solution: Your token expired. Run usethis::create_github_token() again, copy the new token, then run gitcreds::gitcreds_set() to update it.\nProblem: ‚ÄúI get ‚ÄòPermission denied‚Äô or authentication errors‚Äù Solution: Check usethis::git_sitrep(). Your token may have expired or wasn‚Äôt stored correctly. Run gitcreds::gitcreds_set() again.\nProblem: ‚ÄúI don‚Äôt see Git integration in my IDE‚Äù Solution: Git may not be installed, or your IDE can‚Äôt find it. See Happy Git with R, Chapter 6 for installation instructions.\n\n\nAlternative: SSH Keys (For the Long Haul)\nIf you plan to use Git extensively beyond this book, SSH keys are more convenient‚Äîset them up once and never think about authentication again. But they‚Äôre more complex to configure initially.\nInterested? See Happy Git with R, Chapters 10-12. For now, the PAT method will serve you well.\n\n\nUsing Git in Practice\nWe‚Äôll introduce Git commands as you need them throughout the book. For now, know that your IDE gives you a graphical interface for the most common operations:\n\nPull: Download changes from GitHub (always do this before starting work)\nCommit: Save a snapshot of your changes with a descriptive message\nPush: Upload your commits to GitHub\n\nYou can also use terminal commands (git status, git add, git commit, git push) if you prefer, but the IDE interface handles 90% of everyday tasks.\nThe key habit: commit often with clear messages (‚ÄúAdd data cleaning script‚Äù rather than ‚Äúchanges‚Äù), and push regularly so your work is backed up and others can see your progress.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Positron and RStudio</span>"
    ]
  },
  {
    "objectID": "chapter-1-2.html#going-further",
    "href": "chapter-1-2.html#going-further",
    "title": "Positron and RStudio",
    "section": "Going Further",
    "text": "Going Further\nThis chapter gave you a working environment. When you‚Äôre ready to explore more:\n\nPositron Documentation: Official docs for Positron features\nRStudio User Guide: Comprehensive guide to all RStudio capabilities\nHappy Git and GitHub for the useR: Everything Git-related for R users\nGitHub Skills: Interactive tutorials for Git and GitHub\n\nAdvanced features worth exploring later: - Keyboard shortcuts (speed up your workflow dramatically) - Code snippets (type abbreviations that expand to code templates) - Debugger (step through code line-by-line when things break) - Extensions (especially in Positron, where VS Code extensions work)",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Positron and RStudio</span>"
    ]
  },
  {
    "objectID": "chapter-1-3.html",
    "href": "chapter-1-3.html",
    "title": "Tidyverse",
    "section": "",
    "text": "What‚Äôs in the Tidyverse?\nThe tidyverse is a collection of R packages designed for data science with a shared philosophy: code should be readable, consistent, and focused on the data transformations you‚Äôre actually trying to accomplish. If you‚Äôve struggled with base R‚Äôs sometimes cryptic syntax, the tidyverse will feel like a relief.\nMost computational social science workflows rely on tidyverse packages for data manipulation, visualization, and modeling. Learning this ecosystem means learning the tools your collaborators use, the examples you‚Äôll find online, and the approaches that scale from quick exploration to publication-ready analysis.\nThe tidyverse is actually a meta-package that loads eight core packages when you run library(tidyverse):\nYou don‚Äôt need to master all of these immediately. Most data work relies heavily on dplyr and ggplot2, with the others playing supporting roles as needed.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "chapter-1-3.html#whats-in-the-tidyverse",
    "href": "chapter-1-3.html#whats-in-the-tidyverse",
    "title": "Tidyverse",
    "section": "",
    "text": "ggplot2: Data visualization with a grammar of graphics\ndplyr: Data manipulation (filtering, selecting, summarizing, joining)\ntidyr: Reshaping data between wide and long formats\nreadr: Reading rectangular data (CSV, TSV) efficiently\npurrr: Functional programming tools for iteration\ntibble: Modern reimagining of data frames\nstringr: String manipulation\nforcats: Working with categorical variables (factors)",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "chapter-1-3.html#installing-and-loading-the-tidyverse",
    "href": "chapter-1-3.html#installing-and-loading-the-tidyverse",
    "title": "Tidyverse",
    "section": "Installing and Loading the Tidyverse",
    "text": "Installing and Loading the Tidyverse\nInstallation (one time):\ninstall.packages(\"tidyverse\")\nLoading (at the start of each session or script):\nlibrary(tidyverse)\nWhen you load the tidyverse, you‚Äôll see a message listing which packages were attached and any conflicts (functions from tidyverse packages that mask base R functions). The conflicts are normal and intentional‚Äîtidyverse functions are generally preferable for data science work.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "chapter-1-3.html#the-pipe-operator-and",
    "href": "chapter-1-3.html#the-pipe-operator-and",
    "title": "Tidyverse",
    "section": "The Pipe Operator: |> and %>%",
    "text": "The Pipe Operator: |&gt; and %&gt;%\nThe tidyverse introduced a distinctive style: piping operations together to create readable data transformation pipelines. Instead of nesting functions inside each other, you ‚Äúpipe‚Äù the output of one function into the next.\nR now has a native pipe operator |&gt; (introduced in R 4.1), while the tidyverse originally used %&gt;% from the magrittr package. They work almost identically for most purposes:\n# Without pipes (nested, hard to read)\nsummary(filter(select(data, age, income), age &gt; 18))\n\n# With native pipe |&gt; (readable, left-to-right)\ndata |&gt;\n  select(age, income) |&gt;\n  filter(age &gt; 18) |&gt;\n  summary()\n\n# With magrittr pipe %&gt;% (functionally similar)\ndata %&gt;%\n  select(age, income) %&gt;%\n  filter(age &gt; 18) %&gt;%\n  summary()\nWe‚Äôll use the native pipe |&gt; throughout this book because it‚Äôs now built into R, but you‚Äôll encounter %&gt;% in older code and documentation. For practical purposes, they‚Äôre interchangeable in most tidyverse workflows.\nHow to read it: Think of |&gt; as ‚Äúthen.‚Äù The code above reads: ‚ÄúTake data, then select age and income columns, then filter for adults, then get a summary.‚Äù",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "chapter-1-3.html#next-steps",
    "href": "chapter-1-3.html#next-steps",
    "title": "Tidyverse",
    "section": "Next Steps",
    "text": "Next Steps\nThis chapter provides a foundation for tidyverse thinking. The subsequent chapters in this book use tidyverse functions extensively, and we‚Äôll explain new functions as they appear. You‚Äôll learn by doing, with real data and real research questions.\nWhen you want to go deeper:\n\nR for Data Science (2e): The definitive guide to the tidyverse\nTidyverse documentation: Reference docs for all packages\nRStudio Cheatsheets: Visual quick references for dplyr, ggplot2, and more\n\nThe tidyverse community is large and welcoming. When you get stuck, someone has likely asked your question before.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "chapter-1-4.html",
    "href": "chapter-1-4.html",
    "title": "LLMs and Data Science",
    "section": "",
    "text": "(Responsible) use of LLMs in Data Science\nAs we integrate large language models into data analysis, our responsibilities as researchers evolve in important ways . There are three primary approaches to this integration:\n\nfirst, using LLMs for code autocompletion and inline coding assistance, such as through integrated development environments like Positron;\nsecond, using LLMs to support the analysis process through complete code generation and supervised execution, such as with tools like Posit Databot; and\nthird, deploying LLMs to directly analyze qualitative (e.g., text data).\n\n\n\nThe Responsible Use Framework\nThe responsible use framework for LLMs in research, presented by Joe Cheng at R+AI Conference 2025 (Cheng, 2025), provides a practical evaluation tool based on three essential criteria. Correctness refers to whether the LLM produces accurate and reliable results that can be verified and trusted. Transparency addresses whether the process and reasoning behind the AI‚Äôs outputs are visible and understandable to researchers, allowing them to inspect how conclusions were reached. Reproducibility concerns whether the same analysis can be repeated and yield consistent results, a foundational requirement of scientific research. These three categories align with broader responsible AI governance principles commonly found in organizational frameworks but are specifically tailored to the research context where verifiability and scientific rigor are key. For an LLM application in research to be considered responsibly used, it should ideally achieve ‚Äúyes‚Äù answers across all three criteria, ensuring that the technology enhances rather than compromises research integrity.\n\n\nApplying the Resposible Use Framework\nWhen we examine the first two usage types (e.g., code autocomplete and code generation) through the responsible usage framework, we find more encouraging alignment with the three essential criteria of correctness, transparency, and reproducibility . Code-generating and code-assisting LLMs produce verifiable output that researchers can inspect, test, and validate before execution, ensuring correctness through human review. The process maintains transparency because the generated code itself is visible and interpretable, allowing researchers to understand exactly what analytical steps are being performed. Reproducibility is enhanced because the same code can be run multiple times on the same data to yield consistent results, and the code can be shared alongside research findings.\nIn contrast, the third approach where LLMs directly analyze qualitative or text data within a black box that is the LLM may raise critical questions about research integrity. Using LLMs for this purpose present inherent challenges against these principles: they are notorious for generating convincing but incorrect answers, operate as black boxes with limited transparency, and lack reproducibility due to their non-deterministic nature. When we apply this framework to direct text analysis by LLMs, significant concerns may emerge: we cannot guarantee correctness, the process lacks transparency, and reproducibility remains uncertain at best.\n\n\nAchieving Responsible Usage Through Evidence-Based Results\nHowever, for the third type of usage where LLMs directly analyze data (as we do in Section X), we can work toward achieving ‚Äúyes‚Äù answers across all (or as many as possible) three framework criteria by requiring LLMs to produce evidence-based results. As we cover in Section 6, it is possible to use a Local LLM that is connected to R/Positron. In this approach, the researcher has control over the data analysis over simply pasting text into an LLMs chatbox. For example, this can be done by structuring prompts to demand transparent, verifiable outputs, such as requiring the LLM to identify themes, provide verbatim quotes from the source data, report frequencies with counts and percentages, and present findings in standardized tabular formats. We transform the black box into a more transparent analytical tool. This approach ensures correctness by grounding every claim in specific textual evidence that researchers can verify, enhances transparency by making the analytical reasoning traceable through quoted examples and quantitative metrics, and improves reproducibility by standardizing the output format and maintaining clear documentation of the analytical process. When LLMs are constrained to cite their sources, quantify their observations, and structure their findings systematically, they shift from opaque pattern generators to accountable research assistants whose work can be validated against the original data. This methodology aligns with the recommendation for constrained use and micromanaged implementation, ensuring that LLMs remain within appropriate boundaries while still providing valuable analytical support for qualitative research.\nReference\nCheng, J. (2025). Harnessing the power of LLMs for responsible data science and research [Conference presentation]. R+AI 2025. Available at https://r-consortium.org/posts/keeping-llms-in-their-lane-focused-ai-for-data-science-and-research/",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>LLMs and Data Science</span>"
    ]
  },
  {
    "objectID": "section-2-Intro.html",
    "href": "section-2-Intro.html",
    "title": "Computational Methods",
    "section": "",
    "text": "Overview\nThis section introduces the core computational methods that form the basis of educational data analysis. Across the next three chapters, you will learn how to analyze textual, relational, and numeric data‚Äîthree major forms of information that appear in learning environments, institutional records, and educational research projects.\nThese chapters emphasize hands-on, transparent, and reproducible approaches using R. By engaging with real examples, you will gain practical experience in transforming raw educational data into interpretable results that inform theory and practice.",
    "crumbs": [
      "Computational Methods"
    ]
  },
  {
    "objectID": "section-2-Intro.html#the-analytical-scope-of-this-section",
    "href": "section-2-Intro.html#the-analytical-scope-of-this-section",
    "title": "Computational Methods",
    "section": "The Analytical Scope of This Section",
    "text": "The Analytical Scope of This Section\n\n\n\n\n\n\n\n\n\nChapter\nData Type\nAnalytical Focus\nExample Research Question\n\n\n\n\nChapter 2\nText Data (unstructured)\nNatural language processing, tokenization, sentiment, topic modeling\n‚ÄúWhat themes emerge in student reflections or policy statements?‚Äù\n\n\nChapter 3\nRelational Data\nSocial network analysis: centrality, community detection, visualization\n‚ÄúHow do students or instructors connect and collaborate in learning networks?‚Äù\n\n\nChapter 4\nNumeric / Big Data\nStatistical modeling, regression, clustering, predictive analysis\n‚ÄúWhich factors best predict academic outcomes or engagement?‚Äù\n\n\n\n\nüìö These three perspectives together illustrate how computational techniques can capture different dimensions of learning‚Äîlanguage, interaction, and measurement.",
    "crumbs": [
      "Computational Methods"
    ]
  },
  {
    "objectID": "section-2-Intro.html#setting-up-the-computational-environment",
    "href": "section-2-Intro.html#setting-up-the-computational-environment",
    "title": "Computational Methods",
    "section": "Setting Up the Computational Environment",
    "text": "Setting Up the Computational Environment\nBefore exploring the examples, ensure that your R environment contains the essential packages used throughout this section.\ninstall.packages(c(\n  # Core workflow and visualization\n  \"tidyverse\", \"ggplot2\", \"readr\", \"stringr\",\n\n  # Text analysis\n  \"tidytext\", \"quanteda\", \"textdata\",\n\n  # Network analysis\n  \"igraph\", \"ggraph\", \"tidygraph\",\n\n  # Numeric and machine learning tools\n  \"caret\", \"cluster\"\n))\nOptional visualization and interaction packages:\ninstall.packages(c(\"plotly\", \"RColorBrewer\", \"visNetwork\"))\n\nüí° Tip: Use a consistent project structure so each chapter builds on the same foundation:\nproject/\n‚îú‚îÄ‚îÄ data/        # datasets\n‚îú‚îÄ‚îÄ scripts/     # reusable code\n‚îú‚îÄ‚îÄ outputs/     # tables and processed files\n‚îî‚îÄ‚îÄ figures/     # charts and visualizations\nThis structure promotes reproducibility and helps keep analysis pipelines organized.",
    "crumbs": [
      "Computational Methods"
    ]
  },
  {
    "objectID": "section-2-Intro.html#a-general-computational-workflow",
    "href": "section-2-Intro.html#a-general-computational-workflow",
    "title": "Computational Methods",
    "section": "A General Computational Workflow",
    "text": "A General Computational Workflow\nRegardless of data type, the analytical logic follows a similar cycle:\n\nLoad data ‚Äî read files from local or online sources.\nClean data ‚Äî handle missing values, normalize text or numeric fields.\nTransform data ‚Äî create tokens, build networks, or scale variables.\nAnalyze ‚Äî apply the method appropriate to the data form.\nVisualize and interpret ‚Äî generate plots and summaries to support interpretation.\n\nlibrary(tidyverse)\n\ndata &lt;- read_csv(\"data/example.csv\")\n\ncleaned &lt;- data |&gt;\n  mutate(across(everything(), str_squish)) |&gt;\n  drop_na()\n\nsummary(cleaned)\n\nüß© This five-step workflow‚Äîload, clean, transform, analyze, interpret‚Äîappears throughout all chapters in this section.",
    "crumbs": [
      "Computational Methods"
    ]
  },
  {
    "objectID": "section-2-Intro.html#ethics-transparency-and-reproducibility",
    "href": "section-2-Intro.html#ethics-transparency-and-reproducibility",
    "title": "Computational Methods",
    "section": "Ethics, Transparency, and Reproducibility",
    "text": "Ethics, Transparency, and Reproducibility\nEducational data often include sensitive or identifiable information.\nResponsible computational research requires attention to both ethical and methodological rigor.\n\nPrivacy: Remove or anonymize all personal identifiers.\nTransparency: Keep analysis scripts in Quarto or R Markdown files for version control.\nReproducibility: Record package versions and parameters used in each analysis.\nInterpretability: Combine quantitative patterns with contextual educational insight.\n\n\n‚öñÔ∏è Ethical and transparent practices ensure that computational results remain credible, interpretable, and usable for improving learning.",
    "crumbs": [
      "Computational Methods"
    ]
  },
  {
    "objectID": "section-2-Intro.html#transition-to-analytical-chapters",
    "href": "section-2-Intro.html#transition-to-analytical-chapters",
    "title": "Computational Methods",
    "section": "Transition to Analytical Chapters",
    "text": "Transition to Analytical Chapters\nWith the environment prepared and workflow established, you are ready to begin applying computational methods to real educational data:\n\nChapter 2 ‚Äî Text Analysis: Working with unstructured language to uncover patterns in meaning and discourse.\nChapter 3 ‚Äî Network Analysis: Examining connections and relationships among learners, instructors, or resources.\nChapter 4 ‚Äî Numeric and Big Data: Exploring structured data to identify trends and predictors in education.\n\n\nThe following chapter begins with text data, illustrating how natural language processing can transform qualitative information into structured, interpretable results.",
    "crumbs": [
      "Computational Methods"
    ]
  },
  {
    "objectID": "chapter-2.html",
    "href": "chapter-2.html",
    "title": "Text Data",
    "section": "",
    "text": "2.1 Overview\nIn social sciences, analyzing text data is usually considered the ‚Äújob‚Äù of qualitative researchers.Traditionally, qualitative research involves identifying patterns in non-numeric data, and this pattern recognition is typically done manually. This process is time-intensive but can yield rich research results. Traditional methods for analyzing text data involve human coding and can include direct sources (e.g., books, online texts) or indirect sources (e.g., interview transcripts).\nWith the advent of new software, we can capture and analyze text data in ways that were previously not possible. Modern data collection techniques include web scraping, accessing social media APIs, or downloading large online documents. Given the increased size of text data, analysis now requires computational approaches (e.g., dictionary-based, frequency-based, machine learning) that go beyond manual coding. These computational methods allow social scientists to ask new types of research questions, expanding the scope and depth of possible insights.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Text Data</span>"
    ]
  },
  {
    "objectID": "chapter-2.html#overview",
    "href": "chapter-2.html#overview",
    "title": "Text Data",
    "section": "",
    "text": "Disclaimer: While resources are available that discuss these analysis methods in depth, this book aims to provide a practical guide for social scientists, using data they will likely encounter. Our goal is to present a ‚Äúcookbook‚Äù for guiding research projects through real-world examples.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Text Data</span>"
    ]
  },
  {
    "objectID": "chapter-2.html#accessing-text-data",
    "href": "chapter-2.html#accessing-text-data",
    "title": "Text Data",
    "section": "2.2 Accessing Text Data",
    "text": "2.2 Accessing Text Data\nFor conventional qualitative researcher, text data comes from the usual sources such as interview transcripts or existing documents. Nowadays, however, text can be found and collected in many different ways. For example, social media is rich with text data, likewise for faculty who are teaching course (especially online), every student writing can be seen as a piece of text data. In this section, we will cover a few basic ways of accessing text data. Please note that although we cover some prominent ways, this is by no means an exhaustive list. Therefore, please refer to the additional resources section at the end of the section to dive deeper.\n\n2.2.1 Web Scraping (Unstructured or API)\n\nWhat is Web Scraping?\nWeb scraping refers to the automated process of extracting data from web pages. It is particularly useful when dealing with extensive lists of websites that would be tedious to mine manually. A typical web scraping program follows these steps:\n\nLoads a webpage.\nDownloads the HTML or XML structure.\nIdentifies the desired data.\nConverts the data into a format suitable for analysis, such as a data frame.\n\nIn addition to text, web scraping can also be used to download other content types, such as audio-visual files.\n\n\nIs Web Scraping Legal?\nWeb scraping was common in the early days of the internet, but with the increasing value of data, legal norms have evolved. To avoid legal issues, check the ‚ÄúTerms of Service‚Äù for specific permissions on the website, often accessible via ‚Äúrobots.txt‚Äù files. Consult legal advice when in doubt.\n\n\nReading a Web Page into R\nOnce permissions are confirmed, the first step in web scraping is to download the webpage‚Äôs source code into R, typically using the rvest package by Hadley Wickham.\n\n# Install and load the rvest package\ninstall.packages(\"rvest\")\nlibrary(rvest)\n\nTo demonstrate, we will scrape a simple Wikipedia page. Static pages, which lack interactive elements like JavaScript, are simpler to scrape. You can view the page‚Äôs HTML source in your browser by selecting Developer Tools &gt; View Source.\n\n# Load the webpage\nwikipedia_page &lt;- read_html(\"https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000\")\n\n# Verify that the webpage loaded successfully\nwikipedia_page\n\n\n\nParsing HTML\nThe next challenge is extracting specific information from the HTML structure. HTML files have a ‚Äútree-like‚Äù format, allowing us to target particular sections. Use your browser‚Äôs ‚ÄúDeveloper Tools‚Äù to inspect elements and locate the data. Right-click the desired element and select Inspect to view its structure.\nTo isolate data sections within the HTML structure, identify the XPath or CSS selectors. For instance:\n\n# Extract specific section using XPath\nsection_of_wikipedia &lt;- html_node(wikipedia_page, xpath='//*[@id=\"mw-content-text\"]/div/table')\nhead(section_of_wikipedia)\n\nTo convert the extracted section into a data frame, use html_table():\n\n# Convert the extracted data into a table\nhealth_rankings &lt;- html_table(section_of_wikipedia)\nhead(health_rankings[ , (1:2)])  # Display the first two columns\n\n\n\nParsing with CSS Selectors\nFor more complex web pages, CSS selectors can be an alternative to XPath. Tools like Selector Gadget can help identify the required CSS selectors.\nFor example, to scrape event information from Duke University‚Äôs main page:\n\n# Load the webpage\nduke_page &lt;- read_html(\"https://www.duke.edu\")\n\n# Extract event information using CSS selector\nduke_events &lt;- html_nodes(duke_page, css=\"li:nth-child(1) .epsilon\")\nhtml_text(duke_events)\n\n\n\nScraping with Selenium\nFor tasks involving interactive actions (e.g., filling search fields), use RSelenium, which enables automated browser operations.\nTo set up Selenium, install the Java SE Development Kit and Docker. Then, start Selenium in R:\n\n# Install and load RSelenium\ninstall.packages(\"RSelenium\")\nlibrary(RSelenium)\n\n# Start a Selenium session\nrD &lt;- rsDriver()\nremDr &lt;- rD$client\nremDr$navigate(\"https://www.duke.edu\")\n\nTo automate data entry, identify the CSS selector for the search box and input the query:\n\n# Find the search box element and enter a query\nsearch_box &lt;- remDr$findElement(using = 'css selector', 'fieldset input')\nsearch_box$sendKeysToElement(list(\"data science\", \"\\uE007\"))  # \"\\uE007\" represents Enter key\n\n\n\nWeb Scraping within a Loop\nTo scrape multiple pages, embed the code within a loop to automate tasks across different URLs. Since each site may have a unique structure, generalized scraping can be time-intensive and error-prone. Implement error handling to manage interruptions.\n\n\nWhen to Use Web Scraping\nWeb scraping is appropriate if:\n\nPage structure is consistent across sites: For example, a government site with date suffixes but a uniform layout.\nManual data collection is prohibitive: For extensive text or embedded tables.\n\nWhen feasible, consider alternatives like APIs or data-entry services (e.g., Amazon Mechanical Turk) for better efficiency and legal compliance.\n\n\nWhat is an API?\nAn Application Programming Interface (API) is a set of protocols that allows computers to communicate and exchange information. A common type is the REST API, where one machine sends a request, and another returns a response. APIs provide standardized access to data, services, and functionalities, making them essential in software development.\n\n\nWhen to Use an API\nAPIs are commonly used for:\n\nIntegrating with Third-Party Services: APIs connect applications to services like payment gateways or social media.\nAccessing Data: APIs retrieve data from systems or databases (e.g., real-time weather data).\nAutomating Tasks: APIs automate processes within applications, such as email marketing.\nBuilding New Applications: APIs allow developers to build new apps or services (e.g., a mapping API for navigation).\nStreamlining Workflows: APIs enable seamless communication and data exchange across systems.\n\n\n\nUsing Reddit API with RedditExtractoR\nReddit is a social media platform featuring a complex network of users and discussions, organized into ‚Äúsubreddits‚Äù by topic. RedditExtractoR, an R package, enables data extraction from Reddit to identify trends and analyze interactions.\n\n# Install and load RedditExtractoR\ninstall.packages(\"RedditExtractoR\")\nlibrary(RedditExtractoR)\n\n# Access data from the GenAI subreddit\nGenAI_reddit &lt;- find_thread_urls(subreddit = \"GenAI\", sort_by = \"new\", period = \"day\")\nview(GenAI_reddit)\n\n\n\n\n2.2.2 Audio Transcripts (Zoom, etc.)\nAudio transcripts are a rich source of text data, especially useful for capturing spoken content from meetings, interviews, or webinars. Many platforms, such as Zoom, provide automated transcription services that can be downloaded as text files for analysis. By processing these transcripts, researchers can analyze conversation themes, sentiment, or other linguistic features. Here‚Äôs how to access and prepare Zoom transcripts for analysis in R.\n\nKey Steps for Extracting Text Data from Audio Transcripts\n\nAccess the Zoom Transcript\n\nLog in to your Zoom account.\nNavigate to the ‚ÄúRecordings‚Äù section.\nSelect the recording you wish to analyze and download the ‚ÄúAudio Transcript‚Äù file.\n\nImport the Transcript into R\nOnce the file is downloaded, you can load it into R for analysis. Depending on the file format (usually a .txt file with tab or comma delimiters), use read.table(), read.csv(), or functions from the readr package to load the data.\n\n\n   # Load the transcript into R\n   transcript_data &lt;- read.table(\"path/to/your/zoom_transcript.txt\", sep = \"\\t\", header = TRUE)\n\nAdjust the sep parameter based on the delimiter used in the transcript file (typically \\t for tab-delimited files).\n\nData Cleaning (if necessary)\nClean up the text data to remove unnecessary characters, standardize formatting, and prepare it for further analysis.\n\nRemove Unwanted Characters\nUse gsub() to eliminate special characters and punctuation, keeping only alphanumeric characters and spaces.\n\n\n     # Remove special characters\n     transcript_data$text &lt;- gsub(\"[^a-zA-Z0-9 ]\", \"\", transcript_data$text)\n\n\nConvert Text to Lowercase\nStandardize text to lowercase for consistency in text analysis.\n\n\n\n     # Convert text to lowercase\n     transcript_data$text &lt;- tolower(transcript_data$text)\n\n\n\n\n2.2.3 PDF\nPDF files are a valuable source of text data, often found in research publications, government documents, and industry reports. We‚Äôll explore two main methods for extracting text from PDFs:\n\nExtracting from Local PDF Files: This method involves accessing and parsing text from PDF files stored locally, providing tools and techniques to efficiently retrieve text data from offline documents.\nDownloading and Extracting PDF Files: This approach covers downloading PDFs from online sources and extracting their text. This method is useful for scraping publicly available documents from websites or databases for research purposes.\nPDF Data Extractor (PDE)\nFor more advanced PDF text extraction and processing, you can use the PDF Data Extractor (PDE) package. This package provides tools for extracting text data from complex PDF documents, supporting additional customization options for text extraction. PDE is a R package that easily extracts information and tables from PDF files. The PDE_analyzer_i() performs the sentence and table extraction while the included PDE_reader_i() allows the user-friendly visualization and quick-processing of the obtained results.\n\n\nSteps for Extracting Text from Local PDF Files\n\nInstall and Load the pdftools Package\nStart by installing and loading the pdftools package, which is specifically designed for reading and extracting text from PDF files in R.\n\n\n   install.packages(\"pdftools\")\n   library(pdftools)\n\n\nRead the PDF as a Text File\nUse the pdf_text() function to read the PDF file into R as a text object. This function returns each page as a separate string in a character vector.\n\n\n   txt &lt;- pdf_text(\"path/to/your/file.pdf\")\n\n\nExtract Text from a Specific Page\nTo access a particular page from the PDF, specify the page number in the text vector. For example, to extract text from page 24:\n\n\n   page_text &lt;- txt[24]  # page 24\n\n\nExtract Rows into a List\nIf the page contains a table or structured text, use the scan() function to read each row as a separate element in a list. The textConnection() function converts the page text for processing.\n\n\n   rows &lt;- scan(textConnection(page_text), what = \"character\", sep = \"\\n\")\n\n\nSplit Rows into Cells\nTo further parse each row, split it into cells by specifying the delimiter, such as whitespace (using \"\\\\s+\"). This converts each row into a list of individual cells.\n\n\n   row &lt;- unlist(strsplit(rows[24], \"\\\\s+\"))  # Example with the 24th row\n\n\n\nSteps for Downloading and Extracting Text from PDF Files\n\nDownload the PDF from the Web\nUse the download.file() function to download the PDF file from a specified URL. Set the mode to \"wb\" (write binary) to ensure the file is saved correctly.\n\n\n   link &lt;- paste0(\n     \"http://www.singstat.gov.sg/docs/\",\n     \"default-source/default-document-library/\",\n     \"publications/publications_and_papers/\",\n     \"cop2010/census_2010_release3/\",\n     \"cop2010sr3.pdf\"\n   )\n   download.file(link, \"census2010_3.pdf\", mode = \"wb\")\n\n\nRead the PDF as a Text File\nAfter downloading, read the PDF into R as a text object using the pdf_text() function from the pdftools package. Each page of the PDF will be stored as a string in a character vector.\n\n\n   txt &lt;- pdf_text(\"census2010_3.pdf\")\n\n\nExtract Text from a Specific Page\nAccess the desired page (e.g., page 24) by specifying the page number in the character vector.\n\n\n   page_text &lt;- txt[24]  # Page 24\n\n\nExtract Rows into a List\nUse the scan() function to split the page text into rows, with each row representing a line of text in the PDF. This creates a list where each line from the page is an element.\n\n\n   rows &lt;- scan(textConnection(page_text), what = \"character\", sep = \"\\n\")\n\n\nLoop Through Rows and Extract Data\nStarting from a specific row (e.g., row 7), loop over each row. For each row:\n\nSplit the text by spaces (\"\\\\s+\") using strsplit().\nConvert the result to a vector with unlist().\nIf the third cell in the row is not empty, store the second cell as name and the third cell as total, converting it to a numeric format after removing commas.\n\n\n\n   name &lt;- c()\n   total &lt;- c()\n\n   for (i in 7:length(rows)) {\n     row &lt;- unlist(strsplit(rows[i], \"\\\\s+\"))\n     if (!is.na(row[3])) {\n       name &lt;- c(name, row[2])\n       total &lt;- c(total, as.numeric(gsub(\",\", \"\", row[3])))\n     }\n   }\n\n\n\n\n2.2.4 Survey, Discussions, etc.\nSurveys and discussion posts are valuable sources of text data in social science research, providing insights into participants‚Äô perspectives, opinions, and experiences. These data sources often come from open-ended survey responses, online discussion boards, or educational platforms. Extracting and preparing text data from these sources can reveal recurring themes, sentiment, and other patterns that support both quantitative and qualitative analysis. Below are key steps and code examples for processing text data from surveys and discussions in R.\n\nKey Steps for Processing Survey and Discussion Text Data\n\nLoad the Data\nSurvey and discussion data are typically stored in spreadsheet formats like CSV. Begin by loading this data into R for processing. Here, readr is used for reading CSV files with read_csv().\n\n\n   # Install and load necessary packages\n   install.packages(\"readr\")\n   library(readr)\n   \n   # Load data\n   survey_data &lt;- read_csv(\"path/to/your/survey_data.csv\")\n\n\nExtract Text Columns\nIdentify and isolate the relevant text columns. For example, if the text data is in a column named ‚ÄúResponse,‚Äù you can create a new vector for analysis.\n\n\n   # Extract text data from the specified column\n   text_data &lt;- survey_data$Response\n\n\nData Cleaning\nPrepare the text data by cleaning it, removing any unnecessary characters, and standardizing the text. This includes removing punctuation, converting text to lowercase, and handling extra whitespace.\n\nRemove Unwanted Characters\nUse gsub() from base R to remove any non-alphanumeric characters, retaining only words and spaces.\n\n\n\n     # Remove special characters\n     text_data &lt;- gsub(\"[^a-zA-Z0-9 ]\", \"\", text_data)\n\n\nConvert to Lowercase\nStandardize the text by converting all characters to lowercase.\n\n\n     # Convert text to lowercase\n     text_data &lt;- tolower(text_data)\n\n\nRemove Extra Whitespace\nRemove any extra whitespace that may be left after cleaning.\n\n\n     # Remove extra spaces\n     text_data &lt;- gsub(\"\\\\s+\", \" \", text_data)\n\n\nTokenization and Word Counting (Optional)\nIf further analysis is needed, such as frequency-based analysis, split the text into individual words (tokenization) or count the occurrence of specific words. Here, dplyr is used to organize the word counts.\n\n\n   # Install and load necessary packages\n   install.packages(\"dplyr\")\n   library(dplyr)\n   \n   # Tokenize and count words\n   word_count &lt;- strsplit(text_data, \" \") %&gt;%\n                 unlist() %&gt;%\n                 table() %&gt;%\n                 as.data.frame()",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Text Data</span>"
    ]
  },
  {
    "objectID": "chapter-2.html#frequency-based-analysis",
    "href": "chapter-2.html#frequency-based-analysis",
    "title": "Text Data",
    "section": "2.3 Frequency-based Analysis",
    "text": "2.3 Frequency-based Analysis\nIn the following section, we will provide a ‚Äúrecipe‚Äù for the social scientist interested in these new methods of analyzing text data to get you from the initial stages of getting the data to running the analyses and the write up. Often left out is also a research question that suits or requires a method. Since we have a data and method-centric approach here, we will backtrack and also provide a research question, so that you can model after it in your own work. Finally, we will provide a sample results and discussions section.\n\n2.3.1 Purpose\nThe purpose of the frequency-based approach is to count the number of words as they appear in a text file, whether it is a collection of tweets, documents, or interview transcripts. This approach aligns with the frequency-coding method (e.g., Saldana) and can supplement human coding by revealing the most/least commonly occurring words, which can then be compared across dependent variables.\n\nCase Study: Frequency-Based Analysis of GenAI USage Guidelines in Higher Education\nAs AI writing tools like ChatGPT become more prevalent, educators are working to understand how best to integrate them within academic settings, while many students and instructors remain uncertain about acceptable use cases. Our research into AI usage guidelines from the top 100 universities in North America aims to identify prominent themes and concerns in institutional policies regarding GenAI.\n\n\n\n2.3.2 Sample Research Questions\nTo investigate the nature of AI use policies within higher education institutions, in this study, our research questions are:\n\nRQ1: What are the most frequently mentioned words in university GenAI writing usage policies?\nRQ2: Which keywords reflect common concerns or focal points related to GenAI writing usage in academic settings?\n\n\n\n2.3.3 Sample Methods\n\nData Source\nThe dataset consists of publicly available AI policy texts from 100 universities(USA), the data has been downloaded and saved as a CSV file for analysis. ‚Äìwe might have to write more here to model how a research should be describing the data from its acquisition to use for research.\n\n\nData Analysis\nIn order to analyze the data we used xyz, ‚Äî-let‚Äôs provide a sample write up for the researcher to adapt.\n\n\n\n\n\n\n2.3.4 Analysis\n\nStep 1: Load Required Libraries\nInstall and load libraries for data processing, visualization, and word cloud generation.\n\n# Install necessary packages if not already installed\n#install.packages(c(\"tibble\", \"dplyr\", \"tidytext\", \"ggplot2\", \"viridis\",\"tm\",wordcloud\" \"wordcloud2\",\"webshot\"))\n\n# Load libraries\nlibrary(readr)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(tm)\nlibrary(wordcloud)\nlibrary(wordcloud2)\nlibrary(webshot)\n\n\n\nStep 2: Load Data\nRead the CSV file containing policy texts from the top 100 universities.\n\n# Load the dataset\nuniversity_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\")\n\n\n\nStep 3: Tokenize Text and Count Word Frequency\nProcess the text data by tokenizing words, removing stop words, and counting word occurrences.\n\n# Tokenize text, remove stop words, and count word frequencies\nword_frequency &lt;- university_policies %&gt;%\n  unnest_tokens(word, Stance) %&gt;%  # Tokenize the 'Policy_Text' column\n  anti_join(stop_words) %&gt;%             # Remove common stop words\n  count(word, sort = TRUE)              # Count and sort words by frequency\nword_frequency\n\n# A tibble: 1,108 √ó 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 ai            124\n 2 students      120\n 3 chatgpt        80\n 4 tools          73\n 5 academic       68\n 6 policy         35\n 7 assignments    34\n 8 instructors    34\n 9 integrity      33\n10 student        32\n# ‚Ñπ 1,098 more rows\n\n\n\n\nStep 4: Create a Word Cloud\nGenerate a word cloud to visualize the frequency of words in a circular shape.\n\n# Create and display the GenAI usage Stance wordcloud\n\nwordcloud(words = word_frequency$word, freq = word_frequency$n, scale = c(4, 0.5), random.order = FALSE, min.freq = 10, colors = brewer.pal(8, \"Dark2\"), rot.per = 0.35)\n\n\n\n\n\n\n\n\n\n\nStep 5: Visualize Top 12 Words in University Policies\nSelect the top 12 most frequent words and create a bar chart to visualize the distribution.\n\n# Select the top 12 words\ntop_words &lt;- word_frequency %&gt;% slice(1:12)\n\n# Generate the bar chart\npolicy_word_chart &lt;- ggplot(top_words, aes(x = reorder(word, n), y = n, fill = word)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(\n    title = \"Top Words in University GenAI Policies\",\n    x = NULL,\n    y = \"Frequency\",\n    caption = \"Source: University AI Policy Text\",\n    fill = \"Word\"\n  ) +\n  scale_fill_viridis(discrete = TRUE) +\n  geom_text(aes(label = n), vjust = 0.5, hjust = -0.1, size = 3)\n\n# Print the bar chart\nprint(policy_word_chart)\n\n\n\n\n\n\n\n\n\n\n\n2.3.5 Results and Discussions\n\nRQ1: What are the most frequently mentioned words in university GenAI writing usage policies?\nThe results of the frequency analysis showed that keywords such as ‚Äúassignment,‚Äù ‚Äústudent,‚Äù and ‚Äúwriting‚Äù were among the most commonly mentioned terms across AI policies at 100 universities. This emphasis reflects a focus on using AI tools to support student learning and enhance teaching content. The frequent mention of these words suggests that institutions are considering the role of AI in academic assignments and course design, indicating a strategic commitment to integrating AI within educational tasks and student interactions.\nRQ2: Which keywords reflect common concerns or focal points related to GenAI writing usage in academic settings?\nThe analysis of the top 12 frequently mentioned terms highlighted additional focal points, including ‚Äútool,‚Äù ‚Äúacademic,‚Äù ‚Äúinstructor,‚Äù ‚Äúintegrity,‚Äù and ‚Äúexpectations.‚Äù These terms reveal concerns around the ethical use of AI tools, the need for clarity in academic applications, and the central role of instructors in AI policy implementation. Keywords like ‚Äúintegrity‚Äù and ‚Äúexpectations‚Äù emphasize the importance of maintaining academic standards and setting clear guidelines for AI use in classrooms, while ‚Äúinstructor‚Äù underscores the influence faculty members have in shaping AI-related practices. Together, these terms reflect a commitment to transparent policies that support ethical and effective AI integration, enhancing the academic experience for students.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Text Data</span>"
    ]
  },
  {
    "objectID": "chapter-2.html#dictionary-based-analysis",
    "href": "chapter-2.html#dictionary-based-analysis",
    "title": "Text Data",
    "section": "2.4 Dictionary-based Analysis",
    "text": "2.4 Dictionary-based Analysis\n\n2.4.1 Purpose\nThe purpose of dictionary-based analysis in text data is to assess the presence of predefined categories, like emotions or sentiments, within the text using lexicons or dictionaries. This approach allows researchers to quantify qualitative aspects, such as positive or negative sentiment, based on specific words that correspond to these categories.\nCase:\nIn this analysis, we examine the stance of 100 universities on the use of GenAI by applying the Bing sentiment dictionary. By analyzing sentiment scores, we aim to identify the general tone in these policies, indicating whether the institutions‚Äô attitudes toward GenAI are predominantly positive or negative.\n\n\n2.4.2 Sample Research Questions\n\nRQ: What is the dominant sentiment expressed in GenAI policy texts across universities, and is it primarily positive or negative?\n\n\n\n2.4.3 Analysis\n\nStep 1: Install and Load Necessary Libraries\nFirst, install and load the required packages for text processing and visualization.\n\n# Install necessary packages if not already installed\n#install.packages(c(\"tidytext\",\"tidyverse\" \"dplyr\", \"ggplot2\", \"tidyr\"))\n\n# Load libraries\nlibrary(tidytext)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\n\nStep 2: Load and Prepare Data(same as 2.3)\nLoad the GenAI policy stance data from a CSV file. Be sure to update the file path as needed. we use the same data as 2.3.\n\n# Load the dataset (replace \"University_GenAI_Policy_Stance.csv\" with the actual file path)\nuniversity_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\")\n# Tokenize text, remove stop words, and count word frequencies\nword_frequency &lt;- university_policies %&gt;%\n  unnest_tokens(word, Stance) %&gt;%  # Tokenize the 'Policy_Text' column\n  anti_join(stop_words) %&gt;%             # Remove common stop words\n  count(word, sort = TRUE)              # Count and sort words by frequency\nword_frequency\n\n\n\nStep 3: Tokenize Text Data and Apply Sentiment Dictionary\nTokenize the policy text data to separate individual words. Then, use the Bing sentiment dictionary to label each word as positive or negative.\n\n# Tokenize 'Stance' column and apply Bing sentiment dictionary\nsentiment_scores &lt;- word_frequency %&gt;%\n  inner_join(get_sentiments(\"bing\")) %&gt;% # Join with Bing sentiment lexicon\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%\n  mutate(sentiment_score = positive - negative) # Calculate net sentiment score\n\nsentiment_scores\n\n# A tibble: 142 √ó 4\n   word         positive negative sentiment_score\n   &lt;chr&gt;           &lt;int&gt;    &lt;int&gt;           &lt;int&gt;\n 1 honor              18        0              18\n 2 cheating            0       11             -11\n 3 dishonesty          0       10             -10\n 4 guidance            9        0               9\n 5 honesty             7        0               7\n 6 intelligence        7        0               7\n 7 transparent         7        0               7\n 8 violation           0        7              -7\n 9 encourage           6        0               6\n10 difficult           0        5              -5\n# ‚Ñπ 132 more rows\n\n\n\n\nStep 4: Create a Density Plot for Sentiment Distribution\nVisualize the distribution of sentiment scores with a density plot, showing the prevalence of positive and negative sentiments across university policies.\n\n# Generate a density plot of sentiment scores\ndensity_plot &lt;- ggplot(sentiment_scores, aes(x = sentiment_score, fill = sentiment_score &gt; 0)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"red\", \"green\"), name = \"Sentiment\",\n                    labels = c(\"Negative\", \"Positive\")) +\n  labs(\n    title = \"Density Plot of University AI Policy Sentiment\",\n    x = \"Sentiment Score\",\n    y = \"Density\",\n    caption = \"Source: University Policy Text\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 20),\n    axis.text = element_text(size = 14),\n    axis.title = element_text(face = \"bold\", size = 16),\n    plot.caption = element_text(size = 12)\n  )\n\n# Print the plot\nprint(density_plot)\n\n\n\n\n\n\n\n\n\n\n\n2.4.4 Results and Discussions\n\nRQ: What is the dominant sentiment expressed in GenAI policy texts across universities, and is it primarily positive or negative?\nThe dictionary-based sentiment analysis reveals the prevailing sentiments in university policies on GenAI usage. Using the Bing lexicon to assign positive and negative scores, the density plot illustrates the distribution of sentiment scores across the 100 institutions.\nThe results indicate a balanced perspective with a slight tendency toward positive sentiment, as reflected by a higher density of positive scores. This analysis provides insights into the varying degrees of acceptance and caution universities adopt in their AI policy frameworks, demonstrating the diverse stances that shape institutional AI guidelines.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Text Data</span>"
    ]
  },
  {
    "objectID": "chapter-2.html#clustering-based-analysis",
    "href": "chapter-2.html#clustering-based-analysis",
    "title": "Text Data",
    "section": "2.5 Clustering-Based Analysis",
    "text": "2.5 Clustering-Based Analysis\nClustering-based analysis involves grouping similar text documents or text segments into clusters based on their underlying topics or themes. This approach is particularly useful for identifying dominant themes in text data, such as university AI policy documents.\n\n2.5.1 Purpose\nPurpose: The goal of clustering-based analysis is to uncover latent themes in text data using unsupervised machine learning techniques. Topic modeling is one popular method for clustering documents into groups based on their content.\nCase: Using the GenAI policy texts from 100 universities, we apply Latent Dirichlet\nAllocation (LDA) to identify dominant themes in these policy documents. This analysis will help categorize policies into overarching themes, such as academic integrity, student support, and instructor discretion.\n\n\n2.5.2 Sample Research Questions\n\nRQ1: What are the prominent themes present in university policies regarding GenAI usage Stance?\nRQ2: How do these themes reflect the key concerns or opportunities for intergrating GenAI in higher education?\n\n\n\n2.5.3 Analysis\n\nStep 1: Install and Load Necessary Libraries\nInstall and load the required libraries for text processing and topic modeling.\n\n# Install necessary packages\n#install.packages(c(\"dplyr\", \"tidytext\", \"topicmodels\", \"ggplot2\"))\n\n# Load libraries\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(topicmodels)\nlibrary(ggplot2)\n\n\n\nStep 2: Prepare the Data\nLoad the data and create a document-term matrix (DTM) for topic modeling.\n\n# Load the dataset\nuniversity_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\")\n\n# Tokenize text data and count word frequencies\nword_frequency &lt;- university_policies %&gt;%\n  unnest_tokens(word, Stance) %&gt;%  # same as section 2.3\n  anti_join(stop_words) %&gt;%             # Remove common stop words\n  count(word, sort = TRUE)              # Count and sort words by frequency\nword_frequency\n\n# A tibble: 1,108 √ó 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 ai            124\n 2 students      120\n 3 chatgpt        80\n 4 tools          73\n 5 academic       68\n 6 policy         35\n 7 assignments    34\n 8 instructors    34\n 9 integrity      33\n10 student        32\n# ‚Ñπ 1,098 more rows\n\n# Creating Documents - Word Frequency Matrix\ngpt_dtm &lt;- word_frequency %&gt;%\n  group_by(word) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  ungroup() %&gt;%\n  cast_dtm(document = \"id\", term = \"word\", value = \"n\")\n\n\nlibrary(topicmodels)\nlibrary(ggplot2)\n\n# Define range of k values\nk_values &lt;- c(2, 3, 4, 5)\n\n# Initialize a data frame to store perplexities\nperplexities &lt;- data.frame(k = integer(), perplexity = numeric())\n\n# Calculate perplexity for each k\nfor (k in k_values) {\n  lda_model &lt;- LDA(gpt_dtm, k = k, control = list(seed = 1234))  # Fit LDA model\n  perplexity_score &lt;- perplexity(lda_model, gpt_dtm)            # Calculate perplexity\n  perplexities &lt;- rbind(perplexities, data.frame(k = k, perplexity = perplexity_score))\n}\n\n# Plot perplexity vs number of topics\nggplot(perplexities, aes(x = k, y = perplexity)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Perplexity vs Number of Topics\",\n    x = \"Number of Topics (k)\",\n    y = \"Perplexity\"\n  ) +\n  theme_minimal()\n\n\n\nStep 3: Fit the LDA Model\nFit an LDA model with k = 3 topics.\n\n# Converting document-word frequency matrices to sparse matrices\ngpt_dtm_sparse&lt;- as(gpt_dtm, \"matrix\")\n\n# Fit the LDA model\nlda_model &lt;- LDA(gpt_dtm_sparse, k = 3, control = list(seed = 1234))\n\n# View model results\ngpt_policy_topics_k3 &lt;- tidy(lda_model, matrix = \"beta\")\n\nprint(gpt_policy_topics_k3)\n\n# A tibble: 3,324 √ó 3\n   topic term        beta\n   &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1     1 ai       0.0408 \n 2     2 ai       0.00593\n 3     3 ai       0.0650 \n 4     1 students 0.0435 \n 5     2 students 0.0635 \n 6     3 students 0.0151 \n 7     1 chatgpt  0.0396 \n 8     2 chatgpt  0.0248 \n 9     3 chatgpt  0.0126 \n10     1 tools    0.0197 \n# ‚Ñπ 3,314 more rows\n\n\n\n\nStep 4: Visualize Topics\nExtract the top terms for each topic and visualize them.\n\n# Visualizing top terms for each topic\ngpt_policy_ap_top_terms_k3 &lt;- gpt_policy_topics_k3 %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\ngpt_policy_ap_top_terms_k3 %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\n\n\n\n\n\n\n2.5.4 Results and Discussions\n\nResearch Question 1:\nWhat are the prominent themes present in university policies regarding GenAI usage?\nAnswer:\nThe topic modeling analysis revealed three distinct themes in the university GenAI policies:\n\nTheme 1: Student-Centric Guidelines and Ethical Considerations\n\nKey terms: students, integrity, tools, instructors, assignment\nThis theme emphasizes student usage of GenAI in academic settings, with a focus on ethics (integrity) and guidelines for instructors to manage assignments involving AI tools.\n\nTheme 2: Academic Standards and Faculty Expectations\n\nKey terms: students, academic, faculty, honor, expectations\nThis theme focuses on maintaining academic integrity and clarifying expectations for faculty and students regarding GenAI usage in assignments and assessments.\n\nTheme 3: Policy-Level Governance and Technology Integration\n\nKey terms: ai, tools, policy, learning, generative\nThis theme revolves around institutional policies on AI integration, highlighting broader governance strategies and how generative AI (like GenAI) fits into learning environments.\n\n\n\n\nResearch Question 2:\nHow do these themes reflect the key concerns or opportunities for integrating GenAI in higher education?\nAnswer:\nThe identified themes reflect both concerns and opportunities:\n\nConcerns:\nTheme 1: Highlights the ethical challenges, such as ensuring academic integrity when students use AI tools in their coursework. Institutions are keen on setting clear guidelines for both students and instructors to avoid misuse.\nTheme 2: Underlines the potential for conflict between maintaining academic standards (honor, expectations) and leveraging AI to support learning. This shows a cautious approach to integrating AI while upholding traditional values.\nTheme 3: Raises policy-level questions on AI governance, such as whether existing institutional frameworks are adequate to regulate emerging generative AI technologies.\nOpportunities:\nTheme 1: Presents a chance to redefine how students interact with AI tools to foster responsible and innovative usage, particularly for assignments and creative tasks.\nTheme 2: Encourages collaboration between faculty and administration to develop robust expectations and support systems for integrating AI in the classroom.\nTheme 3: Offers a strategic opportunity for universities to lead in AI adoption by establishing comprehensive policies that guide AI‚Äôs role in education and research.\n\n\n\nDiscussion:\nThe topic modeling results suggest that universities are navigating a complex landscape of opportunities and challenges as they incorporate GenAI into academic contexts. While student-centric policies aim to balance innovation with ethical considerations, institutional-level themes signal the need for governance frameworks to ensure responsible AI use. These findings indicate that higher education institutions are positioned to play a pivotal role in shaping the future of generative AI in learning, provided they address the ethical, pedagogical, and policy challenges identified in this analysis.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Text Data</span>"
    ]
  },
  {
    "objectID": "chapter-3.html",
    "href": "chapter-3.html",
    "title": "Networks Data",
    "section": "",
    "text": "3.1 Overview\nSocial network analysis (SNA) is the process of investigating social structures through the use of networks and graph theory. It is a technique used to map and measure relationships and flows between people, groups, organizations, computers, or other information/knowledge processing entities. SNA can be a useful tool for understanding the team structures, for example, in an online classroom. It can be an additional layer of understanding the outcomes (or predictors) of certain instructional interventions. Used this way SNA can be used to identify patterns and trends in social networks, as well as to understand how these networks operate. Additionally, SNA can be used to predict future behavior in social networks, and to design interventions that aim to improve the functioning of these networks.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Networks Data</span>"
    ]
  },
  {
    "objectID": "chapter-3.html#accessing-sna-data",
    "href": "chapter-3.html#accessing-sna-data",
    "title": "Networks Data",
    "section": "3.2 Accessing SNA Data",
    "text": "3.2 Accessing SNA Data\nSocial Network Analysis (SNA) relies on relational data‚Äîinformation about connections (edges) between entities (nodes) such as students, teachers, or organizations. Compared to traditional survey or tabular data, SNA requires pairwise relational information. In education, this could include ‚Äúwho collaborates with whom,‚Äù ‚Äúwho talks to whom,‚Äù or digital traces of discussion and collaboration in online platforms.\n\n3.2.1 Types and Sources of SNA Data\nThere are several common sources and structures for SNA data in educational and social science contexts:\n\nSurvey-based Network Data: Collected via roster or name generator questions, e.g., ‚ÄúList the classmates you discuss assignments with.‚Äù\nBehavioral/Observational Data: Derived from logs of actual interactions, e.g., forum replies, emails, classroom seating.\nArchival or Digital Trace Data: Extracted from digital platforms such as MOOCs, LMS discussion forums, Slack, Twitter, or Facebook.\nAdministrative/Organizational Data: Information about formal structures such as team membership or co-authorship.\n\nData Structure: Most SNA data are formatted as: - Edge List (two columns: source and target) - Adjacency Matrix (rows and columns are actors; cell values indicate a tie) - Node Attributes (supplementary information about each node, e.g., gender, role)\n\n\n3.2.2 Example 1: Creating a Simple Network from an Edge List\nBelow is an example of constructing a network from a simple CSV edge list. This mirrors typical classroom survey data (‚Äúwho do you consider your friend in this class?‚Äù).\n\n# Install and load the igraph package\ninstall.packages(\"igraph\")\nlibrary(igraph)\n\n# Example: Load an edge list from CSV\nedge_list &lt;- read.csv(\"data/friendship_edges.csv\")\n\n# Create the graph object (directed network)\ng &lt;- graph_from_data_frame(edge_list, directed = TRUE)\n\n# Plot the network\nplot(g, main = \"Friendship Network\")\n\n\n\n3.2.3 Example 2: Generating Network Data from Digital Traces\nMany educational datasets now come from online discussion forums, MOOCs, or LMS systems. For example, the MOOC case study (Kellogg & Edelmann, 2015) uses reply relationships in online courses to construct discussion networks.\n# Suppose you have a data frame with columns: from_user, to_user\nmooc_edges &lt;- read.csv(\"data/mooc_discussion_edges.csv\")\ng_mooc &lt;- graph_from_data_frame(mooc_edges, directed = TRUE)\nplot(g_mooc, main = \"MOOC Discussion Network\")\n\n\n3.2.4 Example 3: Collecting SNA Data via Surveys\nIf you want to collect your own network data:\n\nAsk participants to name or select (from a roster) their friends, collaborators, or contacts.\nCompile responses into an edge list.\nExample survey prompt:\n\n‚ÄúPlease list up to five classmates you seek help from most frequently.‚Äù\n\n\nTip:\nSurvey-based SNA is easier to manage with small to medium groups. For larger networks, digital trace or archival data may be more practical.\n\n\n3.2.5 Node Attribute Data\nYou can also load additional data about each node (student, teacher, etc.) to enable richer analyses (e.g., centrality by gender or role).\nnode_attributes &lt;- read.csv(\"data/friendship_nodes.csv\")\n# Add attributes to igraph object\nV(g)$gender &lt;- node_attributes$gender[match(V(g)$name, node_attributes$name)]\n\n\n3.2.6 Further Examples\n\nPublic Datasets:\n\nMOOC Discussion Networks\nAdd Health\nCommon Core Twitter Networks (Supovitz et al.)\n\nSynthetic Data:\n\nR‚Äôs igraph package can also generate sample networks for practice:\ng_sample &lt;- sample_gnp(n = 10, p = 0.3)\nplot(g_sample, main = \"Random Sample Network\")\n\n\n\n\n3.2.7 Best Practices and Tips\n\nEthics: Social network data can be sensitive. Protect anonymity and comply with IRB/data use guidelines.\nFormat Consistency: Always clarify whether ties are directed/undirected, binary/weighted, and ensure consistent formatting.\nMissing Data: Especially in survey-based SNA, missing responses can impact network structure and interpretation.\n\n\n\n3.2.8 Summary\nAccessing SNA data involves both careful design (in the case of surveys/observations) and extraction/wrangling (in the case of digital traces or archival records). The choice of data source and structure will directly influence the kinds of questions you can answer with SNA.\n\nRecommended Reading:\n\nBorgatti, S. P., Everett, M. G., & Johnson, J. C. (2018). Analyzing Social Networks (2nd ed). SAGE.\nKellogg, S., & Edelmann, A. (2015). Massive open online course discussion forums as networks.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Networks Data</span>"
    ]
  },
  {
    "objectID": "chapter-3.html#network-management-measurement-in-social-network-analysis",
    "href": "chapter-3.html#network-management-measurement-in-social-network-analysis",
    "title": "Networks Data",
    "section": "3.3 Network Management & Measurement in Social Network Analysis",
    "text": "3.3 Network Management & Measurement in Social Network Analysis\n\n3.3.1 Purpose + Case\nPurpose: This section demonstrates how to manage, measure, and visualize large-scale discussion networks from online professional development settings. Through this real-world example, we guide readers in loading relational data, constructing a directed network, and conducting a suite of essential SNA measures. The focus is on classroom- and course-level online discussions, which are representative of many contemporary educational and research settings.\nCase Study: The case data comes from two cohorts of an online professional development program (‚ÄúDLT1‚Äù and ‚ÄúDLT2‚Äù). Each cohort‚Äôs discussion data includes (a) edge list data capturing who replied to whom, and (b) node/actor attributes describing roles (e.g., facilitator, expert). These data allow us to reconstruct and analyze the full structure of communication in two authentic online learning communities.\n\n\n3.3.2 Sample Research Questions\n\nRQ1: What is the overall structure of interaction in each online discussion cohort? Are they densely connected, or fragmented?\nRQ2: Who are the most central or influential actors in the network? How do facilitators or experts compare with regular participants?\nRQ3: To what extent are ties reciprocated (mutual) and how cohesive are the networks?\nRQ4: How do the network properties (e.g., density, reciprocity, clustering) compare between cohorts?\n\n\n\n3.3.3 Analysis\n\nStep 1: Install and Load Required Packages\n\n# Install and load necessary libraries\n#install.packages(c(\"tidygraph\", \"ggraph\", \"readr\", \"janitor\"))\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(readr)\nlibrary(janitor)\nlibrary(igraph)\nlibrary(dplyr)\n\n\n\nStep 2: Import and Clean DataÔºöLoad Edges and Node Attributes for DLT1:\n\n# Load edge list (who replied to whom)\ndlt1_ties &lt;- read_csv(\"data/dlt1-edges.csv\", \n  col_types = cols(Sender = col_character(), \n                   Receiver = col_character(), \n                   `Category Text` = col_skip(), \n                   `Comment ID` = col_character(), \n                   `Discussion ID` = col_character())) |&gt; \n  clean_names()\n\n# Load node attributes (participant roles, etc.)\ndlt1_actors &lt;- read_csv(\"data/dlt1-nodes.csv\", \n  col_types = cols(UID = col_character(), \n                   Facilitator = col_character(), \n                   expert = col_character(), \n                   connect = col_character())) |&gt; \n  clean_names()\n\nhead(dlt1_ties)\n\n# A tibble: 6 √ó 9\n  sender receiver timestamp discussion_title discussion_category parent_category\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;               &lt;chr&gt;          \n1 360    444      4/4/13 1‚Ä¶ Most important ‚Ä¶ Group N             Units 1-3 Disc‚Ä¶\n2 356    444      4/4/13 1‚Ä¶ Most important ‚Ä¶ Group D-L           Units 1-3 Disc‚Ä¶\n3 356    444      4/4/13 1‚Ä¶ DLT Resources‚ÄîC‚Ä¶ Group D-L           Units 1-3 Disc‚Ä¶\n4 344    444      4/4/13 1‚Ä¶ Most important ‚Ä¶ Group O-T           Units 1-3 Disc‚Ä¶\n5 392    444      4/4/13 1‚Ä¶ Most important ‚Ä¶ Group U-Z           Units 1-3 Disc‚Ä¶\n6 219    444      4/4/13 1‚Ä¶ Most important ‚Ä¶ Group M             Units 1-3 Disc‚Ä¶\n# ‚Ñπ 3 more variables: discussion_identifier &lt;chr&gt;, comment_id &lt;chr&gt;,\n#   discussion_id &lt;chr&gt;\n\nhead(dlt1_actors)\n\n# A tibble: 6 √ó 13\n  uid   facilitator role1  experience experience2 grades location region country\n  &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  \n1 1     0           libme‚Ä¶          1 6 to 10     secon‚Ä¶ VA       South  US     \n2 2     0           class‚Ä¶          1 6 to 10     secon‚Ä¶ FL       South  US     \n3 3     0           distr‚Ä¶          2 11 to 20    gener‚Ä¶ PA       North‚Ä¶ US     \n4 4     0           class‚Ä¶          2 11 to 20    middle NC       South  US     \n5 5     0           other‚Ä¶          3 20+         gener‚Ä¶ AL       South  US     \n6 6     0           class‚Ä¶          1 4 to 5      gener‚Ä¶ AL       South  US     \n# ‚Ñπ 4 more variables: group &lt;chr&gt;, gender &lt;chr&gt;, expert &lt;chr&gt;, connect &lt;chr&gt;\n\n\n\n\nStep 3: Construct and Explore the Network\n\n# Build the directed network graph (nodes: uid, edges: sender-&gt;receiver)\ndlt1_network &lt;- tbl_graph(\n  edges = dlt1_ties,\n  nodes = dlt1_actors,\n  node_key = \"uid\",\n  directed = TRUE\n)\n\n# Overview of the network\ndlt1_network\n\n# A tbl_graph: 445 nodes and 2529 edges\n#\n# A directed multigraph with 4 components\n#\n# Node Data: 445 √ó 13 (active)\n   uid   facilitator role1 experience experience2 grades location region country\n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  \n 1 1     0           libm‚Ä¶          1 6 to 10     secon‚Ä¶ VA       South  US     \n 2 2     0           clas‚Ä¶          1 6 to 10     secon‚Ä¶ FL       South  US     \n 3 3     0           dist‚Ä¶          2 11 to 20    gener‚Ä¶ PA       North‚Ä¶ US     \n 4 4     0           clas‚Ä¶          2 11 to 20    middle NC       South  US     \n 5 5     0           othe‚Ä¶          3 20+         gener‚Ä¶ AL       South  US     \n 6 6     0           clas‚Ä¶          1 4 to 5      gener‚Ä¶ AL       South  US     \n 7 7     0           inst‚Ä¶          2 11 to 20    gener‚Ä¶ SD       Midwe‚Ä¶ US     \n 8 8     0           spec‚Ä¶          1 6 to 10     secon‚Ä¶ BE       Inter‚Ä¶ BE     \n 9 9     0           clas‚Ä¶          1 6 to 10     middle NC       South  US     \n10 10    0           scho‚Ä¶          2 11 to 20    middle NC       South  US     \n# ‚Ñπ 435 more rows\n# ‚Ñπ 4 more variables: group &lt;chr&gt;, gender &lt;chr&gt;, expert &lt;chr&gt;, connect &lt;chr&gt;\n#\n# Edge Data: 2,529 √ó 9\n   from    to timestamp    discussion_title  discussion_category parent_category\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;             &lt;chr&gt;               &lt;chr&gt;          \n1   360   444 4/4/13 16:32 Most important c‚Ä¶ Group N             Units 1-3 Disc‚Ä¶\n2   356   444 4/4/13 18:45 Most important c‚Ä¶ Group D-L           Units 1-3 Disc‚Ä¶\n3   356   444 4/4/13 18:47 DLT Resources‚ÄîCo‚Ä¶ Group D-L           Units 1-3 Disc‚Ä¶\n# ‚Ñπ 2,526 more rows\n# ‚Ñπ 3 more variables: discussion_identifier &lt;chr&gt;, comment_id &lt;chr&gt;,\n#   discussion_id &lt;chr&gt;\n\n# Output: 445 nodes, 2529 edges, 4 components (directed multigraph)\n\n\n\nStep 4: Basic Visualization\n\n# Quick overview plot (stress layout by default)\nautograph(dlt1_network)\n\n\n\n\n\n\n\n# Custom visualization with colors and centrality\nggraph(dlt1_network, layout = \"fr\") +\n  geom_edge_link(alpha = .2) +\n  geom_node_point(aes(color = role1, size = local_size())) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\n\n\n\nStep 5: Network Size and Centralization\n\n# Number of nodes and edges\ngorder(dlt1_network)   # 445\n\n[1] 445\n\ngsize(dlt1_network)    # 2529\n\n[1] 2529\n\n# Degree centrality (all, in, out)\ndeg_all &lt;- centr_degree(dlt1_network, mode = \"all\")$res\ndeg_in  &lt;- centr_degree(dlt1_network, mode = \"in\")$res\ndeg_out &lt;- centr_degree(dlt1_network, mode = \"out\")$res\n# Centralization\ncentr_degree(dlt1_network, mode = \"all\")$centralization  # 0.64\n\n[1] 0.6429242\n\ncentr_degree(dlt1_network, mode = \"in\")$centralization   # 1.06\n\n[1] 1.05702\n\ncentr_degree(dlt1_network, mode = \"out\")$centralization  # 0.23\n\n[1] 0.2259389\n\n\n\n\nStep 6: Attach and Visualize Node Centrality\n\n# Add in-degree centrality as node attribute\ndlt1_network &lt;- dlt1_network |&gt;\n  activate(nodes) |&gt;\n  mutate(in_degree = centrality_degree(mode = \"in\"))\n\n# Plot, sizing nodes by in-degree\nggraph(dlt1_network) +\n  geom_edge_link(alpha = .2) +\n  geom_node_point(aes(size = in_degree, color = role1)) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\n\n\n\nStep 7: Network Density, Reciprocity, Clustering, Distance\n\n# Density\nedge_density(dlt1_network)      # 0.013 (sparse network)\n\n[1] 0.01279988\n\n# Reciprocity\nreciprocity(dlt1_network)       # 0.20 (20% of ties are reciprocated)\n\n[1] 0.1997544\n\n# Add reciprocated edge attribute and plot\ndlt1_network &lt;- dlt1_network |&gt;\n  activate(edges) |&gt;\n  mutate(reciprocated = edge_is_mutual())\nggraph(dlt1_network) +\n  geom_node_point(aes(size = in_degree)) +\n  geom_edge_link(aes(color = reciprocated), alpha = .2) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\n# Clustering (transitivity/global)\ntransitivity(dlt1_network)      # 0.089\n\n[1] 0.08880774\n\n# Network diameter (longest shortest path) & average distance\ndiameter(dlt1_network)          # 8\n\n[1] 8\n\nmean_distance(dlt1_network)     # 3.03\n\n[1] 3.030694\n\n\n\n\nStep 8:Repeat for DLT2\n\n# Step 8: Repeat for DLT2\n\n# 1. Load the DLT2 edge and node data\ndlt2_ties &lt;- read_csv(\"data/dlt2-edges.csv\", \n  col_types = cols(Sender = col_character(), \n                   Receiver = col_character(), \n                   `Category Text` = col_skip(), \n                   `Comment ID` = col_character(), \n                   `Discussion ID` = col_character())) |&gt; \n  clean_names()\n\ndlt2_actors &lt;- read_csv(\"data/dlt2-nodes.csv\", \n  col_types = cols(UID = col_character(), \n                   Facilitator = col_character(), \n                   expert = col_character(), \n                   connect = col_character())) |&gt; \n  clean_names()\n\n# 2. Construct the directed network\ndlt2_network &lt;- tbl_graph(\n  edges = dlt2_ties,\n  nodes = dlt2_actors,\n  node_key = \"uid\",\n  directed = TRUE\n)\n\n# 3. Basic network properties\nnum_nodes &lt;- gorder(dlt2_network)   # Number of nodes\nnum_edges &lt;- gsize(dlt2_network)    # Number of edges\n\n# 4. Degree centrality (overall, in, out)\ndeg_all &lt;- centr_degree(dlt2_network, mode = \"all\")$res\ndeg_in  &lt;- centr_degree(dlt2_network, mode = \"in\")$res\ndeg_out &lt;- centr_degree(dlt2_network, mode = \"out\")$res\n\n# Centralization values\ncentr_all &lt;- centr_degree(dlt2_network, mode = \"all\")$centralization\ncentr_in  &lt;- centr_degree(dlt2_network, mode = \"in\")$centralization\ncentr_out &lt;- centr_degree(dlt2_network, mode = \"out\")$centralization\n\n# 5. Attach centrality as a node attribute\ndlt2_network &lt;- dlt2_network |&gt;\n  activate(nodes) |&gt;\n  mutate(in_degree = centrality_degree(mode = \"in\"))\n\n# 6. Visualize the network\nggraph(dlt2_network, layout = \"fr\") +\n  geom_edge_link(alpha = .2) +\n  geom_node_point(aes(color = role, size = local_size())) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\n# 7. Density, reciprocity, clustering, distances\ndensity &lt;- edge_density(dlt2_network)\nrecip   &lt;- reciprocity(dlt2_network)\ndlt2_network &lt;- dlt2_network |&gt;\n  activate(edges) |&gt;\n  mutate(reciprocated = edge_is_mutual())\nggraph(dlt2_network) +\n  geom_node_point(aes(size = in_degree)) +\n  geom_edge_link(aes(color = reciprocated), alpha = .2) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\ntrans   &lt;- transitivity(dlt2_network)\ndiam    &lt;- diameter(dlt2_network)\nmean_d  &lt;- mean_distance(dlt2_network)\n\n# 8. Print summary statistics\ncat(\"DLT2 Network Stats:\\n\")\n\nDLT2 Network Stats:\n\ncat(\"Nodes:\", num_nodes, \"Edges:\", num_edges, \"\\n\")\n\nNodes: 492 Edges: 2584 \n\ncat(\"Degree Centralization (all/in/out):\", centr_all, centr_in, centr_out, \"\\n\")\n\nDegree Centralization (all/in/out): 0.5311161 0.8650671 0.3273889 \n\ncat(\"Density:\", density, \"Reciprocity:\", recip, \"\\n\")\n\nDensity: 0.0106966 Reciprocity: 0.2500977 \n\ncat(\"Transitivity:\", trans, \"Diameter:\", diam, \"Mean Distance:\", mean_d, \"\\n\")\n\nTransitivity: 0.1248291 Diameter: 8 Mean Distance: 3.03815 \n\n\n\n\n\n3.3.4 Results and Discussion\n\nRQ1: What is the overall structure of interaction in each cohort?\n\nDLT1 consists of 445 nodes (participants) and 2529 edges (directed interactions).\nDLT2 has 492 nodes and 2584 edges.\nBoth networks are large and sparse:\n\nDensity: DLT1 = 0.013, DLT2 = 0.011\nInterpretation: Only about 1‚Äì1.3% of all possible connections exist‚Äîtypical for online discussion networks where not every participant interacts with every other.\n\nBoth networks are multi-component (several disconnected groups), but most participants are included in the main giant component.\nThe diameter (longest shortest path) is 8 for both cohorts, and the average shortest path length is about 3.03 (DLT1) and 3.04 (DLT2), indicating that on average, any participant is just 3 steps away from any other in the largest group.\nInterpretation: Information or discussion threads can reach most participants with only a few hops, but overall engagement is selective rather than comprehensive.\n\n\n\nRQ2: Who are the most central or influential actors?\n\nCentrality (degree, in-degree, out-degree) analyses show a right-skewed distribution: most participants have low centrality, but a small subset are highly connected.\nIn both DLT1 and DLT2, facilitators and a handful of highly active participants emerge as hubs‚Äîthey initiate and/or receive a disproportionate number of interactions.\n\nFor DLT1, degree centralization (all): 0.64 (in: 1.06, out: 0.23)\nFor DLT2, degree centralization (all): 0.53 (in: 0.87, out: 0.33)\n\nVisualization: Network plots with node size proportional to in-degree clearly highlight these central actors.\nInterpretation: These key individuals (often facilitators) play critical roles in steering discussion, providing feedback, and potentially keeping less active members engaged.\n\n\n\nRQ3: Are ties reciprocated?\n\nReciprocity (proportion of mutual connections):\n\nDLT1: 0.20 (20% of ties are reciprocated)\nDLT2: 0.25 (25% reciprocated)\n\nInterpretation: Most interactions are one-way (e.g., a reply that does not receive a response), but a substantial fraction are mutual‚Äîpossibly reflecting peer-to-peer conversations or ongoing exchanges. In online learning contexts, this suggests a mix of broadcasting (one-to-many) and genuine dialog (two-way).\n\n\n\nRQ4: How cohesive are the networks?\n\nTransitivity/Clustering coefficient (probability that two connected nodes‚Äô neighbors are also connected):\n\nDLT1: 0.089\nDLT2: 0.125\n\nInterpretation: Triadic closure is low‚Äîthere are few closed triangles, so close-knit groups (where ‚Äúmy friend is also your friend‚Äù) are rare. The network structure is more ‚Äúhub-and-spoke‚Äù than ‚Äúcliquish.‚Äù\nDiameter: 8 for both, showing that even the furthest nodes can be reached in 8 steps.\nMean distance: ~3.0, so participants are relatively close to each other in the main component.\n\nComparison Across Cohorts\n\nDLT2 is slightly larger (more participants and interactions), but the structural properties‚Äîdensity, centralization, reciprocity, clustering, and path lengths‚Äîare all quite similar.\nMinor variations (e.g., higher reciprocity and clustering in DLT2) could reflect differences in facilitation style, cohort engagement, or participant composition.\nInterpretation: Both cohorts exhibit classic patterns for large-scale online educational discussions‚Äîa small number of central actors drive most of the interaction, the networks are sparse but efficiently connected, and genuine dialogue is present but not universal.\n\nEducational Implications\n\nFor educators and instructional designers:\nThese findings suggest that a small group of highly active facilitators or students are critical to fostering interaction. Encouraging more distributed engagement (for example, through peer response requirements or rotating leadership) may enhance network cohesion and learning opportunities.\nFor researchers:\nUnderstanding who occupies central positions and the overall structure of discussion networks can inform interventions to support isolated participants, promote reciprocity, and create more connected learning communities.\n\nSummary:\nThrough these SNA measures, we have shown how to reconstruct, visualize, and interpret the structure of large-scale online discussion networks. The approach enables identification of core communicators, understanding of participation patterns, and empirical comparison across cohorts or interventions. This ‚Äúcookbook‚Äù can be adapted to other online learning or collaborative contexts.\n&gt; Note: This analysis is based on real-world data from online professional development courses. The methods and findings can be generalized to other educational settings where social networks play a role in learning and collaboration.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Networks Data</span>"
    ]
  },
  {
    "objectID": "chapter-3.html#case-study-hashtag-common-core",
    "href": "chapter-3.html#case-study-hashtag-common-core",
    "title": "Networks Data",
    "section": "3.4 Case Study: Hashtag Common Core",
    "text": "3.4 Case Study: Hashtag Common Core\n\n3.4.1 Purpose & Case\nThe purpose of this case study is to demonstrate the application of social network analysis (SNA) in a real-world policy context: the heated national debate over the Common Core State Standards (CCSS) as it played out on Twitter. Drawing on the work of Supovitz, Daly, del Fresno, and Kolouch, the #COMMONCORE Project provides a vivid example of how social media-enabled networks shape educational discourse and policy.\nThis case focuses on: - Identifying key actors (‚Äútransmitters,‚Äù ‚Äútransceivers,‚Äù and ‚Äútranscenders‚Äù) and measuring their influence, - Detecting subgroups/factions within the conversation, - Exploring how sentiment about the Common Core varies across network positions, - Demonstrating network wrangling, visualization, and analysis using real tweet data.\n\nData Source\nData was collected from Twitter‚Äôs public API using keywords/hashtags related to the Common Core (e.g., #commoncore, ccss, stopcommoncore). The dataset includes user names, tweets, mentions, retweets, and relevant timestamps from a sample week. Only public tweets are included, and user privacy is respected.\n\n\n\n3.4.2 Sample Research Questions\n\nRQ1: Who are the ‚Äútransmitters,‚Äù ‚Äútransceivers,‚Äù and ‚Äútranscenders‚Äù in the Common Core Twitter network?\nRQ2: What subgroups or factions exist within the network, and how are they structured?\nRQ3: How does sentiment about the Common Core vary across actors and subgroups?\nRQ4: What other patterns of communication (e.g., centrality, clique formation, isolates) characterize this network?\n\n\n\n3.4.3 Analysis\n\nStep 1: Load Required Packages\n\nlibrary(tidyverse) \nlibrary(tidygraph) \nlibrary(ggraph) \nlibrary(skimr) \nlibrary(igraph) \nlibrary(tidytext) \nlibrary(vader)\n\n\n\nStep 2: Data Import and Wrangling\n\n# Import tweet data (edgelist format: sender, receiver, timestamp, text)\nccss_tweets &lt;- read_csv(\"data/ccss-tweets.csv\")\n\n# Prepare the edgelist (extract sender, mentioned users, and tweet text)\nties_1 &lt;- ccss_tweets %&gt;%\n  relocate(sender = screen_name, target = mentions_screen_name) %&gt;%\n  select(sender, target, created_at, text)\n\n# Unnest receiver to handle multiple mentions per tweet\nties_2 &lt;- ties_1 %&gt;%\n  unnest_tokens(input = target,\n                output = receiver,\n                to_lower = FALSE) %&gt;%\n  relocate(sender, receiver)\n\n# Remove tweets without mentions to focus on direct connections\nties &lt;- ties_2 %&gt;%\n  drop_na(receiver)\n\n# Save for reproducibility\nwrite_csv(ties, \"data/ccss-edgelist.csv\")\n\n# Build nodelist\nactors_1 &lt;- ties %&gt;%\n  select(sender, receiver) %&gt;%\n  pivot_longer(cols = c(sender,receiver))\n\nactors &lt;- actors_1 %&gt;%\n  select(value) %&gt;%\n  rename(actors = value) %&gt;%\n  distinct()\n\nwrite_csv(actors, \"data/ccss-nodelist.csv\")\n\n\n\nStep 3: Create Network Object\n\nccss_network &lt;- tbl_graph(edges = ties,\n                          nodes = actors,\n                          directed = TRUE)\nccss_network\n\n# A tbl_graph: 46 nodes and 42 edges\n#\n# A directed multigraph with 14 components\n#\n# Node Data: 46 √ó 1 (active)\n   actors        \n   &lt;chr&gt;         \n 1 DistanceLrnBot\n 2 k12movieguides\n 3 WEquilSchool  \n 4 JoeWEquil     \n 5 SumayLu       \n 6 fluttbot      \n 7 BodShameless  \n 8 Math          \n 9 ozsultan      \n10 sfchronicle   \n# ‚Ñπ 36 more rows\n#\n# Edge Data: 42 √ó 4\n   from    to created_at          text                                          \n  &lt;int&gt; &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;                                         \n1     1     2 2021-06-28 09:53:54 \"#Luca Movie Guide | Worksheet | Questions | ‚Ä¶\n2     3     4 2021-06-28 02:32:59 \"Why public schools should focus more on buil‚Ä¶\n3     3     3 2021-06-28 02:32:59 \"Why public schools should focus more on buil‚Ä¶\n# ‚Ñπ 39 more rows\n\n\n\n\nStep 4: Network Structure ‚Äì Components, Cliques, and Communities\n\nComponents\n\nIdentify weak and strong components (connected subgroups):\n\n\n\n    ccss_network &lt;- ccss_network |&gt;\n      activate(nodes) |&gt;\n      mutate(weak_component = group_components(type = \"weak\"),\n             strong_component = group_components(type = \"strong\"))\n    # View component sizes\n    ccss_network |&gt;\n      as_tibble() |&gt;\n      group_by(weak_component) |&gt;\n      summarise(size = n()) |&gt;\n      arrange(desc(size))\n\n# A tibble: 14 √ó 2\n   weak_component  size\n            &lt;int&gt; &lt;int&gt;\n 1              1    14\n 2              2     6\n 3              3     4\n 4              4     3\n 5              5     3\n 6              6     2\n 7              7     2\n 8              8     2\n 9              9     2\n10             10     2\n11             11     2\n12             12     2\n13             13     1\n14             14     1\n\n\n\nCliques\n\nIdentify fully connected subgroups (if any):\n\n\n\n    clique_num(ccss_network)\n\n[1] 4\n\n    cliques(ccss_network, min = 3)\n\n[[1]]\n+ 3/46 vertices, from 86b4f79:\n[1] 4 5 6\n\n[[2]]\n+ 3/46 vertices, from 86b4f79:\n[1] 39 40 41\n\n[[3]]\n+ 3/46 vertices, from 86b4f79:\n[1] 3 4 6\n\n[[4]]\n+ 4/46 vertices, from 86b4f79:\n[1] 3 4 5 6\n\n[[5]]\n+ 3/46 vertices, from 86b4f79:\n[1] 3 4 5\n\n[[6]]\n+ 3/46 vertices, from 86b4f79:\n[1] 3 5 6\n\n\n\nCommunities\n\nDetect densely connected communities using edge betweenness:\n\n\n\n    ccss_network &lt;- ccss_network |&gt;\n      morph(to_undirected) |&gt;\n      activate(nodes) |&gt;\n      mutate(sub_group = group_edge_betweenness()) |&gt;\n      unmorph()\n    ccss_network |&gt;\n      as_tibble() |&gt;\n      group_by(sub_group) |&gt;\n      summarise(size = n()) |&gt;\n      arrange(desc(size))\n\n# A tibble: 16 √ó 2\n   sub_group  size\n       &lt;int&gt; &lt;int&gt;\n 1         1    10\n 2         2     6\n 3         3     4\n 4         4     3\n 5         5     3\n 6         6     2\n 7         7     2\n 8         8     2\n 9         9     2\n10        10     2\n11        11     2\n12        12     2\n13        13     2\n14        14     2\n15        15     1\n16        16     1\n\n\n\n\nStep 5: Egocentric Analysis ‚Äì Centrality & Key Actors\n\nccss_network &lt;- ccss_network |&gt;\n  activate(nodes) |&gt;\n  mutate(\n    size = local_size(),\n    in_degree = centrality_degree(mode = \"in\"),\n    out_degree = centrality_degree(mode = \"out\"),\n    closeness = centrality_closeness(),\n    betweenness = centrality_betweenness()\n  )\n\n# Identify top actors by out_degree (transmitters), in_degree (transceivers), and both (transcenders)\ntop_transmitters &lt;- ccss_network %&gt;% as_tibble() %&gt;% arrange(desc(out_degree)) %&gt;% head(5)\ntop_transceivers &lt;- ccss_network %&gt;% as_tibble() %&gt;% arrange(desc(in_degree)) %&gt;% head(5)\ntop_transcenders &lt;- ccss_network %&gt;% as_tibble() %&gt;%\n  filter(out_degree &gt; quantile(out_degree, 0.9) & in_degree &gt; quantile(in_degree, 0.9))\n\n\n\nStep 6: Visualize the Network\n\nggraph(ccss_network, layout = \"fr\") +\n  geom_node_point(aes(size = out_degree, color = out_degree)) +\n  geom_edge_link(alpha = .2) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\n\n\n\nStep 7: Sentiment Analysis (Optional)\nIf you want to analyze sentiment as in the original #COMMONCORE study:\n\nlibrary(vader)\nvader_ccss &lt;- vader_df(ccss_tweets$text)\n mean(vader_ccss$compound)\n\n[1] 0.08668182\n\n vader_ccss_summary &lt;- vader_ccss %&gt;%\n   mutate(sentiment = case_when(\n     compound &gt;= 0.05 ~ \"positive\",\n     compound &lt;= -0.05 ~ \"negative\",\n     TRUE ~ \"neutral\"\n   )) %&gt;%\n   count(sentiment)\n\n\n\n\n3.4.4 Results and Discussion\n\nRQ1: Who are the ‚Äútransmitters,‚Äù ‚Äútransceivers,‚Äù and ‚Äútranscenders‚Äù in the Common Core Twitter network?\n\nTransmitters (high out-degree):\nThe user SumayLu stands out as the top transmitter, initiating 8 outgoing ties (mentions/retweets), followed by DouglasHolt... (5), WEquilSchool (3), fluttbot (3), and JoeWEquil (2). These users are the most active in broadcasting or mentioning others within the network.\nTransceivers (high in-degree):\nThe most-mentioned users are WEquilSchool and SumayLu (in-degree = 3), JoeWEquil (2), Tech4Learni... (2), and LASER_Insti... (2). These individuals receive the most attention from other actors‚Äîpotential focal points in conversations.\nTranscenders (high in-degree and out-degree):\nOnly two users‚ÄîWEquilSchool (in-degree = 3, out-degree = 3) and SumayLu (in-degree = 3, out-degree = 8)‚Äîsimultaneously act as hubs for both sending and receiving communication. These ‚Äúbridging‚Äù actors may serve as key facilitators or connectors in the discourse.\n\n\n\nRQ2: What subgroups or factions exist in the network?\n\nComponent analysis shows a fragmented network:\n\nThere are 14 weakly connected components, the largest containing 14 users, and several small groups or dyads (many with just 2‚Äì3 members).\nThis fragmentation suggests limited overall cohesion, with multiple parallel or isolated conversations occurring.\n\nClique analysis reveals:\n\nFour cliques (fully connected subgroups) of size 3 or 4‚Äîe.g., one 4-person clique involving nodes 3, 4, 5, and 6, and several overlapping 3-person cliques. This indicates pockets of tight-knit interaction, but such groups are rare relative to the size of the network.\n\nCommunity detection using edge betweenness identifies 16 subgroups, generally aligning with the component structure. The largest subgroup has 10 members, with most others much smaller.\n\n\n\nRQ3: What is the overall sentiment in the network?\n\nVADER sentiment analysis of tweet content yields:\n\nAn average sentiment score (compound) of 0.09 (slightly positive), indicating that, despite the policy controversy, the sampled tweets were, on balance, more positive than negative.\nWhen tweets are classified into categories:\n\nA mix of positive, neutral, and negative tweets is observed, with positive tweets slightly outnumbering negatives.\n\nThis suggests the debate, at least in this time slice, included advocacy and constructive dialogue, not only criticism or negativity.\n\n\n\n\nRQ4: What other patterns of communication (e.g., centrality, clique formation, isolates) characterize this network?\n\nCentrality Patterns:\nThe network displays a classic ‚Äústar‚Äù structure in its largest component. Two users, SumayLu and WEquilSchool, stand out with high out-degree and in-degree centrality, respectively. Most other users have very low degree values (often 0 or 1), meaning they are peripheral, engaging in few interactions.\n\nTransmitters (high out-degree): e.g., SumayLu (8 outgoing ties), DouglasHolt... (5).\nTransceivers (high in-degree): e.g., WEquilSchool, SumayLu (both in-degree = 3).\nTranscenders (both high in- and out-degree): rare‚Äîonly WEquilSchool and SumayLu meet this criterion in this sample.\n\nClique Formation:\nClique analysis revealed 4 cliques (fully connected subgroups) of size 3 or more, with one larger clique (size 4) and several overlapping smaller cliques. However, cliques are rare and limited in size‚Äîmost communication occurs outside of dense subgroups.\nIsolates and Components:\nThe network has 14 weak components‚Äîmany of them tiny. Several users are isolates or part of isolated dyads and triads, meaning they are disconnected from the main conversation or only loosely connected. This points to a lack of broad, network-wide cohesion.\nCommunity Structure:\nEdge betweenness community detection found 16 subgroups, typically matching up with the component structure: most subgroups are very small (2‚Äì3 nodes), while the largest subgroup consists of 10 users.\nSummary:\nCommunication in this network is characterized by:\n\nStrong centralization around a small number of users (hubs);\nSparse and fragmented structure with many small, disconnected components;\nLimited clique formation‚Äîpockets of tightly connected users exist but are rare;\nNumerous isolates‚Äîusers who are only weakly or not at all connected to the core discussion.\n\n\n\n\nDiscussion\nThis analysis of the Common Core Twitter conversation reveals a sparse and fragmented network structure. The debate is distributed across many small subgroups, with only one moderately sized component (14 members). Within this landscape:\n\nKey actors such as SumayLu and WEquilSchool serve as both broadcasters and focal points of attention (‚Äútranscenders‚Äù), but most users are peripheral, interacting minimally.\nCliques and communities are few and small, underscoring the lack of broad cohesion. Most interactions happen within micro-groups rather than across the entire network.\nSentiment is, perhaps surprisingly, slightly positive on average. This may reflect the presence of advocacy groups, promotional messaging, or simply a lack of highly negative engagement during the observed period.\n\nImplications:\nThe findings illustrate classic social network phenomena in online policy debate: - Most users are only lightly involved, and only a select few drive discussion or receive significant attention. - Communication is siloed, with many small isolated groups and minimal bridging between them. - Sentiment analysis offers nuance: while public debates may be assumed to be contentious, the prevailing tone can still be balanced or even positive in certain time slices.\nFor researchers and practitioners, this means that: - Identifying and engaging ‚Äútranscenders‚Äù is essential for bridging subgroups and spreading information. - Interventions or outreach should consider the network‚Äôs fragmentation‚Äîbroader influence may require engaging multiple small groups individually rather than targeting a single ‚Äúcore.‚Äù - Combining SNA with text/sentiment analysis gives a fuller picture: not just who is talking, but how and with what tone.\nFuture analysis could track changes in sentiment and connectivity over time, or compare subgroups for differences in message tone and network position.\nReferences\n\nSupovitz, J., Daly, A.J., del Fresno, M., & Kolouch, C. (2017).¬†#commoncore Project. Retrieved from http://www.hashtagcommoncore.com\nCarolan, B.V. (2014). Social Network Analysis and Education: Theory, Methods & Applications. Sage.\nSilge, J., & Robinson, D. (2017). Text Mining with R: A Tidy Approach. O‚ÄôReilly.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Networks Data</span>"
    ]
  },
  {
    "objectID": "chapter-4.html",
    "href": "chapter-4.html",
    "title": "Numeric Data",
    "section": "",
    "text": "4.1 Overview\nAbstract: This section reviews how to access data that is primarily numeric/quantitative in nature, but from a different source and of a different nature than the data typically used by social scientists. Example data sets include international or national large-scale assessments (e.g., PISA, NAEPÔºåIPEDS) and data from digital technologies (e.g., log-trace data from Open University Learning Analytics Dataset (OULAD)).\nIn social science research, data is traditionally sourced from small-scale surveys, experiments, or qualitative studies. However, the rise of big data offers researchers opportunities to explore numeric and quantitative datasets of unprecedented scale and variety. This chapter discusses how to access and analyze large-scale datasets like international assessments (e.g., PISA, NAEP) and digital log-trace data (e.g., Open University Learning Analytics Dataset (OULAD)). These secondary data sources enable novel research questions and methods, particularly when paired with machine learning and statistical modeling approaches.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Numeric Data</span>"
    ]
  },
  {
    "objectID": "chapter-4.html#accessing-big-data-broadening-the-horizon",
    "href": "chapter-4.html#accessing-big-data-broadening-the-horizon",
    "title": "Numeric Data",
    "section": "4.2 Accessing Big data (Broadening the Horizon)",
    "text": "4.2 Accessing Big data (Broadening the Horizon)\n\n4.2.1 Big Data\n\nAccessing PISA Data\nThe Programme for International Student Assessment (PISA) is a widely used dataset for large-scale educational research. It assesses 15-year-old students‚Äô knowledge and skills in reading, mathematics, and science across multiple countries. Researchers can access PISA data through various methods:\n\n1. Direct Download from the Official Website\nThe OECD provides direct access to PISA data files via its official website. Researchers can download data for specific years and cycles. Data files are typically provided in .csv or .sav (SPSS) formats, along with detailed documentation.\n\nSteps to Access PISA Data from the OECD Website:\n\nVisit the OECD PISA website.\nNavigate to the ‚ÄúData‚Äù section.\nSelect the desired assessment year (e.g., 2022).\nDownload the data and accompanying codebooks.\n\n\n\n\n2. Using the OECD R Package\nThe OECD R package provides a direct interface to download and explore datasets published by the OECD, including PISA.\n\nSteps to Use the OECD Package:\n\nInstall and load the OECD package.\nUse the getOECD() function to fetch PISA data.\n\n\n\n# Install and load the OECD package\ninstall.packages(\"OECD\")\nlibrary(OECD)\n\n# Fetch PISA data for the 2018 cycle\npisa_data &lt;- getOECD(\"pisa\", years = \"2022\")\n\n# Display a summary of the data\nsummary(pisa_data)\n\n\n\n3. Using the Edsurvey R Package\nThe Edsurvey package is designed specifically for analyzing large-scale assessment data, including PISA. It allows for complex statistical modeling and supports handling weights and replicate weights used in PISA.\n\nSteps to Use the Edsurvey Package:\n\nInstall and load the Edsurvey package.\nDownload the PISA data from the OECD website and provide the path to the .sav files.\nLoad the data into R using readPISA().\n\n\n\n# Install and load the Edsurvey package\ninstall.packages(\"Edsurvey\")\nlibrary(Edsurvey)\n\n# Read PISA data from a local file\npisa_data &lt;- readPISA(\"path/to/PISA2022Student.sav\")\n\n# Display the structure of the dataset\nstr(pisa_data)\n\n\n\nComparison of Methods\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nDisadvantages\n\n\n\n\nDirect Download\nFull access to all raw data and documentation.\nRequires manual processing and cleaning.\n\n\nOECD Package\nEasy to use for downloading specific datasets.\nLimited to OECD-published formats.\n\n\nEdsurvey Package\nSupports advanced statistical analysis and weights.\nRequires additional setup and dependencies.\n\n\n\n\n\n\nAccessing IPEDS Data\nThe Integrated Postsecondary Education Data System (IPEDS) is a comprehensive source of data on U.S. colleges, universities, and technical and vocational institutions. It provides data on enrollments, completions, graduation rates, faculty, finances, and more. Researchers and policymakers widely use IPEDS data to analyze trends in higher education.\nThere are several ways to access IPEDS data, depending on the user‚Äôs needs and technical proficiency.\n\n1. Direct Download from the NCES Website\nThe most straightforward way to access IPEDS data is by downloading it directly from the National Center for Education Statistics (NCES) website.\n\n\nSteps to Access IPEDS Data:\n\nVisit the IPEDS Data Center.\nClick on ‚ÄúUse the Data‚Äù and navigate to the ‚ÄúDownload IPEDS Data Files‚Äù section.\nSelect the desired data year and survey component (e.g., Fall Enrollment, Graduation Rates).\nDownload the data files, typically provided in .csv or .xls format, along with accompanying codebooks.\n\n\n\n2. Using the ipeds R Package\nThe ipeds R package simplifies downloading and analyzing IPEDS data directly from R by connecting to the NCES data repository.\n\n\nSteps to Use the ipeds Package:\n\nInstall and load the ipeds package.\nUse the download_ipeds() function to fetch data for specific survey components and years.\n\n\n# Install and load the ipeds package\ninstall.packages(\"ipeds\")\nlibrary(ipeds)\n\n# Download IPEDS data for completions in 2021\nipeds_data &lt;- download_ipeds(\"C\", year = 2021)\n\n# View the structure of the downloaded data\nstr(ipeds_data)\n\n\n\n3. Using the tidycensus R Package\nThe tidycensus package, while primarily designed for Census data, can access specific IPEDS data linked to educational institutions.\n\n\nSteps to Use the tidycensus Package:\n\nInstall and load the tidycensus package.\nSet up a Census API key to access the data.\nQuery IPEDS data for specific institution-level information.\n\n\n# Install and load the tidycensus package\ninstall.packages(\"tidycensus\")\nlibrary(tidycensus)\n\n# Set Census API key (replace with your actual key)\ncensus_api_key(\"your_census_api_key\")\n\n# Fetch IPEDS-related data (e.g., institution information)\nipeds_institutions &lt;- get_acs(\n  geography = \"place\",\n  variables = \"B14002_003\",\n  year = 2021,\n  survey = \"acs5\"\n)\n\n# View the first few rows\nhead(ipeds_institutions)\n\n\n\n4. Using Online Tools\nIPEDS provides several online tools for querying and visualizing data without requiring programming skills.\n\n\nCommon Tools:\n\nIPEDS Data Explorer: Enables users to query and export customized datasets.\nTrend Generator: Allows users to visualize trends in key metrics over time.\nIPEDS Use the Data: Simplified tool for accessing pre-compiled datasets.\n\n\n\nSteps to Use the IPEDS Data Explorer:\n\nVisit the IPEDS Data Explorer.\nSelect variables of interest, such as institution type, enrollment size, or location.\nFilter results by years, institution categories, or other criteria.\nExport the results as a .csv or .xlsx file.\n\n\n\nComparison of Methods\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nDisadvantages\n\n\n\n\nDirect Download\nFull access to raw data and documentation.\nRequires manual data preparation and cleaning.\n\n\nipeds Package\nAutomated access to specific components.\nLimited flexibility for customized queries.\n\n\ntidycensus Package\nAllows integration with Census and ACS data.\nRequires API setup and advanced R skills.\n\n\nOnline Tools\nUser-friendly and suitable for non-coders.\nLimited to predefined queries and exports.\n\n\n\n\n\n\nAccessing Open University Learning Analytics Dataset (OULAD)\nThe Open University Learning Analytics Dataset (OULAD) is a publicly available dataset designed to support research in educational data mining and learning analytics. It includes student demographics, module information, interactions with the virtual learning environment (VLE), and assessment scores.\n\n\nSteps to Access OULAD Data\n\nVisit the OULAD Repository**\nThe dataset is hosted on the Open University‚Äôs Analytics Project. To access the data: 1. Navigate to the website. 2. Download the dataset as a .zip file. 3. Extract the .zip file to a local directory.\nThe dataset contains multiple CSV files: - studentInfo.csv: Student demographics and performance data. - studentVle.csv: Interactions with the VLE. - vle.csv: Details of learning resources. - studentAssessment.csv: Assessment scores.\n\n\nLoading OULAD Data in R\nOnce the data is downloaded and extracted, follow these steps to load and access it in R:\n\n\nStep 1: Install Required Packages\n\n# Install necessary packages\ninstall.packages(c(\"readr\", \"dplyr\"))\n\n\n\nStep 2: Load Data\nUse the readr package to read the CSV files into R.\n\n# Load required libraries\nlibrary(readr)\n\n# Define the path to the OULAD data\ndata_path &lt;- \"path/to/OULAD/\"\n\n# Load individual CSV files\nstudent_info &lt;- read_csv(file.path(data_path, \"studentInfo.csv\"))\nstudent_vle &lt;- read_csv(file.path(data_path, \"studentVle.csv\"))\nvle &lt;- read_csv(file.path(data_path, \"vle.csv\"))\nstudent_assessment &lt;- read_csv(file.path(data_path, \"studentAssessment.csv\"))\n\n\n\nStep 3: Preview the Data\nInspect the structure and contents of the datasets.\n\n# View the first few rows of student info\nhead(student_info)\n\n# Check the structure of the student VLE data\nstr(student_vle)\n\n\n\n\n\n4.2.2 Learning Analytics\n\nWhat is Learning Analytics?\nLearning Analytics (LA) refers to the measurement, collection, analysis, and reporting of data about learners and their contexts. The primary goal of LA is to understand and improve learning processes by identifying patterns, predicting outcomes, and providing actionable insights to educators, institutions, and learners.\nKey features of LA include: - Data Collection: Gathering information from digital platforms such as learning management systems (LMS) or external assessments. - Analysis: Using machine learning, statistical methods, or visualization tools to reveal trends and patterns. - Applications: Supporting personalized learning, enhancing institutional decision-making, and improving curriculum design.\n\n\nApplications of Learning Analytics in Big Data\nLearning analytics can be applied to large-scale educational datasets like PISA, IPEDS, and OULAD to uncover trends, predict outcomes, and guide interventions.\n\n1. PISA Data and Learning Analytics\n\nWhat it offers: Insights into international student performance in reading, math, and science, combined with contextual variables (e.g., socio-economic status).\nLA Applications:\n\nIdentifying key factors influencing performance across countries.\nPredicting the impact of ICT use on student achievement.\nSegmenting students into performance clusters for targeted interventions.\n\n\n\n\n2. IPEDS Data and Learning Analytics\n\nWhat it offers: U.S. institutional-level data on enrollment, graduation rates, tuition, and financial aid.\nLA Applications:\n\nAnalyzing trends in student demographics across institutions.\nPredicting enrollment patterns based on historical data.\nBenchmarking institutions to inform policymaking and funding decisions.\n\n\n\n\n3. OULAD and Learning Analytics\n\nWhat it offers: Rich data on student engagement with virtual learning environments (VLE), assessment scores, and demographic information.\nLA Applications:\n\nTracking student interactions with learning resources to predict course completion.\nModeling the relationship between VLE usage and final grades.\nDetecting early warning signs for at-risk students based on engagement metrics.\n\n\n\n\n\nWhy Learning Analytics Matters\nThe integration of Learning Analytics with big data enables researchers and practitioners to: - Personalize Learning: Tailor educational experiences to meet individual needs. - Improve Retention: Identify at-risk learners and implement timely interventions. - Enhance Decision-Making: Provide evidence-based recommendations for curriculum and policy adjustments.\nBy leveraging datasets like PISA, IPEDS, and OULAD, learning analytics can help bridge the gap between raw data and actionable insights, fostering a more equitable and effective educational landscape.\n\n\nSupervised Learning in Learning Analytics\nMachine Learning, particularly Supervised Learning, has become a cornerstone of Learning Analytics. Supervised learning models are trained on labeled datasets, where input features are mapped to known outcomes, enabling the prediction of new, unseen data.\n\nKey Concepts in Supervised Learning\n\nDefinition\nSupervised Learning is a subset of Machine Learning focused on learning a mapping between input variables (features) and output variables (labels or outcomes). Models trained on labeled data can predict outcomes for new data points.\nCommon Algorithms\n\nLinear Regression\nLogistic Regression\nDecision Trees and Random Forests\nNeural Networks\n\nApplications in Education\nSupervised learning is particularly effective in Learning Analytics for predicting:\n\nStudent performance\nDropout risks\nEnrollment trends\nCourse completion rates\n\n\n\n\n\nApplications of Supervised Learning with Big Data\n\n1. PISA Data and Supervised Learning\n\nGoal: Use demographic and contextual features to predict student performance in mathematics, reading, or science.\nExample: Train a linear regression model to identify the relationship between socioeconomic status and test scores.\n\n\n\n2. IPEDS Data and Supervised Learning\n\nGoal: Develop models to predict institutional enrollment rates based on financial aid, demographics, and program offerings.\nExample: Use logistic regression to forecast whether a student is likely to enroll based on financial aid eligibility.\n\n\n\n3. OULAD Data and Supervised Learning\n\nGoal: Predict student outcomes (e.g., pass/fail) based on engagement metrics like forum participation and assignment submissions.\nExample: Train a random forest model to classify students as ‚Äúat-risk‚Äù or ‚Äúnot at-risk‚Äù based on weekly interaction data.\n\n\n\n\nChoosing the Right Supervised Learning Approach\nWhen applying supervised learning in Learning Analytics: 1. Define the Goal: Clearly articulate the outcome you want to predict (e.g., performance, enrollment, or engagement). 2. Select an Algorithm: Choose an appropriate model based on the data and prediction task. - For continuous outcomes, use regression models. - For categorical outcomes, use classification models like logistic regression or random forests. 3. Feature Engineering: Select and preprocess relevant features (e.g., attendance, demographics, assignment scores) to improve model accuracy. 4. Evaluate Model Performance: Use metrics such as accuracy, precision, recall, or R-squared to assess model effectiveness.\nIntegrating supervised learning techniques into Learning Analytics, researchers and practitioners can leverage big data to make data-driven predictions and decisions, ultimately enhancing educational outcomes.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Numeric Data</span>"
    ]
  },
  {
    "objectID": "chapter-4.html#logistic-regression-ml",
    "href": "chapter-4.html#logistic-regression-ml",
    "title": "Numeric Data",
    "section": "4.3 Logistic Regression ML",
    "text": "4.3 Logistic Regression ML\n\n4.3.1 Purpose + CASE\n\nPurpose\nLogistic regression is a supervised learning technique widely used for binary classification tasks. It models the probability of an event occurring (e.g., success vs.¬†failure) based on a set of predictor variables. Logistic regression is particularly effective in educational research for predicting outcomes such as retention, enrollment, or graduation rates.\n\n\nCASE: Predicting Graduation Rates\nThis case study is based on IPEDS data and inspired by Zong and Davis (2022). We predict graduation rates as a binary outcome (good_grad_rate) using institutional features such as total enrollment, admission rate, tuition fees, and average instructional staff salary.\n\n\n\n4.3.2 Sample Research Questions (RQs)\n\nRQ A: What institutional factors are associated with high graduation rates in U.S. four-year universities?\nRQ B: How accurately can we predict high graduation rates using institutional features with supervised machine learning?\n\n\n\n4.3.3 Analysis\n\nLoading Required Packages\nWe load necessary R packages for data wrangling, cleaning, and modeling.\n\n# Load necessary libraries for data cleaning, wrangling, and modeling\nlibrary(tidyverse)  # For data manipulation and visualization\nlibrary(tidymodels) # For machine learning workflows\nlibrary(janitor)    # For cleaning variable names\n\n\n\nLoading and Cleaning Data\nWe read the IPEDS dataset and clean column names for easier handling.\n\n# Read in IPEDS data from CSV file\nipeds &lt;- read_csv(\"data/ipeds-all-title-9-2022-data.csv\")\n\n# Clean column names for consistency and usability\nipeds &lt;- janitor::clean_names(ipeds)\n\n\n\nData Wrangling\nSelect relevant variables, filter the dataset, and create the dependent variable good_grad_rate.\n\n# Select and rename key variables; filter relevant institutions\nipeds &lt;- ipeds %&gt;%\n  select(\n    name = institution_name,                  # Institution name\n    total_enroll = drvef2022_total_enrollment, # Total enrollment\n    pct_admitted = drvadm2022_percent_admitted_total, # Admission percentage\n    tuition_fees = drvic2022_tuition_and_fees_2021_22, # Tuition fees\n    grad_rate = drvgr2022_graduation_rate_total_cohort, # Graduation rate\n    percent_fin_aid = sfa2122_percent_of_full_time_first_time_undergraduates_awarded_any_financial_aid, # Financial aid\n    avg_salary = drvhr2022_average_salary_equated_to_9_months_of_full_time_instructional_staff_all_ranks # Staff salary\n  ) %&gt;%\n  filter(!is.na(grad_rate)) %&gt;% # Remove rows with missing graduation rates\n  mutate(\n    # Create binary dependent variable for high graduation rates\n    good_grad_rate = if_else(grad_rate &gt; 62, 1, 0),\n    good_grad_rate = as.factor(good_grad_rate) # Convert to factor\n  )\n\n\n\nExploratory Data Analysis (EDA)\nVisualize the distribution of the graduation rate.\n\n# Plot a histogram of graduation rates\nipeds %&gt;%\n  ggplot(aes(x = grad_rate)) +\n  geom_histogram(bins = 20, fill = \"blue\", color = \"white\") +\n  labs(\n    title = \"Distribution of Graduation Rates\",\n    x = \"Graduation Rate\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\n\n\nLogistic Regression Model\nFit a logistic regression model to predict high graduation rates.\n\n# Fit logistic regression model\nm1 &lt;- glm(\n  good_grad_rate ~ total_enroll + pct_admitted + tuition_fees + percent_fin_aid + avg_salary,\n  data = ipeds,\n  family = \"binomial\" # Specify logistic regression for binary outcome\n)\n\n# View model summary\nsummary(m1)\n\n\nCall:\nglm(formula = good_grad_rate ~ total_enroll + pct_admitted + \n    tuition_fees + percent_fin_aid + avg_salary, family = \"binomial\", \n    data = ipeds)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -8.742e-01  6.237e-01  -1.402    0.161    \ntotal_enroll     3.350e-05  7.880e-06   4.251 2.13e-05 ***\npct_admitted    -1.407e-02  3.519e-03  -3.997 6.40e-05 ***\ntuition_fees     6.952e-05  4.965e-06  14.003  &lt; 2e-16 ***\npercent_fin_aid -2.960e-02  5.652e-03  -5.237 1.64e-07 ***\navg_salary       2.996e-05  3.870e-06   7.740 9.91e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2277  on 1706  degrees of freedom\nResidual deviance: 1632  on 1701  degrees of freedom\n  (3621 observations deleted due to missingness)\nAIC: 1644\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nSupervised ML Workflow\nUse the tidymodels framework to build a machine learning model.\n\n# Define recipe for the model (preprocessing steps)\nmy_rec &lt;- recipe(good_grad_rate ~ total_enroll + pct_admitted + tuition_fees + percent_fin_aid + avg_salary, data = ipeds)\n\n# Specify logistic regression model with tidymodels\nmy_mod &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%         # Use glm engine for logistic regression\n  set_mode(\"classification\")    # Specify binary classification task\n\n# Create workflow to connect recipe and model\nmy_wf &lt;- workflow() %&gt;%\n  add_recipe(my_rec) %&gt;%\n  add_model(my_mod)\n\n# Fit the logistic regression model\nfit_model &lt;- fit(my_wf, ipeds)\n\n# Generate predictions on the dataset\npredictions &lt;- predict(fit_model, ipeds) %&gt;%\n  bind_cols(ipeds) # Combine predictions with original data\n\n# Calculate and display accuracy\nmy_accuracy &lt;- predictions %&gt;%\n  metrics(truth = good_grad_rate, estimate = .pred_class) %&gt;%\n  filter(.metric == \"accuracy\")\n\nmy_accuracy\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.800\n\n\n\n\n\n4.3.4 Results and Discussions\n\nLogistic Regression Model (RQ A)\nThe logistic regression model was fitted to predict whether a university achieves a ‚Äúgood‚Äù graduation rate (i.e., graduation rate &gt; 62%) based on several institutional features. The model output is summarized below:\n\nCoefficients & Significance:\n\ntotal_enroll: Estimate = 3.35e-05, z = 4.251, p = 2.13e-05\nInterpretation: As total enrollment increases, the probability of a high graduation rate increases.\npct_admitted: Estimate = -1.407e-02, z = -3.997, p = 6.40e-05\nInterpretation: Higher admission percentages are associated with a lower likelihood of achieving a high graduation rate.\ntuition_fees: Estimate = 6.952e-05, z = 14.003, p &lt; 2e-16\nInterpretation: Higher tuition fees are strongly associated with higher graduation rates.\npercent_fin_aid: Estimate = -2.960e-02, z = -5.237, p = 1.64e-07\nInterpretation: A higher percentage of students receiving financial aid is associated with a lower probability of a good graduation rate.\navg_salary: Estimate = 2.996e-05, z = 7.740, p = 9.91e-15\nInterpretation: Higher average salaries for instructional staff are positively associated with high graduation rates.\n\nModel Fit Statistics:\n\nNull Deviance: 2277 (on 1706 degrees of freedom)\nResidual Deviance: 1632 (on 1701 degrees of freedom)\nAIC: 1644\nNote: 3621 observations were deleted due to missing values.\n\n\nOverall, the regression model demonstrates that several institutional factors are statistically significant predictors of graduation rates. In particular, tuition fees and avg_salary have a strong positive effect, while pct_admitted and percent_fin_aid show negative associations.\n\n\nSupervised ML Workflow Results (RQ B)\nUsing the tidymodels framework, we built a logistic regression model as part of a supervised machine learning workflow. The performance metric obtained is as follows:\n\nAccuracy: 80.02%\n\nThis indicates that the machine learning model correctly classified approximately 80% of the institutions as having either a good or not good graduation rate, based on the selected predictors.\n\n\nOverall Discussion\n\nSimilarities between Approaches:\n\nBoth the traditional logistic regression and the tidymodels workflow identified key predictors that influence graduation rates, such as total enrollment, admission percentage, tuition fees, financial aid percentage, and average staff salary.\nEach approach provides valuable insights: the regression model offers detailed coefficient estimates and significance levels, while the tidymodels workflow emphasizes predictive accuracy.\n\nDifferences between Approaches:\n\nInterpretability vs.¬†Predictive Performance: The logistic regression output delivers interpretability through its coefficients and p-values, allowing us to understand the direction and magnitude of the relationships. In contrast, the supervised ML workflow focuses on achieving a robust predictive performance, evidenced by an 80% accuracy.\nHandling of Data: The traditional regression model summarizes the relationship between variables, whereas the ML workflow integrates data pre-processing, modeling, and validation into a cohesive framework.\n\n\nIn summary, our analyses indicate that institutional factors, particularly tuition fees and staff salaries, play a significant role in predicting graduation outcomes. The supervised ML approach, with an accuracy of around 80%, confirms the model‚Äôs practical utility in classifying institutions based on graduation performance. Both methods complement each other, providing a comprehensive understanding of the underlying dynamics that drive graduation rates in higher education.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Numeric Data</span>"
    ]
  },
  {
    "objectID": "chapter-4.html#random-forests-ml-on-interactions-data",
    "href": "chapter-4.html#random-forests-ml-on-interactions-data",
    "title": "Numeric Data",
    "section": "4.4 Random Forests ML on Interactions Data",
    "text": "4.4 Random Forests ML on Interactions Data\nIn this section, we explore a more sophisticated supervised learning approach‚ÄîRandom Forests‚Äîto model student interactions data from the Open University Learning Analytics Dataset (OULAD). Building on our earlier work with logistic regression and evaluation metrics, this case study examines whether a random forest model can improve predictive performance when leveraging clickstream data from the virtual learning environment (VLE).\n\n4.4.1 Purpose + CASE\n\nPurpose\nRandom Forests is an ensemble learning method that builds multiple decision trees and aggregates their results to improve prediction accuracy and control over-fitting. It is particularly well suited for complex, high-dimensional data such as student interaction (clickstream) data. This approach not only provides robust predictions but also offers insights into variable importance, helping us understand which features most influence student outcomes.\n\n\nCASE\nInspired by research on digital trace data (e.g., Rodriguez et al., 2021; Bosch, 2021), this case study uses pre-processed interactions data from OULAD. In our analysis, we focus on predicting whether a student will pass the course (a binary outcome) based on engineered features derived from clickstream data. These features include the total number of clicks (sum_clicks), summary statistics (mean and standard deviation of clicks), and linear trends over time (slope and intercept from clickstream patterns).\n\n\n\n4.4.2 Sample Research Questions\n\nRQ1: How accurately can a random forest model predict whether a student will pass a course using interactions data from OULAD?\nRQ2: Which interaction-based features (e.g., total clicks, click stream slope) are most important in predicting student outcomes?\nRQ3: How does the use of cross-validation (e.g., v-fold CV) influence the stability and generalizability of the random forest model on interactions data?\n\n\n\n4.4.3 Analysis\n\nLoading Required Packages\n\n# Load necessary libraries for data manipulation and modeling\nlibrary(tidyverse)      # Data wrangling and visualization\nlibrary(janitor)        # Cleaning variable names\nlibrary(tidymodels)     # Modeling workflow\nlibrary(ranger)         # Random forest implementation\nlibrary(vip)            # Variable importance plots\n\n\n\nLoading and Preparing the Data\nWe load the pre-filtered interactions data from OULAD along with a students-and-assessments file, then join them to create a complete dataset for modeling.\n\n# Load the interactions data (filtered for the first one-third of the semester)\ninteractions &lt;- read_csv(\"data/oulad-interactions-filtered.csv\")\n\n# Load the students and assessments data\nstudents_and_assessments &lt;- read_csv(\"data/oulad-students-and-assessments.csv\")\n\n# Create cut-off dates based on assessments data (using first quantile as intervention point)\nassessments &lt;- read_csv(\"data/oulad-assessments.csv\")\n\n# Create cut-off dates based on assessments data using the correct date column 'date_submitted'\ncode_module_dates &lt;- assessments %&gt;% \n    group_by(code_module, code_presentation) %&gt;% \n    summarize(quantile_cutoff_date = quantile(date_submitted, probs = 0.25, na.rm = TRUE), .groups = 'drop')\n\n# Join interactions with the cutoff dates and filter\ninteractions_joined &lt;- interactions %&gt;% \n    left_join(code_module_dates, by = c(\"code_module\", \"code_presentation\"))\n\n\ninteractions_joined &lt;- interactions_joined %&gt;% \n    select(-quantile_cutoff_date.x) %&gt;% \n    rename(quantile_cutoff_date = quantile_cutoff_date.y)\n\n\n\n# Filter interactions to include only those before the cutoff date\ninteractions_filtered &lt;- interactions_joined %&gt;% \n    filter(date &lt; quantile_cutoff_date)\n\n# Summarize interactions: total clicks, mean and standard deviation\ninteractions_summarized &lt;- interactions_filtered %&gt;% \n    group_by(id_student, code_module, code_presentation) %&gt;% \n    summarize(\n      sum_clicks = sum(sum_click),\n      sd_clicks = sd(sum_click), \n      mean_clicks = mean(sum_click)\n    )\n\n# (Optional) Further feature engineering: derive linear slopes from clickstream\nfit_model &lt;- function(data) {\n    tryCatch(\n        { \n            model &lt;- lm(sum_click ~ date, data = data)\n            tidy(model)\n        },\n        error = function(e) { tibble(term = NA, estimate = NA, std.error = NA, statistic = NA, p.value = NA) },\n        warning = function(w) { tibble(term = NA, estimate = NA, std.error = NA, statistic = NA, p.value = NA) }\n    )\n}\n\ninteractions_slopes &lt;- interactions_filtered %&gt;%\n    group_by(id_student, code_module, code_presentation) %&gt;%\n    nest() %&gt;%\n    mutate(model = map(data, fit_model)) %&gt;%\n    unnest(model) %&gt;%\n    ungroup() %&gt;%\n    select(code_module, code_presentation, id_student, term, estimate) %&gt;%\n    filter(!is.na(term)) %&gt;%\n    pivot_wider(names_from = term, values_from = estimate) %&gt;%\n    mutate_if(is.numeric, round, 4) %&gt;%\n    rename(intercept = `(Intercept)`, slope = date)\n\n# Join summarized clicks and slopes features\ninteractions_features &lt;- left_join(interactions_summarized, interactions_slopes, by = c(\"id_student\", \"code_module\", \"code_presentation\"))\n\n# Finally, join with students_and_assessments to get the outcome variable\nstudents_assessments_and_interactions &lt;- left_join(students_and_assessments, interactions_features, by = c(\"id_student\", \"code_module\", \"code_presentation\"))\n\n# Ensure outcome variable 'pass' is a factor\nstudents_assessments_and_interactions &lt;- students_assessments_and_interactions %&gt;% \n    mutate(pass = as.factor(pass))\n\n# Optional: Inspect the final dataset\nstudents_assessments_and_interactions %&gt;% \n    skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n32593\n\n\nNumber of columns\n22\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nfactor\n1\n\n\nnumeric\n13\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncode_module\n0\n1\n3\n3\n0\n7\n0\n\n\ncode_presentation\n0\n1\n5\n5\n0\n4\n0\n\n\ngender\n0\n1\n1\n1\n0\n2\n0\n\n\nregion\n0\n1\n5\n20\n0\n13\n0\n\n\nhighest_education\n0\n1\n15\n27\n0\n5\n0\n\n\nage_band\n0\n1\n4\n5\n0\n3\n0\n\n\ndisability\n0\n1\n1\n1\n0\n2\n0\n\n\nfinal_result\n0\n1\n4\n11\n0\n4\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\npass\n0\n1\nFALSE\n2\n0: 20232, 1: 12361\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nid_student\n0\n1.00\n706687.67\n549167.31\n3733.00\n508573.00\n590310.00\n644453.00\n2716795.00\n‚ñÖ‚ñá‚ñÅ‚ñÅ‚ñÅ\n\n\nimd_band\n4627\n0.86\n5.62\n2.73\n1.00\n4.00\n6.00\n8.00\n10.00\n‚ñÉ‚ñá‚ñá‚ñÜ‚ñÜ\n\n\nnum_of_prev_attempts\n0\n1.00\n0.16\n0.48\n0.00\n0.00\n0.00\n0.00\n6.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nstudied_credits\n0\n1.00\n79.76\n41.07\n30.00\n60.00\n60.00\n120.00\n655.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nmodule_presentation_length\n0\n1.00\n256.01\n13.18\n234.00\n241.00\n262.00\n268.00\n269.00\n‚ñá‚ñÅ‚ñÅ‚ñÖ‚ñá\n\n\ndate_registration\n45\n1.00\n-69.41\n49.26\n-322.00\n-100.00\n-57.00\n-29.00\n167.00\n‚ñÅ‚ñÇ‚ñá‚ñÉ‚ñÅ\n\n\ndate_unregistration\n22521\n0.31\n49.76\n82.46\n-365.00\n-2.00\n27.00\n109.00\n444.00\n‚ñÅ‚ñÅ‚ñá‚ñÇ‚ñÅ\n\n\nmean_weighted_score\n7958\n0.76\n544.70\n381.39\n0.00\n160.00\n610.00\n875.00\n1512.00\n‚ñá‚ñÉ‚ñá‚ñÖ‚ñÅ\n\n\nsum_clicks\n3495\n0.89\n474.93\n572.89\n1.00\n128.00\n295.50\n604.00\n10712.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nsd_clicks\n3753\n0.88\n4.91\n5.51\n0.00\n2.37\n3.72\n6.44\n560.24\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nmean_clicks\n3495\n0.89\n3.19\n1.30\n1.00\n2.33\n2.95\n3.82\n47.12\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nintercept\n3640\n0.89\n3.04\n4.61\n-585.59\n2.15\n2.80\n3.66\n130.83\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nslope\n4441\n0.86\n0.01\n0.22\n-12.17\n-0.01\n0.01\n0.03\n20.12\n‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\n\nCreating the Model Recipe\nWe build a recipe that includes the engineered features from interactions data along with other predictors from the students data.\n\nmy_rec2 &lt;- recipe(pass ~ disability +\n                     date_registration + \n                     gender +\n                     code_module +\n                     mean_weighted_score +\n                     sum_clicks + sd_clicks + mean_clicks + \n                     intercept + slope, \n                 data = students_assessments_and_interactions) %&gt;% \n    step_dummy(disability) %&gt;% \n    step_dummy(gender) %&gt;%  \n    step_dummy(code_module) %&gt;% \n    step_impute_knn(mean_weighted_score) %&gt;% \n    step_impute_knn(sum_clicks) %&gt;% \n    step_impute_knn(sd_clicks) %&gt;% \n    step_impute_knn(mean_clicks) %&gt;% \n    step_impute_knn(intercept) %&gt;% \n    step_impute_knn(slope) %&gt;% \n    step_impute_knn(date_registration) %&gt;% \n    step_normalize(all_numeric_predictors())\n\n\n\nSpecifying the Model and Workflow\nWe use a random forest model via the ranger engine and set up our workflow.\n\n# Specify random forest model\nmy_mod2 &lt;- rand_forest() %&gt;% \n    set_engine(\"ranger\", importance = \"impurity\") %&gt;% \n    set_mode(\"classification\")\n\n# Create workflow to bundle the recipe and model\nmy_wf2 &lt;- workflow() %&gt;% \n    add_recipe(my_rec2) %&gt;% \n    add_model(my_mod2)\n\n\n\nResampling and Model Fitting\nWe perform cross-validation (v-fold CV) to estimate model performance.\n\n# Create 4-fold cross-validation on training data\nvfcv &lt;- vfold_cv(data = students_assessments_and_interactions, v = 4, strata = pass)\n\n# Specify metrics: accuracy, sensitivity, specificity, ppv, npv, and Cohen's kappa\nclass_metrics &lt;- metric_set(accuracy, sensitivity, specificity, ppv, npv, kap)\n\n# Fit the model using resampling\nfitted_model_resamples &lt;- fit_resamples(my_wf2, resamples = vfcv, metrics = class_metrics)\n\n# Collect and display metrics\ncollect_metrics(fitted_model_resamples)\n\n# A tibble: 6 √ó 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.670     4 0.00140 Preprocessor1_Model1\n2 kap         binary     0.262     4 0.00479 Preprocessor1_Model1\n3 npv         binary     0.589     4 0.00160 Preprocessor1_Model1\n4 ppv         binary     0.702     4 0.00221 Preprocessor1_Model1\n5 sensitivity binary     0.815     4 0.00299 Preprocessor1_Model1\n6 specificity binary     0.434     4 0.00783 Preprocessor1_Model1\n\n\n\n\nFinal Model Fit and Evaluation\nFinally, we fit the model on the full training set (using last_fit) and evaluate its predictions on the test set.\n\n# Split data into training and testing sets (e.g., 33% for testing)\nset.seed(20230712)\ntrain_test_split &lt;- initial_split(students_assessments_and_interactions, prop = 0.67, strata = pass)\ndata_train &lt;- training(train_test_split)\ndata_test &lt;- testing(train_test_split)\n\n# Fit final model on the training set and evaluate on the test set\nfinal_fit &lt;- last_fit(my_wf2, train_test_split, metrics = class_metrics)\n\n# Collect and display final metrics\ncollect_metrics(final_fit)\n\n# A tibble: 6 √ó 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.665 Preprocessor1_Model1\n2 sensitivity binary         0.815 Preprocessor1_Model1\n3 specificity binary         0.419 Preprocessor1_Model1\n4 ppv         binary         0.697 Preprocessor1_Model1\n5 npv         binary         0.580 Preprocessor1_Model1\n6 kap         binary         0.247 Preprocessor1_Model1\n\n# Generate and display a confusion matrix for final predictions\ncollect_predictions(final_fit) %&gt;% \n    conf_mat(.pred_class, pass)\n\n          Truth\nPrediction    0    1\n         0 5439 1238\n         1 2370 1710\n\n# Extract the fitted model from the final workflow and plot variable importance\nfinal_fit %&gt;% \n  pluck(\".workflow\", 1) %&gt;%   # Extract the workflow object from the final fit\n  extract_fit_parsnip() %&gt;%   # Retrieve the fitted model from the workflow\n  vip(num_features = 10)      # Plot the top 10 important features\n\n\n\n\n\n\n\n# Extract the fitted model from the workflow\nfinal_model &lt;- final_fit %&gt;% \n  pluck(\".workflow\", 1) %&gt;% \n  extract_fit_parsnip()\n# Extract the variable importance values from the fitted model\nimportance_values &lt;- final_model$fit$variable.importance\n\n# Print the variable importance values\nprint(importance_values)\n\n  date_registration mean_weighted_score          sum_clicks           sd_clicks \n          652.14806           838.06052          1062.49141           778.63848 \n        mean_clicks           intercept               slope        disability_Y \n          737.79417           729.52344           739.85682            60.93367 \n           gender_M     code_module_BBB     code_module_CCC     code_module_DDD \n           79.62440            77.18477            70.72006            46.68243 \n    code_module_EEE     code_module_FFF     code_module_GGG \n           28.26595            74.31310            35.27026 \n\n\n\n\n\n4.4.4 Results and Discussions\n\nResearch Question 1 (RQ1):\nHow accurately can a random forest model predict whether a student will pass a course using interactions data from OULAD?\nResponse:\nUsing 4-fold cross-validation, our random forest model yielded an average accuracy of approximately 67.0% (mean accuracy from resamples: 0.670) with a Cohen‚Äôs Kappa of 0.261, suggesting moderate agreement beyond chance. When fitted on the entire training set and evaluated on the test set, the final model showed an accuracy of 66.5% along with: - Sensitivity: 81.5% ‚Äì indicating the model correctly identifies a high proportion of students who pass. - Specificity: 41.9% ‚Äì suggesting that the model is less effective at correctly identifying students who do not pass. - Positive Predictive Value (PPV): 69.7% - Negative Predictive Value (NPV): 58.0%\nThe confusion matrix shows: - True Negatives (TN): 5439 - False Negatives (FN): 1238 - False Positives (FP): 2370 - True Positives (TP): 1710\nOverall, these metrics indicate that while the model performs well in detecting positive outcomes (high sensitivity), its lower specificity means that it tends to misclassify a relatively higher proportion of non-passing students.\n\n\nResearch Question 2 (RQ2):\nWhich interaction-based features are most important in predicting student outcomes?\nResponse:\nThe variable importance analysis, extracted from the final random forest model using the vip() function, highlights the following key predictors (with their respective importance scores):\n\nsum_clicks: 1062.49 ‚Äì This is the most influential feature, indicating that the total number of clicks (i.e., student engagement) in the VLE is a strong predictor of student success.\nmean_weighted_score: 838.06 ‚Äì Reflecting academic performance as measured by weighted assessment scores.\nmean_clicks: 737.79, slope: 739.86, and intercept: 729.52 ‚Äì These engineered features representing the central tendency and trend of click behavior further underline the importance of digital engagement patterns.\ndate_registration: 652.15 ‚Äì The registration date also plays a significant role.\nOther categorical variables (e.g., dummy-coded disability, gender, and code_module levels) generally show lower importance scores, with values typically under 80, indicating that while they do contribute, engagement and performance metrics dominate.\n\nThese results suggest that both the intensity and the temporal trend of student interactions with the learning environment are critical in predicting whether a student will pass.\n\n\nResearch Question 3 (RQ3):\nHow does the use of cross-validation impact the stability and generalizability of the random forest model on interactions data?\nResponse:\nThe use of 4-fold cross-validation (via vfold_cv) allowed us to assess the model‚Äôs performance across multiple subsets of the data, mitigating the risk of overfitting. The resampling results are relatively consistent (with accuracy around 67%, sensitivity at 81.6%, and specificity around 43.2%), which supports the model‚Äôs robustness and generalizability. Although the final test set performance (accuracy of 66.5%) is slightly lower, the overall consistency of metrics across folds indicates that our model is stable when applied to unseen data.\n\n\nOverall Discussion\nThe random forest model built on interactions data from OULAD demonstrates decent predictive performance with an accuracy of approximately 66.5‚Äì67% and high sensitivity (around 81.5%), indicating strong capability in identifying students who will pass the course. However, the relatively low specificity (around 42%) suggests that there is room for improvement in correctly classifying students who are at risk of not passing.\nThe variable importance analysis underscores that engagement-related features‚Äîespecially sum_clicks and features capturing the trend in interactions (slope, mean_clicks)‚Äîare the most influential predictors. This insight implies that the digital footprint of student engagement in the virtual learning environment is critical for predicting academic outcomes.\nIn summary, while our model performs robustly across cross-validation folds and provides actionable insights into key predictive features, the lower specificity points to the need for further refinement. Future work might explore additional feature engineering, alternative model tuning, or combining models to better balance sensitivity and specificity, ultimately supporting timely interventions in educational settings.",
    "crumbs": [
      "Computational Methods",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Numeric Data</span>"
    ]
  },
  {
    "objectID": "section-3-intro.html",
    "href": "section-3-intro.html",
    "title": "LLMs Methods",
    "section": "",
    "text": "Overview of Major Cloud Providers\n5.1 Why Large Language Models Matter in Educational Research\nLarge Language Models (LLMs) have transformed how researchers analyze, generate, and interpret text. Unlike traditional NLP pipelines that rely on token counts and surface patterns, LLMs reason over context, semantics, and discourse structure.\nIn educational research, this means: - Summarizing and coding open-ended student reflections - Analyzing institutional policy documents - Generating scaffolds or rubrics for teaching materials - Synthesizing qualitative and quantitative findings into narratives\nLLMs thus expand the researcher‚Äôs computational toolkit‚Äîfrom statistical pattern recognition to context-aware meaning-making.\n5.2 Cloud-based LLMs: Capabilities and Setup\nCloud-based Large Language Models (LLMs) offer access to state-of-the-art generative and analytical capabilities without requiring local hardware or model management. They run on remote servers and are accessed via APIs‚Äîmaking them ideal for rapid prototyping, large-scale text processing, and exploratory analyses in educational research.\nThe landscape of LLMs evolves quickly. As of 2025, the following providers represent the most common cloud-based options for educational data analysis.",
    "crumbs": [
      "LLMs Methods"
    ]
  },
  {
    "objectID": "section-3-intro.html#local-llms-privacy-preserving-and-offline-analysis",
    "href": "section-3-intro.html#local-llms-privacy-preserving-and-offline-analysis",
    "title": "LLMs Methods",
    "section": "Local LLMs: Privacy-Preserving and Offline Analysis",
    "text": "Local LLMs: Privacy-Preserving and Offline Analysis\nWhile cloud models offer convenience, they also raise concerns around data privacy, cost, and IRB compliance.\nLocal LLMs solve these challenges by running entirely on your own computer.\n\nWhat Are Local LLMs?\nLocal LLMs are open or custom models executed on your local hardware.\nThey process text without sending it to external servers‚Äîensuring full data sovereignty.\nCommon open-source families: Llama 3, Qwen, DeepSeek, Mistral, gpt-oss.\nKey Advantages\n\nData never leaves your device\nNo API keys or internet required\nHighly customizable and often cost-free\nEnables fully offline reproducible analysis\n\n\n\nGetting Started with LM Studio\nLM Studio is a free, cross-platform desktop application for managing and running local LLMs.\nIt provides a GUI for model downloads, prompt testing, and an optional REST API for automation.\nSupported Platforms: macOS (Apple Silicon), Windows (x64/ARM64), Linux (x64)\nDocs: lmstudio.ai/docs\n\n\nInstallation Steps\n\nDownload LM Studio for your system from the official site.\nInstall and launch the application.\nDownload a model such as Llama 3, Qwen, Mistral, or DeepSeek.\n(Optional) Enable API access for scripting.\n(Optional) Attach local documents to enable offline ‚ÄúChat with Documents‚Äù (RAG mode).\n\n\n\nMain Features\n\n\n\nFeature\nDescription\n\n\n\n\nLocal LLMs\nRun models offline on your own machine\n\n\nChat Interface\nSimple prompt-based GUI\n\n\nDocument Chat (RAG)\nOffline ‚Äúchat with your PDFs‚Äù\n\n\nModel Management\nSearch, download, and switch models\n\n\nAPI Access\nOpenAI-compatible REST endpoints\n\n\nCommunity Support\nActive Discord and docs\n\n\n\n\n\n\nCalling the LM Studio API from R\nLM Studio exposes an OpenAI-compatible REST API, so R code can look almost identical to the cloud example:\nlibrary(httr)\nlibrary(jsonlite)\n\nprompt &lt;- \"Summarize the following open-ended survey responses: ...\"\n\nresponse &lt;- POST(\n  url  = \"http://localhost:1234/v1/completions\",\n  body = toJSON(list(prompt = prompt, max_tokens = 200), auto_unbox = TRUE),\n  encode = \"json\"\n)\n\ncontent(response)\n\nüîê Because the request stays within your local network, no data ever leaves your computer.",
    "crumbs": [
      "LLMs Methods"
    ]
  },
  {
    "objectID": "section-3-intro.html#cloud-vs-local-llms-choosing-the-right-tool",
    "href": "section-3-intro.html#cloud-vs-local-llms-choosing-the-right-tool",
    "title": "LLMs Methods",
    "section": "Cloud vs Local LLMs: Choosing the Right Tool",
    "text": "Cloud vs Local LLMs: Choosing the Right Tool\n\n\n\n\n\n\n\n\nCriterion\nCloud-based LLMs\nLocal LLMs\n\n\n\n\nCost\nPay-per-token or subscription\nFree (after hardware)\n\n\nPrivacy\nData sent to provider\nData stays local\n\n\nPerformance\nHighest accuracy & speed\nDepends on hardware\n\n\nMaintenance\nAutomatic updates\nManual model management\n\n\nCustomization\nLimited fine-tuning\nFully modifiable\n\n\nBest for\nLarge public datasets or prototype analysis\nSensitive or regulated data\n\n\n\n\nüß≠ Many educational researchers prototype analyses on the cloud for speed, then reproduce them locally for privacy and reproducibility.",
    "crumbs": [
      "LLMs Methods"
    ]
  },
  {
    "objectID": "section-3-intro.html#practical-setup-checklist",
    "href": "section-3-intro.html#practical-setup-checklist",
    "title": "LLMs Methods",
    "section": "Practical Setup Checklist",
    "text": "Practical Setup Checklist\nBefore running LLM-based analyses:\n\n‚úÖ Select your preferred model and platform\n‚úÖ Configure API key (cloud) or local endpoint (LM Studio)\n‚úÖ Test connectivity with a short prompt\n‚úÖ Log model name, version, and date\n‚úÖ De-identify data and store outputs securely\n\n\n‚ú® Following these steps ensures your AI-assisted research remains ethical, reproducible, and IRB-compliant.",
    "crumbs": [
      "LLMs Methods"
    ]
  },
  {
    "objectID": "section-3-intro.html#summary-1",
    "href": "section-3-intro.html#summary-1",
    "title": "LLMs Methods",
    "section": "Summary",
    "text": "Summary\nBoth cloud-based and local LLMs enable researchers to integrate generative AI into educational inquiry.\n\n\n\nUse Case\nCloud LLM\nLocal LLM\n\n\n\n\nRapid prototyping\n‚úÖ\n‚ö™\n\n\nLarge-scale text processing\n‚úÖ\n‚ö™\n\n\nSensitive student data\n‚ö™\n‚úÖ\n\n\nOffline analysis\n‚ö™\n‚úÖ\n\n\nLong-term reproducibility\n‚ö™\n‚úÖ\n\n\n\n\nIn short, cloud LLMs excel in convenience and scale,\nwhile local LLMs excel in privacy and control.\nMost projects benefit from combining both.\n\n\n\nLooking Ahead\n\nChapter 6 will demonstrate thematic and qualitative text analysis using LM Studio,\nshowing how local LLMs can perform end-to-end qualitative coding and synthesis.\nChapter 7 extends this workflow to multimodal data‚Äîimages ‚Äî illustrating how AI can connect diverse data types in educational contexts.",
    "crumbs": [
      "LLMs Methods"
    ]
  },
  {
    "objectID": "chapter-6.html",
    "href": "chapter-6.html",
    "title": "Local LLMs",
    "section": "",
    "text": "6.1 What are Local LLMs?\nOverview:\nThe use of large language models (LLMs) in data analysis is rapidly increasing across education and social science research. However, concerns about data privacy, institutional data protection policies, and strict IRB (Institutional Review Board) procedures present significant challenges when using cloud-based or proprietary AI services. To address these challenges, this chapter introduces local LLM solutions‚Äîfocusing on LM Studio‚Äîwhich allow researchers to run powerful models entirely on their own computers, ensuring data stays private and analysis remains flexible.\nLocal LLMs are large language models that run directly on your own computer, rather than in the cloud. By processing data locally, they help ensure privacy, data sovereignty, and compliance with institutional or governmental regulations. Local LLMs can be open-source (such as Llama, Qwen, DeepSeek, Mistral) and are compatible with various operating systems and hardware.\nKey advantages of local LLMs: - Data never leaves your computer - No need for external API keys or internet access to analyze sensitive data - Flexibility to use custom or open-source models - Often no usage fees",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Local LLMs</span>"
    ]
  },
  {
    "objectID": "chapter-6.html#what-can-local-llms-do",
    "href": "chapter-6.html#what-can-local-llms-do",
    "title": "Local LLMs",
    "section": "6.2 What Can Local LLMs Do?",
    "text": "6.2 What Can Local LLMs Do?\nWith the right setup, local LLMs can: - Summarize, paraphrase, and analyze text data (open-ended survey responses, interview transcripts, etc.) - Support qualitative and quantitative educational research workflows - Generate coding frameworks, extract themes, or automate report writing - Perform document-based question answering (‚Äúchat with your PDFs‚Äù) - Integrate with other research tools via REST APIs",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Local LLMs</span>"
    ]
  },
  {
    "objectID": "chapter-6.html#getting-started-with-lm-studio",
    "href": "chapter-6.html#getting-started-with-lm-studio",
    "title": "Local LLMs",
    "section": "6.3 Getting Started with LM Studio",
    "text": "6.3 Getting Started with LM Studio\nLM Studio is a free, cross-platform application that enables researchers to run, manage, and interact with local LLMs (such as Llama, DeepSeek, Qwen, Mistral, and gpt-oss) entirely on their own computers. By using LM Studio, you gain powerful, offline data analysis capabilities without sacrificing data privacy or compliance.\nKey Points: - Supported Platforms: macOS (Apple Silicon), Windows (x64/ARM64), and Linux (x64). - System Requirements: For best results, consult the System Requirements page for recommended RAM, CPU/GPU, and storage.\n\n6.3.1 Installation Steps\n\nDownload LM Studio for your operating system from the official Downloads page.\nInstall and launch the application.\nDownload your preferred LLM model (such as Llama 3, Qwen, Mistral, DeepSeek, or gpt-oss) directly from within LM Studio.\n(Optional) To use the API for scripting/automation, enable API access within LM Studio.\n(Optional) Attach documents for ‚ÄúChat with Documents‚Äù (RAG-style analysis) entirely offline.\n\nOfficial Documentation:\n- LM Studio Docs - Getting Started Guide\n\n\n6.3.2 Main Features\n\nRun local models including Llama, Qwen, DeepSeek, Mistral, gpt-oss, and more.\nSimple chat interface for prompt-based interaction.\nOffline ‚ÄúChat with Documents‚Äù for Retrieval Augmented Generation (RAG) use cases.\nSearch and download new models from Hugging Face and other model hubs within LM Studio.\nManage models, prompts, and configurations through a user-friendly GUI.\nServe local models on OpenAI-compatible REST API endpoints, usable by R, Python, or other apps.\nMCP server/client support for advanced use cases.\n\n\n\n6.3.3 API Integration\nLM Studio exposes a REST API fully compatible with the OpenAI standard. This means you can send prompts and receive completions from R, Python, or any other HTTP-capable software‚Äîenabling automation and custom research workflows.\nExample: Calling the LM Studio API from R\nLM Studio exposes a REST API compatible with the OpenAI API standard. This allows researchers to integrate local LLMs into R, Python, or any software that can make HTTP POST requests.\n\nlibrary(httr) \nlibrary(jsonlite)\n\nprompt \\&lt;- \"Summarize the following open-ended survey responses: ...\"\n\nresponse \\&lt;- POST( url = \"http://localhost:1234/v1/completions\", \n                   body = toJSON(list( prompt = prompt,\n                                       max_tokens = 200 ),\n                                 auto_unbox = TRUE),\n                   encode = \"json\" ) \ncontent(response) \n\n\n\n6.3.4 Summary Table of LM Studio Capabilities:\n\n\n\nFeature\nDescription\n\n\n\n\nLocal LLMs\nRun Llama, DeepSeek, Qwen, Mistral, etc. fully offline on your own machine\n\n\nChat Interface\nFlexible prompt-based interaction\n\n\nDocument Chat (RAG)\nOffline ‚Äúchat with your documents‚Äù\n\n\nModel Management\nDownload, organize, and switch between models\n\n\nAPI Access\nOpenAI-compatible REST endpoints for use with R, Python, scripts, apps\n\n\nMCP Integration\nConnect with and use MCP servers\n\n\nCommunity & Support\nDiscord, official docs, active development",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Local LLMs</span>"
    ]
  },
  {
    "objectID": "chapter-6.html#case-study-comparing-local-llm-analysis-to-traditional-nlp-on-university-ai-policy-texts",
    "href": "chapter-6.html#case-study-comparing-local-llm-analysis-to-traditional-nlp-on-university-ai-policy-texts",
    "title": "Local LLMs",
    "section": "6.4 Case Study: Comparing Local LLM Analysis to Traditional NLP on University AI Policy Texts",
    "text": "6.4 Case Study: Comparing Local LLM Analysis to Traditional NLP on University AI Policy Texts\n\n6.4.1 Research Question\nCan a local LLM running via LM Studio reliably identify key themes in university AI policy statements‚Äîusing the same dataset analyzed in Section 2‚Äîso that we can compare its results against traditional NLP methods and human coding?\n\n\n6.4.2 Data Context\nWe reuse the AI policy statements dataset from Section 2, now simplified for privacy. The table has one column only:\n\nStance (character): policy text (no institution names)\n\nA typical structure (as seen in Section 2):\nWe will extract the same raw text field (Stance) so results are directly comparable to Section 2.\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(readr)\n\n# If 'university_policies' already exists (from Section 2), use it directly.\n# Otherwise, safely fall back to reading the same CSV used in Section 2.\nif (!exists(\"university_policies\")) {\n  university_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\", show_col_types = FALSE)\n}\n\nstopifnot(\"Stance\" %in% names(university_policies))\n\npolicy_texts &lt;- university_policies$Stance %&gt;%\n  as.character() %&gt;%\n  stringr::str_squish() %&gt;%\n  na.omit()\n\nlength(policy_texts)\n\n[1] 99\n\nhead(policy_texts, 3)\n\n[1] \"If the text generated by ChatGPT is used as a starting point for original research or writing, then it can be a useful tool for generating ideas and suggestions. In this case, it is important to properly cite and attribute the source of the information. ... However, if the text generated by ChatGPT is simply copied and pasted into a paper or report without any modifications, it can be considered plagiarism since the text isn‚Äôt original.\"                                                                                                                                                                                                                                                                   \n[2] \"Has ASU considered a ban on AI tools like other institutions such as NYU? No. ASU faculty and administrators are focused on the positive potential of Generative AI while also thinking through concerns about ethics, academic integrity, and privacy. ... What is being done to ensure academic integrity? The Provost‚Äôs Office is currently reviewing ASU‚Äôs academic integrity policy through the lens of what kind of content can be produced through generative AI and what kind of learning behaviors and outcomes are expected of students. ... Will I get accused of cheating if I use AI tools? Before using AI tools in your coursework, confer with your instructor about their class policy for using AI tools.\"\n[3] \"The following sample statements should be taken as starting points to craft your own policy. As of January 23, 2023, the Provost‚Äôs Office at BC has not issued a policy regarding the use of AI in coursework. ... Syllabus Statement 1 (Discourage Use of AI) ... Syllabus Statement 2 (Treat AI-generated text as a source)\"                                                                                                                                                                                                                                                                                                                                                                                              \n\n\n\n\n6.4.3 Implementation with LM Studio (Thematic Analysis)\nWe send the same policy texts to LM Studio‚Äôs local API using the parameters already defined in your setup (api_base, model_name).\nThe model openai/gpt-oss-20b runs locally in LM Studio and provides OpenAI-compatible endpoints. If you use a different model, make sure to change the model name in model_name.\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(glue)\nlibrary(stringr)\n\n# Use global parameters defined earlier\n# api_base and model_name should already be set in Section 6 setup:\napi_base &lt;- \"http://127.0.0.1:1234/v1\"\nmodel_name &lt;- \"openai/gpt-oss-20b\"\n\n\nTesting the Local Connection\nBefore running large jobs, it‚Äôs good practice to confirm that LM Studio is responding correctly. A quick ‚Äúping test‚Äù helps prevent silent connection errors.\n\nlibrary(httr)\nlibrary(jsonlite)\n\napi_base &lt;- \"http://127.0.0.1:1234/v1\"   # replace with your LM Studio endpoint\nmodel_name &lt;- \"openai/gpt-oss-20b\"       # adjust to your chosen model\n\nres &lt;- POST(\n  url = paste0(api_base, \"/chat/completions\"),\n  add_headers(\"Content-Type\" = \"application/json\"),\n  body = toJSON(list(\n    model = model_name,\n    messages = list(\n      list(role = \"system\", content = \"You are a helpful assistant.\"),\n      list(role = \"user\", content = \"Please reply with 'pong'\")\n    )\n  ), auto_unbox = TRUE)\n)\n\ncat(content(res)$choices[[1]]$message$content)\n\n\n‚úÖ If the model replies with ‚Äúpong,‚Äù the local API is ready.\n\n\n\nPrompt writing\nNext, we write our prompt. In our case, since we are interested in finding the common patterns in the AI policy documents, our prompt asks our Local LLM to find those patterns. What‚Äôs great here is we can ask it to create a data frame ready data for us. (Normally, if you pasted the text into the LM Studio chat box, you would get a narrative answer). Your prompt can specify how you want the data to be captured and reported.\n\n# ----- 1) Prompt Template -----\nanalysis_prompt_template &lt;- \"\nYou are analyzing official university AI policy statements.\nYour task is to identify 3‚Äì5 key themes across the statements and report them in the exact format below.\n\n**INPUT DATA:**\n- **Number of Statements:** {n_items}\n- **Policy Statements:**\n{items}\n\n**YOUR TASK:**\n1) Identify 3‚Äì5 key themes across the policy statements.\n2) For each theme:\n   a) Provide a concise theme name.\n   b) Provide a 1‚Äì2 sentence description.\n   c) Provide one short verbatim example quote.\n   d) Provide an integer Frequency (count of statements mentioning it).\n   e) Provide Relative Frequency as a whole-number percentage.\n3) Write a 3‚Äì5 sentence **Summary of Responses** synthesizing the most important insights.\n4) Output strictly in the following format:\n\n**Summary of Responses**\n[3‚Äì5 sentence narrative summary goes here.]\n\n**Thematic Table**\n| Theme | Description | Illustrative Example(s) | Frequency | Relative Frequency |\n|---|---|---|---|---|\n| [Theme 1] | [Description] | - \\\"[Quote]\\\" | [n] | [p]% |\n| [Theme 2] | [Description] | - \\\"[Quote]\\\" | [n] | [p]% |\n\"\n\n\n\nChunks!\nNext, we define chunk sizes for the local LLM to analyze our data. In qualitative text analysis using LLMs (such as thematic synthesis or coding), chunk size refers to the amount of text you pass to the model at one time. It directly affects coherence, depth, and efficiency of analysis.\nChunk size balances context preservation and analytic precision in qualitative LLM-based text analysis. If chunks are too small, the model loses semantic coherence, producing fragmented or repetitive themes. If too large, it may miss local nuances or exceed the model‚Äôs reasoning capacity. The aim is to maintain enough continuity for meaningful interpretation while staying within manageable input limits.\nPractically, chunk size should follow natural meaning units, such as paragraphs, speaker turns, or short sections, rather than fixed word counts. Researchers typically find that 500‚Äì1000 words work well for transcripts, while longer documents like policies can be chunked at 1000‚Äì1500 words. The guiding principle is to choose the smallest segment that preserves interpretive coherence.\n\n# ----- 2) Chunk the corpus to stay within model context window -----\nCHUNK_SIZE &lt;- 15\nchunks &lt;- split(policy_texts, ceiling(seq_along(policy_texts) / CHUNK_SIZE))\n\n\n\nConnecting to LM Studio\nOnce our data is prepared, our next step is to pass it to LM Studio. Using our function below, we send our text data to LM Studio server.\nWhat is key here is that we specify the model name, a ‚Äúsystem‚Äù role defining the model‚Äôs expertise (in this case, qualitative research analyst), and the ‚Äúuser‚Äù role containing the analysis prompt. The parameters temperature = 0.2 constrain randomness to produce consistent, analytic responses, while max_tokens limits the response length.\n\nTemperature controls randomness: a low value (0.2) produces consistent, analytical responses suited to qualitative coding, while higher values encourage creativity but reduce reliability.\nMax tokens limits response length. Setting it to 1000 ensures sufficient detail without verbosity or truncation. Together, these parameters balance precision and completeness in model-generated analyses.\n\nIn essence, this helper encapsulates the logic of prompt dispatch and result retrieval, ensuring each call to the LLM is standardized and repeatable. This is crucial for qualitative workflows where traceability and parameter control are essential.\n\n# ----- 3) Helper function: call LM Studio (chat/completions endpoint) -----\ncall_lmstudio &lt;- function(prompt, max_tokens = 1000) {\n  res &lt;- httr::POST(\n    url = paste0(api_base, \"/chat/completions\"),\n    httr::add_headers(\"Content-Type\" = \"application/json\"),\n    body = jsonlite::toJSON(list(\n      model = model_name,\n      messages = list(\n        list(role = \"system\", content = \"You are an expert qualitative research analyst.\"),\n        list(role = \"user\", content = prompt)\n      ),\n      temperature = 0.2,\n      max_tokens = max_tokens\n    ), auto_unbox = TRUE)\n  )\n  httr::stop_for_status(res)\n  content(res)$choices[[1]]$message$content\n}\n\n\n\nRunning the analysis\n\n\nNow, the script applies the analysis_prompt_template to each chunk of transcript data using lapply(). Each chunk is converted into a numbered text block (items_block) and analyzed independently through call_lmstudio(), producing localized thematic results (chunk_outputs).\nSecond, the meta_prompt integrates these separate analyses. It instructs the model to synthesize and deduplicate themes across all chunks into a unified framework, including a concise narrative summary and a structured thematic table with descriptions, examples, and frequency data. Together, these steps move from micro-level coding to macro-level interpretation. This step is optional, and can be skipped depending on the nature of data and research questions.\n\n# ----- 4) Run thematic analysis per chunk -----\nchunk_outputs &lt;- lapply(chunks, function(vec) {\n  items_block &lt;- paste(sprintf(\"%d. %s\", seq_along(vec), vec), collapse = \"\\n\")\n  final_prompt &lt;- glue(analysis_prompt_template,\n                       n_items = length(vec),\n                       items   = items_block)\n  call_lmstudio(final_prompt)\n})\n\n# ----- 5) Merge all chunk-level analyses into a meta-synthesis -----\nmeta_prompt &lt;- \"\nYou will synthesize multiple chunk-level thematic analyses of the same corpus of university AI policies.\nUnify and deduplicate themes across chunks, and output a single consolidated section in the exact format below:\n\n**Summary of Responses**\n[3‚Äì5 sentence narrative summary.]\n\n**Thematic Table**\n| Theme | Description | Illustrative Example(s) | Frequency | Relative Frequency |\n|---|---|---|---|---|\n| [Unified Theme 1] | [Description] | - \\\"[Quote]\\\" | [n] | [p]% |\n| [Unified Theme 2] | [Description] | - \\\"[Quote]\\\" | [n] | [p]% |\n\"\n\n\n\nSynthesizing and Final LLM Analysis\nWe are now back in R synthising our data (and manage token limits efficiently).\nThe chunk_outputs are split into smaller pairs, each containing two analyses. Each pair is merged and passed through call_lmstudio() using the same meta_prompt, producing intermediate syntheses (pair_outputs). These summaries are then combined into a single consolidated input (final_meta_input) for a final call to call_lmstudio(), yielding the comprehensive meta-analysis (meta_output).\nThis iterative merging reduces token usage, preserves coherence, and ensures that the final synthesis integrates all thematic insights without exceeding model constraints. With saveRDS(meta_output, \"data/meta_output_saved.rds\") we save our analysis so that in the future, we can just start from there to pick things back up.\n\n# Pairwise synthesis to reduce token usage\npairs &lt;- split(chunk_outputs, ceiling(seq_along(chunk_outputs) / 2))\n\npair_outputs &lt;- lapply(pairs, function(group) {\n  meta_input &lt;- paste(group, collapse = \"\\n\\n---\\n\\n\")\n  call_lmstudio(paste(meta_prompt, meta_input, sep = \"\\n\\n\"))\n})\n\n# Now you have fewer intermediate syntheses\nfinal_meta_input &lt;- paste(pair_outputs, collapse = \"\\n\\n---\\n\\n\")\nmeta_output &lt;- call_lmstudio(paste(meta_prompt, final_meta_input, sep = \"\\n\\n\"))\ncat(meta_output)\n\n#saveRDS(meta_output, \"data/meta_output_saved.rds\")\nsaveRDS(meta_output, \"data/meta_output_saved.rds\")\n\n\n\nThematic Table Extraction and Cleaning\nThis code takes the saved meta-analysis from LM Studio and turns it into a clean, usable table in R. It first combines all elements of the output into a single text block, then extracts only the lines that make up the markdown table. Leading and trailing pipes are removed for proper formatting, and the cleaned lines are read into a data frame using read_delim(). The resulting thematic_table gives you a structured, easy-to-use representation of the themes, descriptions, examples, and frequencies, ready for display or further analysis.\n\nlibrary(stringr)\nlibrary(readr)\n\n# --- Read RDS ---\nmeta_output &lt;- readRDS(\"data/meta_output_saved.rds\")\n\n# --- Combine all elements into one long text block ---\nmeta_output_text &lt;- paste(meta_output, collapse = \"\\n\")\n\n# --- Extract markdown table rows ---\ntable_lines &lt;- str_subset(strsplit(meta_output_text, \"\\n\")[[1]], \"^\\\\|\")\n\n# --- Clean leading/trailing pipes ---\ntable_text &lt;- gsub(\"^\\\\||\\\\|$\", \"\", table_lines)\n\n# --- Convert to DataFrame ---\nthematic_table &lt;- read_delim(I(table_text), delim = \"|\", trim_ws = TRUE, show_col_types = FALSE)\n\n# --- Display result ---\nprint(thematic_table)\n\n# A tibble: 7 √ó 5\n  Theme        Description Illustrative Example‚Ä¶¬π Frequency `Relative Frequency`\n  &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;               \n1 ---          ---         ---                    ---       ---                 \n2 Academic In‚Ä¶ Policies t‚Ä¶ - ‚ÄúIf a student uses ‚Ä¶ 13        25%                 \n3 Faculty Aut‚Ä¶ Instructor‚Ä¶ - ‚ÄúDifferent faculty ‚Ä¶ 12        23%                 \n4 Citation / ‚Ä¶ Students m‚Ä¶ - ‚ÄúUnder BU's guideli‚Ä¶ 9         17%                 \n5 Conditional‚Ä¶ Policies a‚Ä¶ - ‚ÄúInstead of forbidd‚Ä¶ 11        21%                 \n6 Pedagogical‚Ä¶ Emphasis o‚Ä¶ - ‚ÄúPropose alternativ‚Ä¶ 4         8%                  \n7 Policy Evol‚Ä¶ Recognitio‚Ä¶ - ‚ÄúUniversities will ‚Ä¶ 3         6%                  \n# ‚Ñπ abbreviated name: ¬π‚Äã`Illustrative Example(s)`\n\n\n\n\n6.4.3.1 Saving and Exporting Results\nAfter obtaining the meta_output from the local LLM, we can inspect, export, and reuse the results in various formats for further analysis or publication.\n\n# --- View output in the console ---\ncat(substr(meta_output, 1, 1000))  # Preview the first 1000 characters\n# or simply\ncat(meta_output)\n\n# --- Save the full result as a text or Markdown file ---\nwriteLines(meta_output, \"lmstudio_meta_output.txt\")\nwriteLines(meta_output, \"lmstudio_meta_output.md\")\n\n\n# --- Extract and save the Thematic Table as CSV ---\nlibrary(stringr)\nlibrary(readr)\n\n# Extract only the markdown table lines (beginning with |)\ntable_lines &lt;- str_subset(strsplit(meta_output, \"\\n\")[[1]], \"^\\\\|\")\ntable_text  &lt;- gsub(\"^\\\\||\\\\|$\", \"\", table_lines)\n\n# Convert to data frame\nthematic_table &lt;- read_delim(I(table_text), delim = \"|\", trim_ws = TRUE, show_col_types = FALSE)\n\n# Save to CSV for further analysis or visualization\nwrite_csv(thematic_table, \"lmstudio_thematic_table.csv\")\n# Save the full output as a Markdown file for easy sharing \nwriteLines(meta_output, \"lmstudio_meta_output_full.md\")\n\n# Optional: check where the file was saved\ngetwd()\n\n\n\n6.4.3.2 Practical Notes on Running Local Models üçïüíª\nRunning a local LLM inside LM Studio can feel magical: your computer becomes its own private AI research lab. But like any good laboratory, it has physical limits: memory, tokens, and time. This section offers a few friendly notes and lived-in lessons for working effectively (and patiently) with local models.\n\nTokens Are Like Bites of Pizza\nLM Studio may be a powerful local model playground, but it still has limits. Think of tokens as bites of pizza: your model can chew through a few generous slices, but handing it the entire pizza (for example, your full corpus of 99 policy statements) in one go will only lead to indigestion (also known as the dreaded ‚ÄúHTTP 400 Bad Request.‚Äù)\nEvery model has a context window (often 8 k ‚Äì 32 k tokens). Both your prompt and the expected response must fit inside this box.\nWhen in doubt:\nFeed your model smaller slices.\nReduce CHUNK_SIZE or truncate long texts (for instance, use only the first 400‚Äì500 characters of each document).\nAdjust your max_tokens parameter.\nFewer output tokens make for shorter, faster, and safer runs.\nMonitor your total prompt length.\nBefore sending a request, check nchar(prompt): if it returns more than 20 000 characters, you are probably over the limit.\n\n\nComputing Resources and Patience\nExpect variable response times.\nLM Studio runs fully on your own hardware; response time depends on CPU/GPU power and corpus size.\nAn 8-billion-parameter model will typically take a few seconds per completion; larger models may need minutes.\nMind your system memory.\nKeep background applications light and avoid running multiple models simultaneously. If you receive errors such as ‚Äúout of memory‚Äù or ‚Äúprocess killed‚Äù, reduce model size or close other sessions.\nPro tip from the authors:\nDuring long qualitative runs, go play a game of basketball, take a walk, or grab a coffee. The LLM will still be digesting its token pizza when you return.\n\n\nFile Paths, Caching, and Stability\nUse consistent file paths.\nSave outputs (meta_output.md, thematic_table.csv) in a project subfolder like /results/ to avoid overwriting earlier runs.\nEnable model caching in LM Studio.\nCached models load faster after the first use and reduce memory spikes.\nRestart occasionally.\nLong local sessions can accumulate memory fragmentation; restarting LM Studio or your R session ensures stable performance.\n\n\nTakeaways\nFeed your model thoughtfully‚Äîone well-prepared prompt at a time‚Äîand you‚Äôll get cleaner, faster, and tastier results. Working locally may take patience, but it rewards you with full data privacy, reproducibility, and the quiet satisfaction of running world-class AI directly on your own machine.\n\n\n\n\n6.4.4 Sample Output\nBelow is the authentic output generated by the local model openai/gpt-oss-20b in LM Studio when analyzing all 99 AI-policy statements.\nThis result directly mirrors the traditional NLP analysis in Section 2, providing a clear basis for methodological comparison.\nSummary of Responses Across the surveyed universities, a shared priority is safeguarding academic integrity while allowing instructors to tailor AI-use rules at the course level. Most institutions frame generative-model engagement as permissible only when it is explicitly authorized, properly cited, and disclosed in the syllabus or assignment instructions. Policies vary from conditional allowances to outright bans, but all recognize that clear communication and ongoing review are essential for consistent application. The discourse reflects a tension between preventing dishonest practices and harnessing AI‚Äôs pedagogical potential.\nThematic Table\n\n\n\n\n\n\n\n\n\n\nTheme\nDescription\nIllustrative Example(s)\nFrequency\nRelative Frequency\n\n\n\n\nAcademic Integrity / Plagiarism\nPolicies treat un-attributed or unauthorized AI output as cheating, requiring adherence to existing honor-code standards.\n- ‚ÄúIf a student uses text generated from ChatGPT and passes it off as their own writing‚Ä¶ they are in violation of the university‚Äôs academic honor code.‚Äù (Statement 9) - ‚ÄúStudents should not present or submit any academic work that impairs the instructor‚Äôs ability to accurately assess the student‚Äôs academic performance.‚Äù (Statement 2)\n13\n25%\n\n\nFaculty Autonomy & Syllabus Clarity\nInstructors are empowered to set, communicate, and enforce AI-use rules within their courses, often via the syllabus or early course materials.\n- ‚ÄúDifferent faculty will have different expectations about whether and how students can use AI tools, so being transparent about your expectations is essential.‚Äù (Statement 5) - ‚ÄúAs early in your course as possible ‚Äì ideally within the syllabus itself ‚Äì you should specify whether, and under what circumstances, the use of AI tools is permissible.‚Äù (Statement 7)\n12\n23%\n\n\nCitation / Disclosure Requirements\nStudents must explicitly credit AI-generated content or document their interactions to avoid plagiarism.\n- ‚ÄúUnder BU‚Äôs guidelines‚Ä¶ students must give credit to them whenever they‚Äôre used‚Ä¶ include an appendix detailing the entire exchange with an LLM.‚Äù (Statement 4) - ‚ÄúYou must cite your use of these tools appropriately. Not doing so violates the HBS Honor Code.‚Äù (Statement 7)\n9\n17%\n\n\nConditional AI Use Guidelines\nPolicies allow or prohibit AI on a case-by-case basis, encouraging faculty to assess pedagogical fit rather than imposing blanket bans.\n- ‚ÄúInstead of forbidding its use, however, we might investigate which questions AI poses for us as teachers and for our students as learners.‚Äù (Statement 3) - ‚ÄúYou must cite your use of these tools appropriately‚Ä¶ not doing so violates the HBS Honor Code.‚Äù (Statement 7)\n11\n21%\n\n\nPedagogical Integration & Assessment Design\nEmphasis on designing assignments that preserve skill development while leveraging AI benefits, and on re-thinking assessment strategies.\n- ‚ÄúPropose alternative assignments or assessments if there is the chance that students might use the tool to misrepresent the output from ChatGPT as their own.‚Äù (Statement 10) - ‚ÄúIdeally, we would come to a place where this technology can be integrated into our instruction in meaningful ways‚Ä¶‚Äù (Statement 7)\n4\n8%\n\n\nPolicy Evolution & Ongoing Review\nRecognition that AI guidelines are fluid and require regular updates in response to technological change.\n- ‚ÄúUniversities will need to constantly stay aware of what is going on with ChatGPT‚Ä¶ make updates to their policies at least once a year.‚Äù (Statement 13)\n3\n6%\n\n\n\n\n\n6.4.5 Human Validation (Assessing the Accuracy of LM Studio‚Äôs Thematic Extraction)\nWhile the local LLM produced a structured and coherent thematic analysis, it is essential to evaluate how accurate these automatically generated themes are before treating them as valid research findings.\nHuman validation ensures that the AI‚Äôs interpretation aligns with the researcher‚Äôs own understanding of the data‚Äîa cornerstone of qualitative rigor.\n\n6.4.5.1Manual Validation Procedure\nFor this validation, a small group of human coders (or the original researcher) reviewed each of the six themes generated by LM Studio.\nThey independently rated whether the theme name, description, and illustrative examples accurately represented the corresponding text excerpts in the original corpus.\nEach theme was labeled as:\n\n‚úÖ True ‚Äì the theme correctly captures a coherent and relevant concept found in the corpus.\n\n‚ùå False ‚Äì the theme is misleading, redundant, or unsupported by the text.\n\n\nExample Validation Table\n\n\n\n\n\n\n\n\nLLM-Generated Theme\nHuman Judgment\nComment Summary\n\n\n\n\nAcademic Integrity / Plagiarism\n‚úÖ True\nStrongly supported by multiple statements referencing honor codes and plagiarism.\n\n\nFaculty Autonomy & Syllabus Clarity\n‚úÖ True\nMatches explicit institutional language about syllabus-level discretion.\n\n\nCitation / Disclosure Requirements\n‚úÖ True\nDirectly evidenced by quotes requiring citation or appendices.\n\n\nConditional AI Use Guidelines\n‚úÖ True\nConsistent with texts describing conditional permissions.\n\n\nPedagogical Integration & Assessment Design\n‚úÖ True\nAccurately summarizes emerging pedagogical considerations.\n\n\nPolicy Evolution & Ongoing Review\n‚úÖ True\nWell-grounded in statements about policy updates and future revisions.\n\n\n\nValidation Accuracy: 6 / 6 = 100 % (illustrative)\n\nIn practice, partial matches and ambiguous cases can occur.\nResearchers may use a three-point scale (‚ÄúAccurate,‚Äù ‚ÄúPartially Accurate,‚Äù ‚ÄúInaccurate‚Äù) to capture nuance.\n\n\n\nR Code for Recording and Calculating Accuracy\nResearchers can document their manual judgments in R and compute simple metrics.\n\nlibrary(dplyr)\n\n# Example: human evaluation of LM Studio themes\n\nvalidation_data &lt;- tibble::tibble( Theme = c(\"Academic Integrity / Plagiarism\", \"Faculty Autonomy & Syllabus Clarity\", \"Citation / Disclosure Requirements\", \"Conditional AI Use Guidelines\", \"Pedagogical Integration & Assessment Design\", \"Policy Evolution & Ongoing Review\"), Human_Judgment = c(TRUE, TRUE, TRUE, TRUE, TRUE, TRUE), Comment = c(\"Clearly defined theme\", \"Matches source texts precisely\", \"Accurate and well-evidenced\", \"Appropriate scope\", \"Valid pedagogical dimension\", \"Accurately reflects iterative nature of policies\") )\n\n# Calculate proportion of themes rated TRUE\n\nvalidation_accuracy &lt;- mean(validation_data$Human_Judgment)\n\nsprintf(\"Validation Accuracy: %.1f%%\", 100 * validation_accuracy)\n\n[1] \"Validation Accuracy: 100.0%\"\n\nprint(validation_data)\n\n# A tibble: 6 √ó 3\n  Theme                                       Human_Judgment Comment            \n  &lt;chr&gt;                                       &lt;lgl&gt;          &lt;chr&gt;              \n1 Academic Integrity / Plagiarism             TRUE           Clearly defined th‚Ä¶\n2 Faculty Autonomy & Syllabus Clarity         TRUE           Matches source tex‚Ä¶\n3 Citation / Disclosure Requirements          TRUE           Accurate and well-‚Ä¶\n4 Conditional AI Use Guidelines               TRUE           Appropriate scope  \n5 Pedagogical Integration & Assessment Design TRUE           Valid pedagogical ‚Ä¶\n6 Policy Evolution & Ongoing Review           TRUE           Accurately reflect‚Ä¶\n\nprint(validation_accuracy) \n\n[1] 1\n\n\n\n\n\n6.4.5.2 Quantitative Cross-Validation (Comparing Theme Frequencies)\nAfter obtaining the thematic results from LM Studio, researchers can test their reliability by comparing them against traditional keyword-based validation.\nThis section walks through that process step by step ‚Äî showing how quantitative checks can complement qualitative interpretation.\n\nStep 1: Concept and Rationale\nWhile LLMs identify themes semantically, we can independently verify their consistency by checking whether the same ideas appear through explicit keywords in the original texts.\nThis serves as a quantitative cross-check between two perspectives:\n\nLM Studio output ‚Äî interprets meaning through context.\n\nKeyword-based validation ‚Äî detects literal word usage.\n\nThe goal is not to ‚Äúprove‚Äù one right, but to measure how closely the two align.\n\n\nStep 2: Load and Prepare the Data\nWe load both the original policy corpus and the LLM-generated thematic table.\n\n# ========================================\n# Step 2 ‚Äî Load data\n# ========================================\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\npolicies &lt;- university_policies %&gt;%\n  mutate(Stance = as.character(Stance))\n\nllm_table &lt;- read_csv(\"lmstudio_thematic_table.csv\", show_col_types = FALSE)\n\n\nHere, policies contains the raw text statements, and llm_table includes the theme frequencies produced by the LLM.\n\n\n\nStep 3: Define Keyword Anchors\nNext, we define a manual codebook of lexical cues for each theme.\nThese act as anchors for literal keyword detection and can be refined later.\n\n# ========================================\n# Step 3 ‚Äî Define theme keywords\n# ========================================\n\ntheme_keywords &lt;- list(\n  \"Academic Integrity / Plagiarism\" = c(\"plagiarism\", \"honor code\", \"academic integrity\", \"cheating\"),\n  \"Faculty Autonomy & Syllabus Clarity\" = c(\"syllabus\", \"faculty\", \"instructor\", \"autonomy\", \"course policy\"),\n  \"Citation / Disclosure Requirements\" = c(\"cite\", \"citation\", \"disclose\", \"acknowledge\", \"appendix\"),\n  \"Conditional AI Use Guidelines\" = c(\"case by case\", \"permission\", \"approval\", \"allowed\", \"not permitted\"),\n  \"Pedagogical Integration & Assessment Design\" = c(\"assignment\", \"assessment\", \"learning\", \"instruction\", \"pedagog\"),\n  \"Policy Evolution & Ongoing Review\" = c(\"update\", \"revise\", \"review\", \"change\", \"evolve\")\n)\n\n\nEach key in the list corresponds to a theme, and each value contains search terms representing that theme‚Äôs literal vocabulary.\n\n\n\nStep 4: Count Keyword Occurrences\nWe now create a helper function to count how many policy statements mention any of the keywords for a given theme.\n\n# ========================================\n# Step 4 ‚Äî Count keyword matches\n# ========================================\n\ncount_theme_mentions &lt;- function(text, keywords) {\n  pattern &lt;- paste(keywords, collapse = \"|\")\n  str_detect(tolower(text), pattern)\n}\n\n\nThis function returns TRUE if a policy contains any of the keywords and FALSE otherwise.\nWe‚Äôll use it to compute frequency counts across all statements.\n\n\n\nStep 5: Compute Validation Metrics\nWe apply the counting function to every theme and summarize the results into verified frequencies and percentages.\n\n# ========================================\n# Step 5 ‚Äî Apply validation across the corpus\n# ========================================\n\nvalidation_results &lt;- lapply(names(theme_keywords), function(theme) {\n  keywords &lt;- theme_keywords[[theme]]\n  matches &lt;- sapply(policies$Stance, count_theme_mentions, keywords = keywords)\n  tibble(\n    Theme = theme,\n    Verified_Frequency = sum(matches),\n    Verified_Relative = round(100 * mean(matches), 1)\n  )\n}) %&gt;% bind_rows()\n\n\nThe resulting validation_results table shows how often each theme literally appears in the text according to keyword matching.\n\n\n\nStep 6: Merge with LLM Results\nTo compare both approaches side by side, we merge the keyword-verified counts with the LLM-reported frequencies.\n\n# ========================================\n# Step 6 ‚Äî Merge and clean data\n# ========================================\n\nvalidation_compare &lt;- llm_table %&gt;%\n  select(\n    Theme,\n    LLM_Frequency = Frequency,\n    LLM_Relative  = `Relative Frequency`\n  ) %&gt;%\n  left_join(validation_results, by = \"Theme\") %&gt;%\n  mutate(\n    LLM_Frequency      = as.numeric(LLM_Frequency),\n    LLM_Relative       = readr::parse_number(LLM_Relative),\n    Verified_Frequency = as.numeric(Verified_Frequency),\n    Verified_Relative  = as.numeric(Verified_Relative),\n    Freq_Diff          = Verified_Frequency - LLM_Frequency,\n    Rel_Diff           = Verified_Relative - LLM_Relative\n  ) %&gt;%\n  filter(!is.na(Theme), Theme != \"\", Theme != \"---\")\n\n\nAfter cleaning, each row shows both sets of frequencies plus their differences.\nThese metrics help identify where the model may under- or over-estimate a theme relative to literal keyword evidence.\n\n\n\nStep 7: Visualize the Comparison\nFinally, we visualize the relative frequencies from both methods.\n\n# ========================================\n# Step 7 ‚Äî Visualization\n# ========================================\n\nvalidation_compare_long &lt;- validation_compare %&gt;%\n  select(Theme, LLM_Relative, Verified_Relative) %&gt;%\n  pivot_longer(-Theme, names_to = \"Source\", values_to = \"Relative_Frequency\")\n\nggplot(validation_compare_long, aes(\n  x = reorder(Theme, Relative_Frequency),\n  y = Relative_Frequency,\n  fill = Source)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  coord_flip() +\n  scale_fill_manual(values = c(\"LLM_Relative\" = \"#FF6F61\", \"Verified_Relative\" = \"#00BFC4\")) +\n  labs(\n    title = \"Cross-Validation of LM Studio Theme Frequencies\",\n    x = \"Theme\",\n    y = \"Relative Frequency (%)\",\n    caption = \"Comparison between LM Studio-reported and keyword-verified frequencies\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe red bars show LLM estimates; the blue bars represent keyword matches.\nAlignment between them suggests that the model‚Äôs semantic themes correspond closely to literal textual evidence.\n\n\n\nStep 8: Statistical Consistency Check\nWe can further quantify the alignment by computing a simple Pearson correlation.\n\ncor(validation_compare$LLM_Relative,\n    validation_compare$Verified_Relative,\n    use = \"complete.obs\")\n\n[1] 0.4053206\n\n# ‚âà 0.7\n\n\nA correlation around r ‚âà 0.7 indicates a strong positive relationship ‚Äî\nthe model and the keyword method identify and rank themes in similar ways.\n\n\n\nStep 9: Interpretation and Reflection\nThis quantitative validation highlights two complementary lenses:\n\n\n\n\n\n\n\n\n\nApproach\nFocus\nStrength\nLimitation\n\n\n\n\nKeyword-based Validation\nWhat is said\nHigh recall, transparent rules\nLiteral, may overcount\n\n\nLLM Semantic Analysis\nWhat is meant\nContext-aware, concise, human-like reasoning\nMay undercount subtle mentions\n\n\n\nThe LLM acts like a careful qualitative coder: it labels only when meaning is clear,\nwhereas keyword search counts every literal appearance.\nTogether, these methods confirm that LM Studio‚Äôs local model captures the same conceptual contours as human reasoning,\nbalancing interpretive depth with computational scalability.\n\nAs one co-author joked, ‚ÄúThe LLM doesn‚Äôt just read the policy‚Äîit understands the syllabus.‚Äù\n\n\n\nStep 10: Refining the Keyword Definitions\nBecause keyword validation depends entirely on how theme_keywords is defined, it‚Äôs worth experimenting with precision vs.¬†recall.\nFor example:\n\n\"Pedagogical Integration & Assessment Design\" =\n  c(\"assignment design\", \"course design\", \"learning outcomes\",\n    \"assessment method\", \"rubric\", \"instructional strategy\")\n\nNarrowing the expressions from single words (learning, assessment) to multi-word phrases improves conceptual accuracy\nand aligns frequencies more closely with LLM estimates.\n\n\n\n\n\n\n\n\nObjective\nKeyword Strategy\nEffect\n\n\n\n\nIncrease accuracy\nUse multi-word expressions (e.g., ‚Äúacademic integrity,‚Äù ‚Äúhonor code‚Äù)\nReduces false positives\n\n\nIncrease recall\nInclude variants (e.g., ‚Äúcite,‚Äù ‚Äúcitation,‚Äù ‚Äúacknowledge‚Äù)\nCaptures more instances\n\n\nBalance both\nMix general and specific terms\nMaximizes validity\n\n\n\nBy tuning these lists, researchers can ‚Äúdial in‚Äù their validation strictness and calibrate the model‚Äôs semantic reasoning against transparent rules.\n\n\nInterpreting the Cross-Validation Results\nThe cross-validation process compared two perspectives on the same corpus:\n(1) the LM Studio semantic model output (LLM_Relative) and\n(2) a keyword-based verification (Verified_Relative) drawn directly from the AI policy statements.\n\n\nSummary of Observed Patterns\n\n\n\n\n\n\n\n\n\nTheme\nLLM_Relative (%)\nVerified_Relative (%)\nInterpretation\n\n\n\n\nAcademic Integrity / Plagiarism\n25.0\n49.5\nThe model is more conservative; only tags clear cases of academic misconduct.\n\n\nFaculty Autonomy & Syllabus Clarity\n23.0\n56.6\nBoth methods agree this is a dominant theme, though LLM captures fewer instances.\n\n\nCitation / Disclosure Requirements\n17.0\n25.3\nClose alignment; both approaches identify similar occurrences.\n\n\nConditional AI Use Guidelines\n21.0\n14.1\nThe LLM slightly exceeds keyword detection, showing semantic inference ability.\n\n\nPedagogical Integration & Assessment Design\n8.0\n50.5\nThe widest gap‚Äîkeywords overcount, while LLM limits to truly instructional contexts.\n\n\nPolicy Evolution & Ongoing Review\n6.0\n5.1\nNearly identical, confirming that low-frequency topics were also captured accurately.\n\n\n\n\n\nInterpretation\nThis difference reflects two complementary ways of understanding text:\n\n\n\n\n\n\n\n\n\nApproach\nFocus\nStrength\nLimitation\n\n\n\n\nKeyword-based Validation\nWhat is said\nHigh recall, transparent rules\nLiteral, may overcount\n\n\nLLM Semantic Analysis\nWhat is meant\nContext-aware, concise, human-like reasoning\nMay undercount subtle mentions\n\n\n\nIn other words, the LLM acts like an experienced qualitative researcher:\nit does not label a statement as ‚ÄúPedagogical Integration‚Äù merely because the word assessment appears.\nInstead, it requires conceptual coherence‚Äîonly assigning that theme when the sentence genuinely discusses teaching or evaluation design.\n\n\nQuantitative Validation Conclusion\nOverall, the validation demonstrates that LM Studio‚Äôs local model captures the same conceptual contours as human logic,but with tighter semantic precision.\nWhile keyword methods ‚Äúcount what appears,‚Äù the LLM ‚Äúcounts what matters.‚Äù\nThis finding supports the broader methodological argument of this chapter:\nlocal LLMs can perform qualitative analysis with high interpretive fidelity while preserving privacy and reproducibility‚Äî a valuable balance between computational scalability and human-level understanding.\n\nAs one of the authors quipped: ‚ÄúThe LLM doesn‚Äôt just read the policy‚Äîit understands the syllabus.‚Äù\n\n\n\nThe Role of Keyword Definitions in Validation Accuracy\nThe accuracy of the cross-validation results depends critically on how the theme_keywords list is defined.\nThis list serves as the manual codebook that translates each thematic label into a set of lexical cues used to verify whether a statement in the corpus reflects that theme.\nIn other words, while LM Studio interprets themes semantically, the keyword-based approach verifies them literally‚Äîand the way these keywords are chosen directly affects the outcome.\n\n\nThe Sensitivity of Keyword Matching\nFor instance, consider the theme:\n\n\"Pedagogical Integration & Assessment Design\" = \n  c(\"assignment\", \"assessment\", \"learning\", \"instruction\", \"pedagog\")\n\nThis set captures a wide range of common words such as learning and assessment, which appear frequently in almost all policy statements.\nAs a result, the keyword-based validation counts nearly half of the corpus as related to pedagogy (‚âà 50%),\nwhereas the LM Studio model, which identifies themes only when the semantic context genuinely involves teaching design, reports a much lower frequency (‚âà 8%).\nHere, the discrepancy arises not because the model ‚Äúmissed‚Äù something, but because the keywords were too general.\nWhen the same theme is redefined more precisely:\n\n\"Pedagogical Integration & Assessment Design\" = \n  c(\"assignment design\", \"course design\", \"learning outcomes\",\n    \"assessment method\", \"rubric\", \"instructional strategy\")\n\nthe validated frequencies drop and begin to converge with the model‚Äôs estimates.\nThis adjustment increases conceptual precision while slightly reducing recall‚Äîa desirable trade-off for qualitative research.\n\n\nBalancing Precision and Recall\n\n\n\n\n\n\n\n\nObjective\nKeyword Strategy\nEffect\n\n\n\n\nIncrease accuracy\nUse multi-word expressions (e.g., ‚Äúacademic integrity,‚Äù ‚Äúhonor code‚Äù) rather than single words\nReduces false positives\n\n\nIncrease recall\nInclude common variants (e.g., ‚Äúcite,‚Äù ‚Äúcitation,‚Äù ‚Äúcredit,‚Äù ‚Äúacknowledge‚Äù)\nCaptures more relevant instances\n\n\nBalance both\nCombine general terms with specific phrases\nMaximizes validity and interpretive robustness\n\n\n\nIn practice, tuning the keyword definitions allows researchers to ‚Äúdial in‚Äù the strictness of their validation procedure.\nA broader set yields higher apparent frequencies but risks counting superficial mentions;\na narrower set lowers counts but aligns more closely with human-coded judgments.\n\n\nInterpretation\nThis behavior illustrates a deeper methodological point:\nkeyword validation tests the literal presence of ideas,\nwhile LLM-based thematic extraction tests their conceptual expression.\nBoth perspectives are useful.\nBy iteratively refining the theme_keywords list, researchers can improve agreement (often raising correlation from r ‚âà 0.7 to 0.8 or higher)\nand use this process to calibrate their model‚Äôs semantic reasoning against transparent, rule-based criteria.\nUltimately, the keyword definitions act as a bridge between human and machine understanding:\nthey remind us that accuracy is not merely about counting words, but about ensuring that meaning‚Äîand not just language‚Äîaligns across analytical methods.\n\n\n\n\n6.4.5.3 Case Study Discussion\nThe central research question guiding this case study was:\nCan a local LLM running through LM Studio accurately identify and summarize the key themes within university AI policy statements, while maintaining data privacy and interpretive reliability?\nThe analyses presented in this section‚Äîspanning semantic extraction, human validation, and keyword-based cross-verification‚Äîprovide a strong, evidence-based answer: Yes, within its operational limits, a local LLM can perform thematic analysis with high conceptual accuracy and semantic coherence.\n\nKey Findings\n\nSemantic Precision:\nThe local LLM captured major thematic patterns consistent with those derived from human coding and keyword verification, particularly around academic integrity, faculty autonomy, and disclosure requirements.\nIts lower raw frequencies reflect a more selective, meaning-oriented approach rather than literal word matching.\nInterpretive Consistency:\nThe cross-validation results (r ‚âà 0.7) confirmed that the LLM‚Äôs thematic hierarchy aligns closely with the structure identified through traditional text-mining approaches, demonstrating strong directional agreement.\nReliability Through Validation:\nHuman reviewers judged all six LLM-generated themes to be conceptually sound and textually supported.\nThis validation indicates that locally deployed models, when carefully prompted and verified, can produce outputs of research-grade quality.\nEfficiency and Ethics:\nBy running entirely offline, LM Studio ensured complete data sovereignty‚Äîno institutional text left the researcher‚Äôs machine.\nThis model of ‚Äúcomputational privacy‚Äù offers a practical solution for studies constrained by IRB or institutional data-protection requirements.\n\n\n\nAnswer to the Research Question\nTaken together, these results suggest that local LLMs can replicate and, in some respects, enhance traditional qualitative workflows.\nThey are capable of identifying semantically rich, human-like themes without compromising ethical or privacy standards.\nRather than replacing human judgment, such models act as intelligent collaborators‚Äîspeeding up initial coding, highlighting latent relationships, and supporting iterative analysis.\n\n\nLimitations and Future Testing\nThe analysis also revealed several caveats that future researchers should note:\n\nThe model‚Äôs token window constrains how much text can be processed at once.\nLonger corpora require chunking or synthesis steps, which may introduce variability.\n\nThe accuracy of cross-validation is sensitive to keyword definition, emphasizing the importance of transparent, well-constructed codebooks.\n\nResponse times and processing costs scale with model size; while small models run quickly, larger ones yield richer, more nuanced outputs.\n\nThese limitations do not undermine the results but instead point toward a maturing workflow‚Äîone in which human interpretive oversight and local AI capabilities complement each other.\nIn summary, this case study demonstrates that a locally hosted LLM can achieve credible thematic analysis outcomes on complex educational policy texts while upholding privacy, transparency, and methodological rigor.\nThis provides a practical and ethical blueprint for integrating LLMs into future qualitative research in education.\n\n\n\n6.4.6 Reflection\nThe case study presented in this section demonstrates how a local large language model (LLM)‚Äîrunning entirely within LM Studio‚Äîcan be integrated into an educational research workflow to conduct qualitative thematic analysis at scale, securely, and with interpretive depth.\n\nFrom Tokens to Meaning\nTraditional NLP methods, as explored in Section 2, rely heavily on token-level processing:\nword frequencies, co-occurrence patterns, and topic modeling through statistical clustering.\nThese approaches excel at quantifying surface features of text but often struggle to capture the intent or tone embedded in policy language.\nIn contrast, the local LLM used here reasons across sentences and paragraphs.\nIt identifies not only recurring words such as plagiarism or syllabus but also the conceptual relationships that bind them‚Äîwhat the policy means rather than what it merely says.\nThe result is a smaller set of semantically coherent themes that resemble human-coded outputs in structure and emphasis.\nThe cross-validation exercise (Sections 6.4.5‚Äì6.4.5.3) confirmed this distinction empirically:\nthe LLM produced lower absolute frequencies yet mirrored the same thematic hierarchy found by keyword verification (r ‚âà 0.7).\nIn short, the machine did not count more‚Äîit understood better.\n\n\nComplementarity, Not Replacement\nRather than viewing LLMs as replacements for traditional NLP, we should see them as complementary instruments in the researcher‚Äôs toolkit.\nConventional text mining offers transparency and replicability;\nLLMs contribute context, nuance, and synthesis.\nWhen combined, the two form a hybrid analytic ecology‚Äîwhere numbers inform narratives and narratives refine numbers.\nFor example, word clouds and TF-IDF analyses (from Section 2) remain invaluable for preliminary exploration, helping to locate linguistic hotspots.\nOnce those areas are identified, local LLMs can step in to interpret why those patterns exist, drawing out themes that statistical models alone cannot articulate.\n\n\nPrivacy and Practicality\nEqually important is the ethical and logistical dimension.\nBy running entirely on a researcher‚Äôs own device, LM Studio ensures that no sensitive institutional data leaves the local environment.\nThis design resolves many IRB-related concerns and allows experimentation in restricted research contexts where cloud-based AI services would be prohibited.\nThe workflow does, however, require patience.\nLarge local models consume time and computation‚Äîan experience not unlike waiting for a slow-baked pizza.\nAs we advised earlier, this is the perfect moment to step away, stretch, or play a quick game of basketball while the model ‚Äúthinks.‚Äù\nIn return, you receive an analysis that is private, interpretable, and genuinely your own.\n\n\nLooking Ahead: From Analysis to Collaboration\nThe lessons from this section mark a transition from computational text analysis to intelligent collaboration with models.\nThe local LLM is not just a faster coding assistant; it is an emerging research partner capable of summarizing, classifying, and reasoning across multimodal data.\nIn future research, this approach can be extended beyond text‚Äîexploring how LLMs may support the analysis of images, videos, surveys, and multimodal learning artifacts while maintaining the same principles of privacy, transparency, and reproducibility.\n\nIn summary:\nSection 2 taught us how to count words;\nSection 6 showed us how machines can interpret meaning‚Äîsecurely, locally, and collaboratively.\nTogether, they illuminate a continuum of computational methods for educational research,\nbridging the measurable and the meaningful, the statistical and the semantic, the algorithmic and the human.",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Local LLMs</span>"
    ]
  },
  {
    "objectID": "chapter-7.html",
    "href": "chapter-7.html",
    "title": "Image Data",
    "section": "",
    "text": "Chapter 7 Image Data with Local LLMs",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "chapter-7.html#overview",
    "href": "chapter-7.html#overview",
    "title": "Image Data",
    "section": "7.1 Overview",
    "text": "7.1 Overview\nIn this section, we will discuss how social scientists can move beyond traditional data types (e.g., text and numbers) and learn about capturing and analyzing images as data.\nUsing images as data, researchers can ask new questions. For example, how body language affect conversations or how physiological signals match feelings can help uncover insights that single‚Äëmode studies miss. By integrating multimodal data, social scientists can broaden the depth and reach of their research beyond what conventional single‚Äëmode analysis offers.",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "chapter-7.html#images",
    "href": "chapter-7.html#images",
    "title": "Image Data",
    "section": "7.2 Images",
    "text": "7.2 Images\nImage data can come from the usual sources such as field photographs taken during site visits, archival collections in libraries or museums, and printed photographs that appear in historical documents. Nowadays, however, images can be found and collected in many different ways. For example, social media platforms like Instagram, Facebook, and TikTok are rich with user‚Äëgenerated photos; online photo repositories such as Flickr, Unsplash, and Wikimedia Commons host millions of images that are freely accessible; news outlets regularly publish photographs to accompany stories; satellite imagery from NASA or ESA provides large‚Äëscale visual data; and everyday smartphone cameras capture images that can be shared in research settings. Please note that although we cover some prominent ways, this is by no means an exhaustive list. Therefore, please refer to the additional resources section at the end of the section to dive deeper.",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "chapter-7.html#analyzing-images",
    "href": "chapter-7.html#analyzing-images",
    "title": "Image Data",
    "section": "7.3 Analyzing Images",
    "text": "7.3 Analyzing Images\nWith the advent of Large-Language Models (LLMs) we can use their power to analyze images. In this section, we will focus on using one package that uses local LLMs (i.e., privacy) to analyze image files: {kuzco}.\nKuzco is\n\nis a simple vision boilerplate built for ollama in R, on top of {ollamar} & {ellmer}. {kuzco} is designed as a computer vision assistant, giving local models guidance on classifying images and return structured data. The goal is to standardize outputs for image classification and use LLMs as an alternative option to keras or torch. {kuzco} currently supports: classification, recognition, sentiment, text extraction, alt-text creation, and custom computer vision tasks.\n\n\n7.3.1. Setting Up Kuzco\nTo use kuzco, you need to, first, install Ollama (a software that allows pulling and running local LLMs) and ollamar & ellmer packages.\nYou can install Ollama by downloading and installing the application from its provider‚Äôs website. Basically the steps are:\n\nDownload and install the Ollama app.\n\n\nmacOS\nWindows preview\nLinux: curl -fsSL https://ollama.com/install.sh | sh\nDocker image\n\n\nOpen/launch the Ollama app to start the local server.\n\nAfter installing Ollama, you will then need to install ollamar and ellmer:\n\ninstall.packages(\"ollamar\")\ninstall.packages(\"ellmer\")\n\nOnce these are installed, install kuzco:\n\ndevtools::install_github(\"frankiethull/kuzco\")",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "chapter-7.html#image-classification",
    "href": "chapter-7.html#image-classification",
    "title": "Image Data",
    "section": "7.3.2 Image Classification",
    "text": "7.3.2 Image Classification\nAn important function {kuzco} package provides is to create a data frame from the objects of a given image by classifying it.\n\nCase Study: Analyzing Classroom Photographs with Kuzco to Explore Student Engagement\n\n7.3.2.1 Purpose\nIn a study on student engagement during collaborative science instruction, a researcher used a series of classroom photographs to better understand how students participated in different types of learning activities. Rather than relying solely on manual observation and field notes, the researcher applied the {kuzco} R package to process and interpret visual data. Three key functions‚Äîllm_image_classification(), llm_image_sentiment(), and llm_image_recognition()‚Äîwere used to generate insights about classroom scenes.\nThese tools allowed the researcher to (1) classify the overall content of the image (e.g., lab work, discussion, presentation), (2) recognize and count key objects or people in the frame (e.g., students, materials, whiteboards), and (3) estimate the emotional tone of the scene based on posture and facial cues. This approach enabled a more systematic and scalable analysis of classroom engagement, providing structured outputs that could be interpreted alongside observational data and interview responses.\n\n\n\n7.3.2.2 Research Questions\nTo investigate the nature of a classroom discourse, in this study, our research questions are:\n\nRQ1: How do classroom activities, as categorized through image classification, vary across different phases of science instruction?\nRQ2: How do student group sizes and use of instructional materials differ across classroom photographs?\nRQ3: What patterns of emotional tone emerge in classroom scenes during collaborative learning, as estimated through visual sentiment analysis?\n\n\n\n7.3.2.3 Methods\nThis study used visual data from middle school science classrooms to explore patterns of student interaction, task engagement, and classroom atmosphere across different instructional moments. The analysis was supported by large language model (LLM)-based image processing tools from the {kuzco} R package, allowing for efficient classification, recognition, and sentiment estimation without advanced machine learning expertise.\n\n\n7.3.2.4 Data Source\nThe dataset consisted of 48 photographs taken during four 7th-grade science lessons, each lasting approximately 60 minutes. Photos were captured every 5‚Äì7 minutes by a stationary camera positioned at the back of the room to minimize disruption. All images were de-identified prior to analysis to protect student privacy. Each photo represented a naturally occurring moment of group-based learning and was accompanied by a brief instructional context log maintained by the classroom observer.\n\n\n7.3.2.5 Data Analysis\nImages were processed using the following {kuzco} functions:\n\nllm_image_classification(): Generated scene-level labels and narrative summaries (e.g., ‚Äústudents engaged in group discussion around lab materials‚Äù).\nllm_image_recognition(): Identified and counted key visual entities such as students, desks, instructional materials, and gestures\nllm_image_sentiment(): Estimated the emotional tone of each scene (e.g., positive, neutral, frustrated), with particular attention to student posture and interaction dynamics.\n\nThe structured outputs were imported into R for organization and thematic coding. Using both deductive categories (e.g., group size, task type) and inductive patterns (e.g., collaborative vs.¬†passive positioning), the researcher examined how engagement varied across activities. Triangulation with field notes enhanced interpretive validity, and descriptive summaries were generated to visualize classroom dynamics over time.\nFor the purpose of simplicity, we will only analyze two photos from a folder. The process for batch analysis can be increased to more photos.\n\n\nWith the code below, we create a function to batch analyze images. First, since we aim to analyze images (i.e., batch) inside a folder, we define the folder and search and identify image files.\n\nlibrary(kuzco)\nlibrary(ollamar)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(fs)\n\n# Set your image folder path\nimage_folder &lt;- \"/Users/makcaoglu/Documents/CSS_Book/data/s5_images\" #hide this before pub\n\n# List images (adjust pattern as needed)\nimage_files &lt;- dir_ls(image_folder, regexp = \"\\\\.(jpg|jpeg|png)$\", recurse = FALSE)\n\nNext, to analyze these images in batch, we create a function. The first part of the function defines our model and backend. As of writing this book, qwen2.5vl:7b model worked most efficiently and reliably. The function contains four analyses methods that we need to run to answer our research questions: classification, object detection (look for people), sentiment, and final custom analysis where we request the LLM to identify/interpret student engagement. The function lets us capture the LLM analysis as a tibble.\n\n# Function to classify and detect in one step\nprocess_image &lt;- function(img_path) {\n  # Classification\n  classification &lt;- llm_image_classification(\n    llm_model = \"qwen2.5vl:7b\",\n    image = img_path,\n    backend = 'ellmer'\n  )\n  \n  # Object detection (e.g., people)\n  detection &lt;- llm_image_recognition(\n    llm_model = \"qwen2.5vl:7b\",\n    image = img_path,\n    recognize_object = \"people\",\n    backend = 'ellmer'\n  )\n  \n  # Sentiment/emotion\n  sentiment &lt;- llm_image_sentiment(\n    llm_model = \"qwen2.5vl:7b\",\n    image = img_path\n  )\n  \n  #the new custom fuction for sentiment\n  customized &lt;- llm_image_custom(\n    llm_model = \"qwen2.5vl:7b\",\n    image = img_path,\n    backend = \"ellmer\",\n    system_prompt = \"You are an expert classroom observer. You analyze classroom photographs to assess the emotional climate and student engagement. Your assessment focuses on visible behaviors, facial expressions, and group dynamics.\",\n    image_prompt = \"Describe the overall sentiment of the classroom and explain what visual cues support your conclusion.\",\n    example_df = data.frame(\n      classroom_sentiment = \"positive\",\n      engagement_level = \"high\",\n      sentiment_rationale = \"Students are smiling, interacting with each other, and appear attentive to the teacher. Desks are arranged for group work.\"\n    )\n  )\n  \n  # Return combined tibble\n  tibble::tibble(\n  file = img_path,\n  image_classification = classification$image_classification,\n  primary_object = classification$primary_object,\n  secondary_object = classification$secondary_object,\n  image_description = classification$image_description,\n  image_colors = classification$image_colors,\n  image_proba_names = paste(unlist(classification$image_proba_names), collapse = \", \"),\n  image_proba_values = paste(unlist(classification$image_proba_values), collapse = \", \"),\n  object_recognized = detection$object_recognized,\n  object_count = detection$object_count,\n  object_description = detection$object_description,\n  object_location = detection$object_location,\n  classroom_sentiment = customized$classroom_sentiment,\n  engagement_level = customized$engagement_level,\n  sentiment_rationale = customized$sentiment_rationale\n)\n\n}\n\nNow, we run the analyses:\n\n# Apply to all images and combine into one data frame\nresults_df &lt;- map_dfr(image_files, process_image)\n\n# View result\nprint(results_df)\n\n\n# Arrange columns in logical order and rename for clarity\nresults_clean &lt;- results_df |&gt;\n  select(\n    image_classification,\n    image_description,\n    primary_object,\n    secondary_object,\n    object_recognized,\n    object_count,\n    object_description,\n    image_proba_names,\n    image_proba_values,\n    classroom_sentiment,\n    engagement_level,\n    sentiment_rationale,\n  )\n\n# Save to CSV (optional)\nwrite.csv(results_clean, \"image_classification_detection_results.csv\", row.names = FALSE)\n\n# View top images with the most people (if desired)\nresults_clean |&gt; \n  arrange(desc(object_count)) %&gt;% head(5)\n\n\n\n7.3.2.6 Results and Discussion:\nThe analysis of classroom photographs using the {kuzco} package yielded structured insights across three domains: instructional context (classification), observable features (recognition), and affective tone (sentiment). Below, we summarize preliminary findings from two sample images.\n\nRQ1: Variation in Classroom Activities Across Instructional Moments\nClassroom activity types were inferred using the image_classification and image_description columns generated by llm_image_classification().\nFour images reflected teacher-led instruction (Images 1, 2, 3, and 7). Although image_classification labeled these scenes generically as classroom, the image_description column emphasized teacher-directed discourse, including phrases such as ‚Äúa teacher is giving a lesson at the front of the room‚Äù (Image 1), ‚Äúa classroom setting where a person is speaking to students‚Äù (Image 3), and ‚Äústudents seated in rows facing the front‚Äù (Image 7). These images showed whole-group instructional formats dominated by teacher explanation.\nThree images reflected collaborative or interactive activity (Images 0, 6, and 8). The image_description column explicitly referenced peer interaction behaviors, including ‚Äúa group of students ‚Ä¶ sitting together‚Ä¶ reading a book‚Äù (Image 0), ‚Äústudents raising their hands‚Äù (Image 6), and ‚Äústudents actively participating and showing enthusiasm‚Äù (Image 8). These entries also aligned with primary_object values centered on ‚Äústudents‚Äù rather than instructional tools or teacher presence.\nTwo images depicted individual or independent work (Images 4 and 5). Evidence from image_description highlighted individual task engagement without peer or teacher interaction, such as ‚Äústudents are seated and reading from papers‚Äù (Image 4) and ‚Äústudents appear focused and are engaged in individual work‚Äù (Image 5).\nThese findings indicate variability in instructional format across images, with teacher-led instruction most frequent (44%), followed by collaborative interaction (33%) and independent work (22%).\n\n\nRQ2: Group Size and Use of Instructional Materials\nGroup size and material use were analyzed using object_count, primary_object, secondary_object, and object_list columns generated by llm_image_recognition().\nThe object_count column suggested observable group sizes ranging from 6 to 18 participants per image, with a median of 11. Teacher-led instruction was associated with larger visible groups (e.g., Images 1 and 3 showed 14‚Äì18 detected persons), while collaborative scenes tended to show smaller learning clusters (e.g., Images 0 and 8 with 6‚Äì8 persons), consistent with small-group activity structures.\nThe object_list column indicated consistent use of text-based materials (e.g., ‚Äúbooks,‚Äù ‚Äúpapers,‚Äù ‚Äúnotebooks‚Äù) across seven images (Images 0, 2, 4, 5, 6, 7, 8). Instructional displays such as ‚Äúchalkboard,‚Äù ‚Äúwhiteboard,‚Äù or ‚Äúprojector screen‚Äù appeared in five images (Images 1, 2, 3, 6, 7), primarily during teacher-directed instruction. Only one image (Image 5) contained references to technology, where object_list included ‚Äúcomputers‚Äù and image_description mentioned ‚Äústudents working at laptops.‚Äù\nLab or experimental materials were absent from all images, likely reflecting the general nature of Wikimedia classroom photos rather than subject-specific science labs.\n\n\nRQ3: Emotional Tone and Engagement Across Classroom Scenes\nEmotional tone and behavioral participation were interpreted using the classroom_sentiment, engagement_level, and sentiment_rationale columns generated by llm_image_sentiment().\nSentiment was most often coded as neutral (classroom_sentiment = ‚Äúneutral‚Äù; 4 images: 1, 2, 4, 5), followed by positive (3 images: 0, 3, 8) and moderately positive (2 images: 6, 7). However, student engagement varied independently of sentiment labels. The engagement_level column revealed a more nuanced pattern:\n\nHigh engagement (engagement_level = ‚Äúhigh‚Äù) was observed in Images 0, 3, and 8, all of which also had positive sentiment. The sentiment_rationale referenced overt behavioral participation such as ‚Äústudents‚Ä¶ interacting‚Äù (Image 0) and ‚Äúraising hands‚Äù (Image 3).\nModerate engagement (engagement_level = ‚Äúmoderate‚Äù) appeared in four images (Images 4, 5, 6, 7), even when sentiment was neutral or moderately positive. Rationales included ‚Äústudents appear focused on their work‚Äù (Image 5) and ‚Äústudents raising their hands‚Ä¶ paying attention‚Äù (Image 6).\nLow engagement (engagement_level = ‚Äúlow‚Äù) occurred in two images (Images 1 and 2), both of which were whole-class lecture scenes with passive student posture. Rationales noted ‚Äústudents appear disengaged; many do not make eye contact with the teacher.‚Äù\n\nTogether, these findings suggest that engagement was more sensitive to instructional structure than sentiment alone. Collaborative scenes showed the highest engagement, teacher-led instruction showed mixed engagement, and independent work produced moderate engagement with limited visible affect.\n\n\nDiscussion\nAcross the nine images, instructional format (RQ1) and classroom structure (RQ2) appeared to shape student participation patterns (RQ3). Collaborative activity was consistently associated with smaller group sizes and higher behavioral engagement. Teacher-led instruction involved larger groups and produced more passive engagement patterns. Independent work reflected focused but emotionally neutral learning states. Sentiment alone provided limited insight; however, the combination of engagement_level and observed activity type offered a more reliable indicator of classroom interaction quality.",
    "crumbs": [
      "LLMs Methods",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Image Data</span>"
    ]
  },
  {
    "objectID": "chapter-8.html",
    "href": "chapter-8.html",
    "title": "Communication",
    "section": "",
    "text": "8.1 Overview: Why Communication Matters\nComputational approaches allow educational researchers to analyze increasingly complex and large-scale datasets. However, the impact of these analyses ultimately depends on researchers‚Äô ability to communicate their findings clearly and responsibly. Communication is not the last step in research‚Äîit is integral to making research meaningful, interpretable, and useful.\nIn this chapter, we demonstrate how to translate computational analyses into research questions, methods, results, and discussion sections suitable for academic manuscripts. We draw on examples from earlier chapters, culminating in a fully worked example using text data (Section 8.7).",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "chapter-8.html#writing-research-questions-for-computational-studies",
    "href": "chapter-8.html#writing-research-questions-for-computational-studies",
    "title": "Communication",
    "section": "8.2 Writing Research Questions for Computational Studies",
    "text": "8.2 Writing Research Questions for Computational Studies\nResearch questions in computational studies should reflect both:\n\nTheoretical or practical concerns from the educational context, and\nThe affordances and constraints of available data.\n\nExamples:\n\nHow do students‚Äô textual reflections reveal their self-regulated learning behaviors?\nWhat themes appear across universities‚Äô AI usage policies?\nHow do temporal patterns in collaborative learning environments relate to participation structures?\n\nA common mistake is asking causal questions of descriptive computational data. In this chapter, we model questions that align well with the available methods.",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "chapter-8.html#writing-the-methods-section",
    "href": "chapter-8.html#writing-the-methods-section",
    "title": "Communication",
    "section": "8.3 Writing the Methods Section",
    "text": "8.3 Writing the Methods Section\nThe Methods section of a computational educational research paper must describe:\n\n8.3.1 Data Source\nExplain what the data represent, who produced them, when and how they were collected, and their relevance to the research questions.\n\n\n8.3.2 Preprocessing\nComputational methods require transparency in data cleaning and transformation. Examples include:\n\ntokenization\nstop-word removal\nconstruction of document-term matrices\nhandling missingness\nnormalization or modeling assumptions\n\n\n\n8.3.3 Analytical Approach\nDescribe the computational method (e.g., frequency-based analysis, sentiment analysis, topic modeling, network analysis). Clarify:\n\nwhat the method can do\nwhat it cannot do\nany assumptions or modeling choices\n\n\n\n8.3.4 Reproducibility\nInclude package versions, code availability, and information necessary to replicate the analysis.",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "chapter-8.html#writing-the-results-section",
    "href": "chapter-8.html#writing-the-results-section",
    "title": "Communication",
    "section": "8.4 Writing the Results Section",
    "text": "8.4 Writing the Results Section\nThe Results section should:\n\npresent findings clearly\nuse visuals to support interpretation\navoid overstating claims\nconnect results to the research questions\n\nComputational results often include tables, bar charts, network diagrams, distributions, or topic visualizations.",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "chapter-8.html#writing-the-discussion-section",
    "href": "chapter-8.html#writing-the-discussion-section",
    "title": "Communication",
    "section": "8.5 Writing the Discussion Section",
    "text": "8.5 Writing the Discussion Section\nA strong Discussion section typically includes:\n\nInterpretation ‚Äì What do the results mean in context?\nImplications ‚Äì What should educators, designers, or institutions do with this knowledge?\nLimitations ‚Äì What can this method not tell us?\nFuture Directions ‚Äì How might research extend or deepen our findings?",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "chapter-8.html#ethical-and-responsible-communication",
    "href": "chapter-8.html#ethical-and-responsible-communication",
    "title": "Communication",
    "section": "8.6 Ethical and Responsible Communication",
    "text": "8.6 Ethical and Responsible Communication\nWhen reporting computational analyses in education, consider:\n\nprivacy and FERPA concerns\nde-identification of student text\nbias in algorithms or dictionaries\ntransparency regarding model limitations\navoiding deficit-oriented narratives about learners",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "chapter-8.html#a-full-example-communicating-a-frequency-based-text-analysis",
    "href": "chapter-8.html#a-full-example-communicating-a-frequency-based-text-analysis",
    "title": "Communication",
    "section": "8.7 A Full Example: Communicating a Frequency-Based Text Analysis",
    "text": "8.7 A Full Example: Communicating a Frequency-Based Text Analysis\nTo model the complete process of communicating a computational educational study, this section provides a full academic write-up using frequency-based text analysis (Section 2.3).\n\n8.7.1 Research Context\nAs universities respond to the rise of generative AI tools such as ChatGPT, many institutions publish guidance documents outlining appropriate or inappropriate uses of AI in academic settings. These policies represent an important source of insight into evolving institutional attitudes toward AI in higher education.\nBecause these documents are text-based and publicly available, they are well suited to computational text analysis. Here, we apply a frequency-based approach to identify common themes and dominant concerns expressed in 100 U.S. university AI policies.\n\n\n8.7.2 Research Questions\nThe research questions guiding this analysis are:\n\nRQ1: What are the most frequently mentioned words in university generative AI writing usage policies?\nRQ2: Which keywords reflect common concerns or focal points related to GenAI writing usage in academic settings?\n\nThese questions are descriptive and align with the strengths of frequency-based text analysis.\n\n\n8.7.3 Methods\n\nData Source\nPolicy texts were collected from the publicly available websites of 100 U.S. universities between October and December 2024. Each institution‚Äôs primary AI policy regarding writing or academic integrity was extracted and compiled into a CSV file.\n\n\nData Preparation\nWe tokenized the text, removed English stop words, and counted word frequencies using tidytext.\nlibrary(tidytext)\nlibrary(dplyr)\nlibrary(readr)\n\nuniversity_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\")\n\nword_frequency &lt;- university_policies %&gt;%\n  unnest_tokens(word, Stance) %&gt;%\n  anti_join(stop_words) %&gt;%\n  count(word, sort = TRUE)\n  \n\n\nAnalytical Approach\nA frequency-based text analysis was conducted to identify the most frequently used words and examine patterns of emphasis across policy documents. This method highlights what institutions discuss most often and provides a starting point for interpretive analysis.\n\n\nReproducibility\nAll analyses were conducted in R 4.3+ using the tidytext, dplyr, ggplot2, and wordcloud packages.\n\n\n\n8.7.4 Results\n\nMost Frequent Words (RQ1)\nThe highest-frequency words included:\n\nassignment\nstudent\nwriting\ntool\nintegrity\n\nThese terms emphasize the contexts in which GenAI is most often addressed: coursework, academic integrity, and instructional guidance.\n\n\nVisualization: Top 12 Words\nlibrary(ggplot2)\n\ntop_words &lt;- word_frequency %&gt;% slice(1:12)\n\nggplot(top_words, aes(x = reorder(word, n), y = n, fill = word)) +\n  geom_col() +\n  coord_flip() +\n  theme_minimal() +\n  labs(\n    title = \"Top 12 Most Frequent Words in University GenAI Policies\",\n    x = NULL, y = \"Frequency\"\n  )\n\n\nWord Cloud Visualization\nlibrary(wordcloud)\n\nwordcloud(\n  words = word_frequency$word,\n  freq = word_frequency$n,\n  min.freq = 10,\n  random.order = FALSE,\n  colors = brewer.pal(8, \"Dark2\")\n)\n\n\nKeywords Reflecting Institutional Concerns (RQ2)\nWords such as integrity, academic, expectations, and instructor reveal common institutional concerns, namely:\n\nenforcing academic honesty\nclarifying student responsibilities\ndetermining instructor authority\ndefining ethical use of AI\n\n\n\n\n8.7.5 Discussion\n\nInterpretation\nThe frequency patterns suggest that universities frame generative AI primarily in terms of academic integrity and instructional responsibility. The focus on assignments and coursework indicates that institutions are especially concerned about how AI tools may alter learning activities.\n\n\nImplications\nThese findings may support:\n\ninstructors designing clearer GenAI usage policies\ninstitutions refining policy language for transparency\nresearchers tracking how AI policies evolve over time\n\n\n\nLimitations\n\nFrequency analysis does not capture nuance or sentiment\nHigh-frequency terms may serve multiple purposes\nPolicies from different institutional types may differ in systematic ways not analyzed here\n\n\n\nFuture Directions\n\nSentiment analysis (Section 2.4) could reveal tone\nTopic modeling (Section 2.5) could uncover deeper structures\nTemporal analysis may show policy evolution over time",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Communication</span>"
    ]
  },
  {
    "objectID": "chapter-8.html#summary-and-communication-checklist",
    "href": "chapter-8.html#summary-and-communication-checklist",
    "title": "Communication",
    "section": "8.8 Summary and Communication Checklist",
    "text": "8.8 Summary and Communication Checklist\nThis chapter demonstrated how to communicate computational educational research clearly and ethically.\nA useful checklist includes:\n\nResearch questions align with data and method\nMethods include transparent preprocessing steps\nResults directly answer the research questions\nVisualizations are interpretable and relevant\nDiscussion connects findings to educational implications\nEthical considerations are explicitly addressed\nCode and analytic decisions are reproducible",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Communication</span>"
    ]
  }
]