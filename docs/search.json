[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Analysis of Educational Data: A Field Guide Using R",
    "section": "",
    "text": "Preface\nWelcome to Computational Analysis of Educational Data: A Field Guide Using R!\nThe book is organized around six sections. Within these seven sections are specific chapters, which serve as cookbook “entries”. While the section overviews (the first bullet point within each section) introduce the techniques or methodologies used in the section’s chapters, the entries (the subsequent bullet points) are intended to address a specific, narrow problem, as well a to provide a sample for researchers in writing their research questions, methods, results (and discussions) sections based on the analyses.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "chapter-1.html",
    "href": "chapter-1.html",
    "title": "Chapter 1 Getting Started",
    "section": "",
    "text": "Section Overview\nAbstract: In this section, we will introduce the steps to get started with using R and RStudio. The section will also cover the key concepts in working with R such as packages, R markdown. We will also cover important steps in preparing data, which we will be using frequently in the rest of the book. Since these topics have been covered in so much more detail in other books, or other resources online, we will end the section with a list of key resources on these topics that are easily accessible online.",
    "crumbs": [
      "Section 1 — Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1 **Getting Started**</span>"
    ]
  },
  {
    "objectID": "chapter-1.html#using-r-and-rstudio",
    "href": "chapter-1.html#using-r-and-rstudio",
    "title": "Chapter 1 Getting Started",
    "section": "Using R and RStudio",
    "text": "Using R and RStudio\n\nFoundational skills: Data, packages, functions, projects\nReproducibility and R Markdown documents\nBase R: assignment, equality, dollar-sign indexing, xx\nR files\nProject setup within RStudio",
    "crumbs": [
      "Section 1 — Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1 **Getting Started**</span>"
    ]
  },
  {
    "objectID": "chapter-1.html#core-data-wrangling-capabilities",
    "href": "chapter-1.html#core-data-wrangling-capabilities",
    "title": "Chapter 1 Getting Started",
    "section": "Core Data Wrangling Capabilities",
    "text": "Core Data Wrangling Capabilities\n\nSelecting and renaming variables\nCreating new variables\nArranging variables\nJoining data",
    "crumbs": [
      "Section 1 — Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Chapter 1 **Getting Started**</span>"
    ]
  },
  {
    "objectID": "chapter-2.html",
    "href": "chapter-2.html",
    "title": "Chapter 2 Capturing and Analyzing Text Data with Computational Methods",
    "section": "",
    "text": "2.1 Overview\nIn social sciences, analyzing text data is usually considered the “job” of qualitative researchers.Traditionally, qualitative research involves identifying patterns in non-numeric data, and this pattern recognition is typically done manually. This process is time-intensive but can yield rich research results. Traditional methods for analyzing text data involve human coding and can include direct sources (e.g., books, online texts) or indirect sources (e.g., interview transcripts).\nWith the advent of new software, we can capture and analyze text data in ways that were previously not possible. Modern data collection techniques include web scraping, accessing social media APIs, or downloading large online documents. Given the increased size of text data, analysis now requires computational approaches (e.g., dictionary-based, frequency-based, machine learning) that go beyond manual coding. These computational methods allow social scientists to ask new types of research questions, expanding the scope and depth of possible insights.",
    "crumbs": [
      "Section 2 — Traditional Computational Analyses of Educational Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2 **Capturing and Analyzing Text Data with Computational Methods**</span>"
    ]
  },
  {
    "objectID": "chapter-2.html#overview",
    "href": "chapter-2.html#overview",
    "title": "Chapter 2 Capturing and Analyzing Text Data with Computational Methods",
    "section": "",
    "text": "Disclaimer: While resources are available that discuss these analysis methods in depth, this book aims to provide a practical guide for social scientists, using data they will likely encounter. Our goal is to present a “cookbook” for guiding research projects through real-world examples.",
    "crumbs": [
      "Section 2 — Traditional Computational Analyses of Educational Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2 **Capturing and Analyzing Text Data with Computational Methods**</span>"
    ]
  },
  {
    "objectID": "chapter-2.html#accessing-text-data",
    "href": "chapter-2.html#accessing-text-data",
    "title": "Chapter 2 Capturing and Analyzing Text Data with Computational Methods",
    "section": "2.2 Accessing Text Data",
    "text": "2.2 Accessing Text Data\nFor conventional qualitative researcher, text data comes from the usual sources such as interview transcripts or existing documents. Nowadays, however, text can be found and collected in many different ways. For example, social media is rich with text data, likewise for faculty who are teaching course (especially online), every student writing can be seen as a piece of text data. In this section, we will cover a few basic ways of accessing text data. Please note that although we cover some prominent ways, this is by no means an exhaustive list. Therefore, please refer to the additional resources section at the end of the section to dive deeper.\n\n2.2.1 Web Scraping (Unstructured or API)\n\nWhat is Web Scraping?\nWeb scraping refers to the automated process of extracting data from web pages. It is particularly useful when dealing with extensive lists of websites that would be tedious to mine manually. A typical web scraping program follows these steps:\n\nLoads a webpage.\nDownloads the HTML or XML structure.\nIdentifies the desired data.\nConverts the data into a format suitable for analysis, such as a data frame.\n\nIn addition to text, web scraping can also be used to download other content types, such as audio-visual files.\n\n\nIs Web Scraping Legal?\nWeb scraping was common in the early days of the internet, but with the increasing value of data, legal norms have evolved. To avoid legal issues, check the “Terms of Service” for specific permissions on the website, often accessible via “robots.txt” files. Consult legal advice when in doubt.\n\n\nReading a Web Page into R\nOnce permissions are confirmed, the first step in web scraping is to download the webpage’s source code into R, typically using the rvest package by Hadley Wickham.\n\n# Install and load the rvest package\ninstall.packages(\"rvest\")\nlibrary(rvest)\n\nTo demonstrate, we will scrape a simple Wikipedia page. Static pages, which lack interactive elements like JavaScript, are simpler to scrape. You can view the page’s HTML source in your browser by selecting Developer Tools &gt; View Source.\n\n# Load the webpage\nwikipedia_page &lt;- read_html(\"https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000\")\n\n# Verify that the webpage loaded successfully\nwikipedia_page\n\n\n\nParsing HTML\nThe next challenge is extracting specific information from the HTML structure. HTML files have a “tree-like” format, allowing us to target particular sections. Use your browser’s “Developer Tools” to inspect elements and locate the data. Right-click the desired element and select Inspect to view its structure.\nTo isolate data sections within the HTML structure, identify the XPath or CSS selectors. For instance:\n\n# Extract specific section using XPath\nsection_of_wikipedia &lt;- html_node(wikipedia_page, xpath='//*[@id=\"mw-content-text\"]/div/table')\nhead(section_of_wikipedia)\n\nTo convert the extracted section into a data frame, use html_table():\n\n# Convert the extracted data into a table\nhealth_rankings &lt;- html_table(section_of_wikipedia)\nhead(health_rankings[ , (1:2)])  # Display the first two columns\n\n\n\nParsing with CSS Selectors\nFor more complex web pages, CSS selectors can be an alternative to XPath. Tools like Selector Gadget can help identify the required CSS selectors.\nFor example, to scrape event information from Duke University’s main page:\n\n# Load the webpage\nduke_page &lt;- read_html(\"https://www.duke.edu\")\n\n# Extract event information using CSS selector\nduke_events &lt;- html_nodes(duke_page, css=\"li:nth-child(1) .epsilon\")\nhtml_text(duke_events)\n\n\n\nScraping with Selenium\nFor tasks involving interactive actions (e.g., filling search fields), use RSelenium, which enables automated browser operations.\nTo set up Selenium, install the Java SE Development Kit and Docker. Then, start Selenium in R:\n\n# Install and load RSelenium\ninstall.packages(\"RSelenium\")\nlibrary(RSelenium)\n\n# Start a Selenium session\nrD &lt;- rsDriver()\nremDr &lt;- rD$client\nremDr$navigate(\"https://www.duke.edu\")\n\nTo automate data entry, identify the CSS selector for the search box and input the query:\n\n# Find the search box element and enter a query\nsearch_box &lt;- remDr$findElement(using = 'css selector', 'fieldset input')\nsearch_box$sendKeysToElement(list(\"data science\", \"\\uE007\"))  # \"\\uE007\" represents Enter key\n\n\n\nWeb Scraping within a Loop\nTo scrape multiple pages, embed the code within a loop to automate tasks across different URLs. Since each site may have a unique structure, generalized scraping can be time-intensive and error-prone. Implement error handling to manage interruptions.\n\n\nWhen to Use Web Scraping\nWeb scraping is appropriate if:\n\nPage structure is consistent across sites: For example, a government site with date suffixes but a uniform layout.\nManual data collection is prohibitive: For extensive text or embedded tables.\n\nWhen feasible, consider alternatives like APIs or data-entry services (e.g., Amazon Mechanical Turk) for better efficiency and legal compliance.\n\n\nWhat is an API?\nAn Application Programming Interface (API) is a set of protocols that allows computers to communicate and exchange information. A common type is the REST API, where one machine sends a request, and another returns a response. APIs provide standardized access to data, services, and functionalities, making them essential in software development.\n\n\nWhen to Use an API\nAPIs are commonly used for:\n\nIntegrating with Third-Party Services: APIs connect applications to services like payment gateways or social media.\nAccessing Data: APIs retrieve data from systems or databases (e.g., real-time weather data).\nAutomating Tasks: APIs automate processes within applications, such as email marketing.\nBuilding New Applications: APIs allow developers to build new apps or services (e.g., a mapping API for navigation).\nStreamlining Workflows: APIs enable seamless communication and data exchange across systems.\n\n\n\nUsing Reddit API with RedditExtractoR\nReddit is a social media platform featuring a complex network of users and discussions, organized into “subreddits” by topic. RedditExtractoR, an R package, enables data extraction from Reddit to identify trends and analyze interactions.\n\n# Install and load RedditExtractoR\ninstall.packages(\"RedditExtractoR\")\nlibrary(RedditExtractoR)\n\n# Access data from the GenAI subreddit\nGenAI_reddit &lt;- find_thread_urls(subreddit = \"GenAI\", sort_by = \"new\", period = \"day\")\nview(GenAI_reddit)\n\n\n\n\n2.2.2 Audio Transcripts (Zoom, etc.)\nAudio transcripts are a rich source of text data, especially useful for capturing spoken content from meetings, interviews, or webinars. Many platforms, such as Zoom, provide automated transcription services that can be downloaded as text files for analysis. By processing these transcripts, researchers can analyze conversation themes, sentiment, or other linguistic features. Here’s how to access and prepare Zoom transcripts for analysis in R.\n\nKey Steps for Extracting Text Data from Audio Transcripts\n\nAccess the Zoom Transcript\n\nLog in to your Zoom account.\nNavigate to the “Recordings” section.\nSelect the recording you wish to analyze and download the “Audio Transcript” file.\n\nImport the Transcript into R\nOnce the file is downloaded, you can load it into R for analysis. Depending on the file format (usually a .txt file with tab or comma delimiters), use read.table(), read.csv(), or functions from the readr package to load the data.\n\n\n   # Load the transcript into R\n   transcript_data &lt;- read.table(\"path/to/your/zoom_transcript.txt\", sep = \"\\t\", header = TRUE)\n\nAdjust the sep parameter based on the delimiter used in the transcript file (typically \\t for tab-delimited files).\n\nData Cleaning (if necessary)\nClean up the text data to remove unnecessary characters, standardize formatting, and prepare it for further analysis.\n\nRemove Unwanted Characters\nUse gsub() to eliminate special characters and punctuation, keeping only alphanumeric characters and spaces.\n\n\n     # Remove special characters\n     transcript_data$text &lt;- gsub(\"[^a-zA-Z0-9 ]\", \"\", transcript_data$text)\n\n\nConvert Text to Lowercase\nStandardize text to lowercase for consistency in text analysis.\n\n\n\n     # Convert text to lowercase\n     transcript_data$text &lt;- tolower(transcript_data$text)\n\n\n\n\n2.2.3 PDF\nPDF files are a valuable source of text data, often found in research publications, government documents, and industry reports. We’ll explore two main methods for extracting text from PDFs:\n\nExtracting from Local PDF Files: This method involves accessing and parsing text from PDF files stored locally, providing tools and techniques to efficiently retrieve text data from offline documents.\nDownloading and Extracting PDF Files: This approach covers downloading PDFs from online sources and extracting their text. This method is useful for scraping publicly available documents from websites or databases for research purposes.\nPDF Data Extractor (PDE)\nFor more advanced PDF text extraction and processing, you can use the PDF Data Extractor (PDE) package. This package provides tools for extracting text data from complex PDF documents, supporting additional customization options for text extraction. PDE is a R package that easily extracts information and tables from PDF files. The PDE_analyzer_i() performs the sentence and table extraction while the included PDE_reader_i() allows the user-friendly visualization and quick-processing of the obtained results.\n\n\nSteps for Extracting Text from Local PDF Files\n\nInstall and Load the pdftools Package\nStart by installing and loading the pdftools package, which is specifically designed for reading and extracting text from PDF files in R.\n\n\n   install.packages(\"pdftools\")\n   library(pdftools)\n\n\nRead the PDF as a Text File\nUse the pdf_text() function to read the PDF file into R as a text object. This function returns each page as a separate string in a character vector.\n\n\n   txt &lt;- pdf_text(\"path/to/your/file.pdf\")\n\n\nExtract Text from a Specific Page\nTo access a particular page from the PDF, specify the page number in the text vector. For example, to extract text from page 24:\n\n\n   page_text &lt;- txt[24]  # page 24\n\n\nExtract Rows into a List\nIf the page contains a table or structured text, use the scan() function to read each row as a separate element in a list. The textConnection() function converts the page text for processing.\n\n\n   rows &lt;- scan(textConnection(page_text), what = \"character\", sep = \"\\n\")\n\n\nSplit Rows into Cells\nTo further parse each row, split it into cells by specifying the delimiter, such as whitespace (using \"\\\\s+\"). This converts each row into a list of individual cells.\n\n\n   row &lt;- unlist(strsplit(rows[24], \"\\\\s+\"))  # Example with the 24th row\n\n\n\nSteps for Downloading and Extracting Text from PDF Files\n\nDownload the PDF from the Web\nUse the download.file() function to download the PDF file from a specified URL. Set the mode to \"wb\" (write binary) to ensure the file is saved correctly.\n\n\n   link &lt;- paste0(\n     \"http://www.singstat.gov.sg/docs/\",\n     \"default-source/default-document-library/\",\n     \"publications/publications_and_papers/\",\n     \"cop2010/census_2010_release3/\",\n     \"cop2010sr3.pdf\"\n   )\n   download.file(link, \"census2010_3.pdf\", mode = \"wb\")\n\n\nRead the PDF as a Text File\nAfter downloading, read the PDF into R as a text object using the pdf_text() function from the pdftools package. Each page of the PDF will be stored as a string in a character vector.\n\n\n   txt &lt;- pdf_text(\"census2010_3.pdf\")\n\n\nExtract Text from a Specific Page\nAccess the desired page (e.g., page 24) by specifying the page number in the character vector.\n\n\n   page_text &lt;- txt[24]  # Page 24\n\n\nExtract Rows into a List\nUse the scan() function to split the page text into rows, with each row representing a line of text in the PDF. This creates a list where each line from the page is an element.\n\n\n   rows &lt;- scan(textConnection(page_text), what = \"character\", sep = \"\\n\")\n\n\nLoop Through Rows and Extract Data\nStarting from a specific row (e.g., row 7), loop over each row. For each row:\n\nSplit the text by spaces (\"\\\\s+\") using strsplit().\nConvert the result to a vector with unlist().\nIf the third cell in the row is not empty, store the second cell as name and the third cell as total, converting it to a numeric format after removing commas.\n\n\n\n   name &lt;- c()\n   total &lt;- c()\n\n   for (i in 7:length(rows)) {\n     row &lt;- unlist(strsplit(rows[i], \"\\\\s+\"))\n     if (!is.na(row[3])) {\n       name &lt;- c(name, row[2])\n       total &lt;- c(total, as.numeric(gsub(\",\", \"\", row[3])))\n     }\n   }\n\n\n\n\n2.2.4 Survey, Discussions, etc.\nSurveys and discussion posts are valuable sources of text data in social science research, providing insights into participants’ perspectives, opinions, and experiences. These data sources often come from open-ended survey responses, online discussion boards, or educational platforms. Extracting and preparing text data from these sources can reveal recurring themes, sentiment, and other patterns that support both quantitative and qualitative analysis. Below are key steps and code examples for processing text data from surveys and discussions in R.\n\nKey Steps for Processing Survey and Discussion Text Data\n\nLoad the Data\nSurvey and discussion data are typically stored in spreadsheet formats like CSV. Begin by loading this data into R for processing. Here, readr is used for reading CSV files with read_csv().\n\n\n   # Install and load necessary packages\n   install.packages(\"readr\")\n   library(readr)\n   \n   # Load data\n   survey_data &lt;- read_csv(\"path/to/your/survey_data.csv\")\n\n\nExtract Text Columns\nIdentify and isolate the relevant text columns. For example, if the text data is in a column named “Response,” you can create a new vector for analysis.\n\n\n   # Extract text data from the specified column\n   text_data &lt;- survey_data$Response\n\n\nData Cleaning\nPrepare the text data by cleaning it, removing any unnecessary characters, and standardizing the text. This includes removing punctuation, converting text to lowercase, and handling extra whitespace.\n\nRemove Unwanted Characters\nUse gsub() from base R to remove any non-alphanumeric characters, retaining only words and spaces.\n\n\n\n     # Remove special characters\n     text_data &lt;- gsub(\"[^a-zA-Z0-9 ]\", \"\", text_data)\n\n\nConvert to Lowercase\nStandardize the text by converting all characters to lowercase.\n\n\n     # Convert text to lowercase\n     text_data &lt;- tolower(text_data)\n\n\nRemove Extra Whitespace\nRemove any extra whitespace that may be left after cleaning.\n\n\n     # Remove extra spaces\n     text_data &lt;- gsub(\"\\\\s+\", \" \", text_data)\n\n\nTokenization and Word Counting (Optional)\nIf further analysis is needed, such as frequency-based analysis, split the text into individual words (tokenization) or count the occurrence of specific words. Here, dplyr is used to organize the word counts.\n\n\n   # Install and load necessary packages\n   install.packages(\"dplyr\")\n   library(dplyr)\n   \n   # Tokenize and count words\n   word_count &lt;- strsplit(text_data, \" \") %&gt;%\n                 unlist() %&gt;%\n                 table() %&gt;%\n                 as.data.frame()",
    "crumbs": [
      "Section 2 — Traditional Computational Analyses of Educational Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2 **Capturing and Analyzing Text Data with Computational Methods**</span>"
    ]
  },
  {
    "objectID": "chapter-2.html#frequency-based-analysis",
    "href": "chapter-2.html#frequency-based-analysis",
    "title": "Chapter 2 Capturing and Analyzing Text Data with Computational Methods",
    "section": "2.3 Frequency-based Analysis",
    "text": "2.3 Frequency-based Analysis\nIn the following section, we will provide a “recipe” for the social scientist interested in these new methods of analyzing text data to get you from the initial stages of getting the data to running the analyses and the write up. Often left out is also a research question that suits or requires a method. Since we have a data and method-centric approach here, we will backtrack and also provide a research question, so that you can model after it in your own work. Finally, we will provide a sample results and discussions section.\n\n2.3.1 Purpose\nThe purpose of the frequency-based approach is to count the number of words as they appear in a text file, whether it is a collection of tweets, documents, or interview transcripts. This approach aligns with the frequency-coding method (e.g., Saldana) and can supplement human coding by revealing the most/least commonly occurring words, which can then be compared across dependent variables.\n\nCase Study: Frequency-Based Analysis of GenAI USage Guidelines in Higher Education\nAs AI writing tools like ChatGPT become more prevalent, educators are working to understand how best to integrate them within academic settings, while many students and instructors remain uncertain about acceptable use cases. Our research into AI usage guidelines from the top 100 universities in North America aims to identify prominent themes and concerns in institutional policies regarding GenAI.\n\n\n\n2.3.2 Sample Research Questions\nTo investigate the nature of AI use policies within higher education institutions, in this study, our research questions are:\n\nRQ1: What are the most frequently mentioned words in university GenAI writing usage policies?\nRQ2: Which keywords reflect common concerns or focal points related to GenAI writing usage in academic settings?\n\n\n\n2.3.3 Sample Methods\n\nData Source\nThe dataset consists of publicly available AI policy texts from 100 universities(USA), the data has been downloaded and saved as a CSV file for analysis. –we might have to write more here to model how a research should be describing the data from its acquisition to use for research.\n\n\nData Analysis\nIn order to analyze the data we used xyz, —-let’s provide a sample write up for the researcher to adapt.\n\n\n\n\n\n\n2.3.4 Analysis\n\nStep 1: Load Required Libraries\nInstall and load libraries for data processing, visualization, and word cloud generation.\n\n# Install necessary packages if not already installed\n#install.packages(c(\"tibble\", \"dplyr\", \"tidytext\", \"ggplot2\", \"viridis\",\"tm\",wordcloud\" \"wordcloud2\",\"webshot\"))\n\n# Load libraries\nlibrary(readr)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(tm)\nlibrary(wordcloud)\nlibrary(wordcloud2)\nlibrary(webshot)\n\n\n\nStep 2: Load Data\nRead the CSV file containing policy texts from the top 100 universities.\n\n# Load the dataset\nuniversity_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\")\n\n\n\nStep 3: Tokenize Text and Count Word Frequency\nProcess the text data by tokenizing words, removing stop words, and counting word occurrences.\n\n# Tokenize text, remove stop words, and count word frequencies\nword_frequency &lt;- university_policies %&gt;%\n  unnest_tokens(word, Stance) %&gt;%  # Tokenize the 'Policy_Text' column\n  anti_join(stop_words) %&gt;%             # Remove common stop words\n  count(word, sort = TRUE)              # Count and sort words by frequency\nword_frequency\n\n# A tibble: 1,108 × 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 ai            124\n 2 students      120\n 3 chatgpt        80\n 4 tools          73\n 5 academic       68\n 6 policy         35\n 7 assignments    34\n 8 instructors    34\n 9 integrity      33\n10 student        32\n# ℹ 1,098 more rows\n\n\n\n\nStep 4: Create a Word Cloud\nGenerate a word cloud to visualize the frequency of words in a circular shape.\n\n# Create and display the GenAI usage Stance wordcloud\n\nwordcloud(words = word_frequency$word, freq = word_frequency$n, scale = c(4, 0.5), random.order = FALSE, min.freq = 10, colors = brewer.pal(8, \"Dark2\"), rot.per = 0.35)\n\n\n\n\n\n\n\n\n\n\nStep 5: Visualize Top 12 Words in University Policies\nSelect the top 12 most frequent words and create a bar chart to visualize the distribution.\n\n# Select the top 12 words\ntop_words &lt;- word_frequency %&gt;% slice(1:12)\n\n# Generate the bar chart\npolicy_word_chart &lt;- ggplot(top_words, aes(x = reorder(word, n), y = n, fill = word)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(\n    title = \"Top Words in University GenAI Policies\",\n    x = NULL,\n    y = \"Frequency\",\n    caption = \"Source: University AI Policy Text\",\n    fill = \"Word\"\n  ) +\n  scale_fill_viridis(discrete = TRUE) +\n  geom_text(aes(label = n), vjust = 0.5, hjust = -0.1, size = 3)\n\n# Print the bar chart\nprint(policy_word_chart)\n\n\n\n\n\n\n\n\n\n\n\n2.3.5 Results and Discussions\n\nRQ1: What are the most frequently mentioned words in university GenAI writing usage policies?\nThe results of the frequency analysis showed that keywords such as “assignment,” “student,” and “writing” were among the most commonly mentioned terms across AI policies at 100 universities. This emphasis reflects a focus on using AI tools to support student learning and enhance teaching content. The frequent mention of these words suggests that institutions are considering the role of AI in academic assignments and course design, indicating a strategic commitment to integrating AI within educational tasks and student interactions.\nRQ2: Which keywords reflect common concerns or focal points related to GenAI writing usage in academic settings?\nThe analysis of the top 12 frequently mentioned terms highlighted additional focal points, including “tool,” “academic,” “instructor,” “integrity,” and “expectations.” These terms reveal concerns around the ethical use of AI tools, the need for clarity in academic applications, and the central role of instructors in AI policy implementation. Keywords like “integrity” and “expectations” emphasize the importance of maintaining academic standards and setting clear guidelines for AI use in classrooms, while “instructor” underscores the influence faculty members have in shaping AI-related practices. Together, these terms reflect a commitment to transparent policies that support ethical and effective AI integration, enhancing the academic experience for students.",
    "crumbs": [
      "Section 2 — Traditional Computational Analyses of Educational Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2 **Capturing and Analyzing Text Data with Computational Methods**</span>"
    ]
  },
  {
    "objectID": "chapter-2.html#dictionary-based-analysis",
    "href": "chapter-2.html#dictionary-based-analysis",
    "title": "Chapter 2 Capturing and Analyzing Text Data with Computational Methods",
    "section": "2.4 Dictionary-based Analysis",
    "text": "2.4 Dictionary-based Analysis\n\n2.4.1 Purpose\nThe purpose of dictionary-based analysis in text data is to assess the presence of predefined categories, like emotions or sentiments, within the text using lexicons or dictionaries. This approach allows researchers to quantify qualitative aspects, such as positive or negative sentiment, based on specific words that correspond to these categories.\nCase:\nIn this analysis, we examine the stance of 100 universities on the use of GenAI by applying the Bing sentiment dictionary. By analyzing sentiment scores, we aim to identify the general tone in these policies, indicating whether the institutions’ attitudes toward GenAI are predominantly positive or negative.\n\n\n2.4.2 Sample Research Questions\n\nRQ: What is the dominant sentiment expressed in GenAI policy texts across universities, and is it primarily positive or negative?\n\n\n\n2.4.3 Analysis\n\nStep 1: Install and Load Necessary Libraries\nFirst, install and load the required packages for text processing and visualization.\n\n# Install necessary packages if not already installed\n#install.packages(c(\"tidytext\",\"tidyverse\" \"dplyr\", \"ggplot2\", \"tidyr\"))\n\n# Load libraries\nlibrary(tidytext)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\n\nStep 2: Load and Prepare Data(same as 2.3)\nLoad the GenAI policy stance data from a CSV file. Be sure to update the file path as needed. we use the same data as 2.3.\n\n# Load the dataset (replace \"University_GenAI_Policy_Stance.csv\" with the actual file path)\nuniversity_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\")\n# Tokenize text, remove stop words, and count word frequencies\nword_frequency &lt;- university_policies %&gt;%\n  unnest_tokens(word, Stance) %&gt;%  # Tokenize the 'Policy_Text' column\n  anti_join(stop_words) %&gt;%             # Remove common stop words\n  count(word, sort = TRUE)              # Count and sort words by frequency\nword_frequency\n\n\n\nStep 3: Tokenize Text Data and Apply Sentiment Dictionary\nTokenize the policy text data to separate individual words. Then, use the Bing sentiment dictionary to label each word as positive or negative.\n\n# Tokenize 'Stance' column and apply Bing sentiment dictionary\nsentiment_scores &lt;- word_frequency %&gt;%\n  inner_join(get_sentiments(\"bing\")) %&gt;% # Join with Bing sentiment lexicon\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%\n  mutate(sentiment_score = positive - negative) # Calculate net sentiment score\n\nsentiment_scores\n\n# A tibble: 142 × 4\n   word         positive negative sentiment_score\n   &lt;chr&gt;           &lt;int&gt;    &lt;int&gt;           &lt;int&gt;\n 1 honor              18        0              18\n 2 cheating            0       11             -11\n 3 dishonesty          0       10             -10\n 4 guidance            9        0               9\n 5 honesty             7        0               7\n 6 intelligence        7        0               7\n 7 transparent         7        0               7\n 8 violation           0        7              -7\n 9 encourage           6        0               6\n10 difficult           0        5              -5\n# ℹ 132 more rows\n\n\n\n\nStep 4: Create a Density Plot for Sentiment Distribution\nVisualize the distribution of sentiment scores with a density plot, showing the prevalence of positive and negative sentiments across university policies.\n\n# Generate a density plot of sentiment scores\ndensity_plot &lt;- ggplot(sentiment_scores, aes(x = sentiment_score, fill = sentiment_score &gt; 0)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"red\", \"green\"), name = \"Sentiment\",\n                    labels = c(\"Negative\", \"Positive\")) +\n  labs(\n    title = \"Density Plot of University AI Policy Sentiment\",\n    x = \"Sentiment Score\",\n    y = \"Density\",\n    caption = \"Source: University Policy Text\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 20),\n    axis.text = element_text(size = 14),\n    axis.title = element_text(face = \"bold\", size = 16),\n    plot.caption = element_text(size = 12)\n  )\n\n# Print the plot\nprint(density_plot)\n\n\n\n\n\n\n\n\n\n\n\n2.4.4 Results and Discussions\n\nRQ: What is the dominant sentiment expressed in GenAI policy texts across universities, and is it primarily positive or negative?\nThe dictionary-based sentiment analysis reveals the prevailing sentiments in university policies on GenAI usage. Using the Bing lexicon to assign positive and negative scores, the density plot illustrates the distribution of sentiment scores across the 100 institutions.\nThe results indicate a balanced perspective with a slight tendency toward positive sentiment, as reflected by a higher density of positive scores. This analysis provides insights into the varying degrees of acceptance and caution universities adopt in their AI policy frameworks, demonstrating the diverse stances that shape institutional AI guidelines.",
    "crumbs": [
      "Section 2 — Traditional Computational Analyses of Educational Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2 **Capturing and Analyzing Text Data with Computational Methods**</span>"
    ]
  },
  {
    "objectID": "chapter-2.html#clustering-based-analysis",
    "href": "chapter-2.html#clustering-based-analysis",
    "title": "Chapter 2 Capturing and Analyzing Text Data with Computational Methods",
    "section": "2.5 Clustering-Based Analysis",
    "text": "2.5 Clustering-Based Analysis\nClustering-based analysis involves grouping similar text documents or text segments into clusters based on their underlying topics or themes. This approach is particularly useful for identifying dominant themes in text data, such as university AI policy documents.\n\n2.5.1 Purpose\nPurpose: The goal of clustering-based analysis is to uncover latent themes in text data using unsupervised machine learning techniques. Topic modeling is one popular method for clustering documents into groups based on their content.\nCase: Using the GenAI policy texts from 100 universities, we apply Latent Dirichlet\nAllocation (LDA) to identify dominant themes in these policy documents. This analysis will help categorize policies into overarching themes, such as academic integrity, student support, and instructor discretion.\n\n\n2.5.2 Sample Research Questions\n\nRQ1: What are the prominent themes present in university policies regarding GenAI usage Stance?\nRQ2: How do these themes reflect the key concerns or opportunities for intergrating GenAI in higher education?\n\n\n\n2.5.3 Analysis\n\nStep 1: Install and Load Necessary Libraries\nInstall and load the required libraries for text processing and topic modeling.\n\n# Install necessary packages\n#install.packages(c(\"dplyr\", \"tidytext\", \"topicmodels\", \"ggplot2\"))\n\n# Load libraries\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(topicmodels)\nlibrary(ggplot2)\n\n\n\nStep 2: Prepare the Data\nLoad the data and create a document-term matrix (DTM) for topic modeling.\n\n# Load the dataset\nuniversity_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\")\n\n# Tokenize text data and count word frequencies\nword_frequency &lt;- university_policies %&gt;%\n  unnest_tokens(word, Stance) %&gt;%  # same as section 2.3\n  anti_join(stop_words) %&gt;%             # Remove common stop words\n  count(word, sort = TRUE)              # Count and sort words by frequency\nword_frequency\n\n# A tibble: 1,108 × 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 ai            124\n 2 students      120\n 3 chatgpt        80\n 4 tools          73\n 5 academic       68\n 6 policy         35\n 7 assignments    34\n 8 instructors    34\n 9 integrity      33\n10 student        32\n# ℹ 1,098 more rows\n\n# Creating Documents - Word Frequency Matrix\ngpt_dtm &lt;- word_frequency %&gt;%\n  group_by(word) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  ungroup() %&gt;%\n  cast_dtm(document = \"id\", term = \"word\", value = \"n\")\n\n\nlibrary(topicmodels)\nlibrary(ggplot2)\n\n# Define range of k values\nk_values &lt;- c(2, 3, 4, 5)\n\n# Initialize a data frame to store perplexities\nperplexities &lt;- data.frame(k = integer(), perplexity = numeric())\n\n# Calculate perplexity for each k\nfor (k in k_values) {\n  lda_model &lt;- LDA(gpt_dtm, k = k, control = list(seed = 1234))  # Fit LDA model\n  perplexity_score &lt;- perplexity(lda_model, gpt_dtm)            # Calculate perplexity\n  perplexities &lt;- rbind(perplexities, data.frame(k = k, perplexity = perplexity_score))\n}\n\n# Plot perplexity vs number of topics\nggplot(perplexities, aes(x = k, y = perplexity)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Perplexity vs Number of Topics\",\n    x = \"Number of Topics (k)\",\n    y = \"Perplexity\"\n  ) +\n  theme_minimal()\n\n\n\nStep 3: Fit the LDA Model\nFit an LDA model with k = 3 topics.\n\n# Converting document-word frequency matrices to sparse matrices\ngpt_dtm_sparse&lt;- as(gpt_dtm, \"matrix\")\n\n# Fit the LDA model\nlda_model &lt;- LDA(gpt_dtm_sparse, k = 3, control = list(seed = 1234))\n\n# View model results\ngpt_policy_topics_k3 &lt;- tidy(lda_model, matrix = \"beta\")\n\nprint(gpt_policy_topics_k3)\n\n# A tibble: 3,324 × 3\n   topic term        beta\n   &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1     1 ai       0.0408 \n 2     2 ai       0.00593\n 3     3 ai       0.0650 \n 4     1 students 0.0435 \n 5     2 students 0.0635 \n 6     3 students 0.0151 \n 7     1 chatgpt  0.0396 \n 8     2 chatgpt  0.0248 \n 9     3 chatgpt  0.0126 \n10     1 tools    0.0197 \n# ℹ 3,314 more rows\n\n\n\n\nStep 4: Visualize Topics\nExtract the top terms for each topic and visualize them.\n\n# Visualizing top terms for each topic\ngpt_policy_ap_top_terms_k3 &lt;- gpt_policy_topics_k3 %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\ngpt_policy_ap_top_terms_k3 %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\n\n\n\n\n\n\n2.5.4 Results and Discussions\n\nResearch Question 1:\nWhat are the prominent themes present in university policies regarding GenAI usage?\nAnswer:\nThe topic modeling analysis revealed three distinct themes in the university GenAI policies:\n\nTheme 1: Student-Centric Guidelines and Ethical Considerations\n\nKey terms: students, integrity, tools, instructors, assignment\nThis theme emphasizes student usage of GenAI in academic settings, with a focus on ethics (integrity) and guidelines for instructors to manage assignments involving AI tools.\n\nTheme 2: Academic Standards and Faculty Expectations\n\nKey terms: students, academic, faculty, honor, expectations\nThis theme focuses on maintaining academic integrity and clarifying expectations for faculty and students regarding GenAI usage in assignments and assessments.\n\nTheme 3: Policy-Level Governance and Technology Integration\n\nKey terms: ai, tools, policy, learning, generative\nThis theme revolves around institutional policies on AI integration, highlighting broader governance strategies and how generative AI (like GenAI) fits into learning environments.\n\n\n\n\nResearch Question 2:\nHow do these themes reflect the key concerns or opportunities for integrating GenAI in higher education?\nAnswer:\nThe identified themes reflect both concerns and opportunities:\n\nConcerns:\nTheme 1: Highlights the ethical challenges, such as ensuring academic integrity when students use AI tools in their coursework. Institutions are keen on setting clear guidelines for both students and instructors to avoid misuse.\nTheme 2: Underlines the potential for conflict between maintaining academic standards (honor, expectations) and leveraging AI to support learning. This shows a cautious approach to integrating AI while upholding traditional values.\nTheme 3: Raises policy-level questions on AI governance, such as whether existing institutional frameworks are adequate to regulate emerging generative AI technologies.\nOpportunities:\nTheme 1: Presents a chance to redefine how students interact with AI tools to foster responsible and innovative usage, particularly for assignments and creative tasks.\nTheme 2: Encourages collaboration between faculty and administration to develop robust expectations and support systems for integrating AI in the classroom.\nTheme 3: Offers a strategic opportunity for universities to lead in AI adoption by establishing comprehensive policies that guide AI’s role in education and research.\n\n\n\nDiscussion:\nThe topic modeling results suggest that universities are navigating a complex landscape of opportunities and challenges as they incorporate GenAI into academic contexts. While student-centric policies aim to balance innovation with ethical considerations, institutional-level themes signal the need for governance frameworks to ensure responsible AI use. These findings indicate that higher education institutions are positioned to play a pivotal role in shaping the future of generative AI in learning, provided they address the ethical, pedagogical, and policy challenges identified in this analysis.",
    "crumbs": [
      "Section 2 — Traditional Computational Analyses of Educational Data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Chapter 2 **Capturing and Analyzing Text Data with Computational Methods**</span>"
    ]
  },
  {
    "objectID": "chapter-3.html",
    "href": "chapter-3.html",
    "title": "Chapter 3 Social Network Analyses (Relational Data)",
    "section": "",
    "text": "3.1 Overview\nSocial network analysis (SNA) is the process of investigating social structures through the use of networks and graph theory. It is a technique used to map and measure relationships and flows between people, groups, organizations, computers, or other information/knowledge processing entities. SNA can be a useful tool for understanding the team structures, for example, in an online classroom. It can be an additional layer of understanding the outcomes (or predictors) of certain instructional interventions. Used this way SNA can be used to identify patterns and trends in social networks, as well as to understand how these networks operate. Additionally, SNA can be used to predict future behavior in social networks, and to design interventions that aim to improve the functioning of these networks.",
    "crumbs": [
      "Section 2 — Traditional Computational Analyses of Educational Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3 **Social Network Analyses (Relational Data)**</span>"
    ]
  },
  {
    "objectID": "chapter-3.html#accessing-sna-data",
    "href": "chapter-3.html#accessing-sna-data",
    "title": "Chapter 3 Social Network Analyses (Relational Data)",
    "section": "3.2 Accessing SNA Data",
    "text": "3.2 Accessing SNA Data\nSocial Network Analysis (SNA) relies on relational data—information about connections (edges) between entities (nodes) such as students, teachers, or organizations. Compared to traditional survey or tabular data, SNA requires pairwise relational information. In education, this could include “who collaborates with whom,” “who talks to whom,” or digital traces of discussion and collaboration in online platforms.\n\n3.2.1 Types and Sources of SNA Data\nThere are several common sources and structures for SNA data in educational and social science contexts:\n\nSurvey-based Network Data: Collected via roster or name generator questions, e.g., “List the classmates you discuss assignments with.”\nBehavioral/Observational Data: Derived from logs of actual interactions, e.g., forum replies, emails, classroom seating.\nArchival or Digital Trace Data: Extracted from digital platforms such as MOOCs, LMS discussion forums, Slack, Twitter, or Facebook.\nAdministrative/Organizational Data: Information about formal structures such as team membership or co-authorship.\n\nData Structure: Most SNA data are formatted as: - Edge List (two columns: source and target) - Adjacency Matrix (rows and columns are actors; cell values indicate a tie) - Node Attributes (supplementary information about each node, e.g., gender, role)\n\n\n3.2.2 Example 1: Creating a Simple Network from an Edge List\nBelow is an example of constructing a network from a simple CSV edge list. This mirrors typical classroom survey data (“who do you consider your friend in this class?”).\n\n# Install and load the igraph package\ninstall.packages(\"igraph\")\nlibrary(igraph)\n\n# Example: Load an edge list from CSV\nedge_list &lt;- read.csv(\"data/friendship_edges.csv\")\n\n# Create the graph object (directed network)\ng &lt;- graph_from_data_frame(edge_list, directed = TRUE)\n\n# Plot the network\nplot(g, main = \"Friendship Network\")\n\n\n\n3.2.3 Example 2: Generating Network Data from Digital Traces\nMany educational datasets now come from online discussion forums, MOOCs, or LMS systems. For example, the MOOC case study (Kellogg & Edelmann, 2015) uses reply relationships in online courses to construct discussion networks.\n# Suppose you have a data frame with columns: from_user, to_user\nmooc_edges &lt;- read.csv(\"data/mooc_discussion_edges.csv\")\ng_mooc &lt;- graph_from_data_frame(mooc_edges, directed = TRUE)\nplot(g_mooc, main = \"MOOC Discussion Network\")\n\n\n3.2.4 Example 3: Collecting SNA Data via Surveys\nIf you want to collect your own network data:\n\nAsk participants to name or select (from a roster) their friends, collaborators, or contacts.\nCompile responses into an edge list.\nExample survey prompt:\n\n“Please list up to five classmates you seek help from most frequently.”\n\n\nTip:\nSurvey-based SNA is easier to manage with small to medium groups. For larger networks, digital trace or archival data may be more practical.\n\n\n3.2.5 Node Attribute Data\nYou can also load additional data about each node (student, teacher, etc.) to enable richer analyses (e.g., centrality by gender or role).\nnode_attributes &lt;- read.csv(\"data/friendship_nodes.csv\")\n# Add attributes to igraph object\nV(g)$gender &lt;- node_attributes$gender[match(V(g)$name, node_attributes$name)]\n\n\n3.2.6 Further Examples\n\nPublic Datasets:\n\nMOOC Discussion Networks\nAdd Health\nCommon Core Twitter Networks (Supovitz et al.)\n\nSynthetic Data:\n\nR’s igraph package can also generate sample networks for practice:\ng_sample &lt;- sample_gnp(n = 10, p = 0.3)\nplot(g_sample, main = \"Random Sample Network\")\n\n\n\n\n3.2.7 Best Practices and Tips\n\nEthics: Social network data can be sensitive. Protect anonymity and comply with IRB/data use guidelines.\nFormat Consistency: Always clarify whether ties are directed/undirected, binary/weighted, and ensure consistent formatting.\nMissing Data: Especially in survey-based SNA, missing responses can impact network structure and interpretation.\n\n\n\n3.2.8 Summary\nAccessing SNA data involves both careful design (in the case of surveys/observations) and extraction/wrangling (in the case of digital traces or archival records). The choice of data source and structure will directly influence the kinds of questions you can answer with SNA.\n\nRecommended Reading:\n\nBorgatti, S. P., Everett, M. G., & Johnson, J. C. (2018). Analyzing Social Networks (2nd ed). SAGE.\nKellogg, S., & Edelmann, A. (2015). Massive open online course discussion forums as networks.",
    "crumbs": [
      "Section 2 — Traditional Computational Analyses of Educational Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3 **Social Network Analyses (Relational Data)**</span>"
    ]
  },
  {
    "objectID": "chapter-3.html#network-management-measurement-in-social-network-analysis",
    "href": "chapter-3.html#network-management-measurement-in-social-network-analysis",
    "title": "Chapter 3 Social Network Analyses (Relational Data)",
    "section": "3.3 Network Management & Measurement in Social Network Analysis",
    "text": "3.3 Network Management & Measurement in Social Network Analysis\n\n3.3.1 Purpose + Case\nPurpose: This section demonstrates how to manage, measure, and visualize large-scale discussion networks from online professional development settings. Through this real-world example, we guide readers in loading relational data, constructing a directed network, and conducting a suite of essential SNA measures. The focus is on classroom- and course-level online discussions, which are representative of many contemporary educational and research settings.\nCase Study: The case data comes from two cohorts of an online professional development program (“DLT1” and “DLT2”). Each cohort’s discussion data includes (a) edge list data capturing who replied to whom, and (b) node/actor attributes describing roles (e.g., facilitator, expert). These data allow us to reconstruct and analyze the full structure of communication in two authentic online learning communities.\n\n\n3.3.2 Sample Research Questions\n\nRQ1: What is the overall structure of interaction in each online discussion cohort? Are they densely connected, or fragmented?\nRQ2: Who are the most central or influential actors in the network? How do facilitators or experts compare with regular participants?\nRQ3: To what extent are ties reciprocated (mutual) and how cohesive are the networks?\nRQ4: How do the network properties (e.g., density, reciprocity, clustering) compare between cohorts?\n\n\n\n3.3.3 Analysis\n\nStep 1: Install and Load Required Packages\n\n# Install and load necessary libraries\n#install.packages(c(\"tidygraph\", \"ggraph\", \"readr\", \"janitor\"))\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(readr)\nlibrary(janitor)\nlibrary(igraph)\nlibrary(dplyr)\n\n\n\nStep 2: Import and Clean Data：Load Edges and Node Attributes for DLT1:\n\n# Load edge list (who replied to whom)\ndlt1_ties &lt;- read_csv(\"data/dlt1-edges.csv\", \n  col_types = cols(Sender = col_character(), \n                   Receiver = col_character(), \n                   `Category Text` = col_skip(), \n                   `Comment ID` = col_character(), \n                   `Discussion ID` = col_character())) |&gt; \n  clean_names()\n\n# Load node attributes (participant roles, etc.)\ndlt1_actors &lt;- read_csv(\"data/dlt1-nodes.csv\", \n  col_types = cols(UID = col_character(), \n                   Facilitator = col_character(), \n                   expert = col_character(), \n                   connect = col_character())) |&gt; \n  clean_names()\n\nhead(dlt1_ties)\n\n# A tibble: 6 × 9\n  sender receiver timestamp discussion_title discussion_category parent_category\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;               &lt;chr&gt;          \n1 360    444      4/4/13 1… Most important … Group N             Units 1-3 Disc…\n2 356    444      4/4/13 1… Most important … Group D-L           Units 1-3 Disc…\n3 356    444      4/4/13 1… DLT Resources—C… Group D-L           Units 1-3 Disc…\n4 344    444      4/4/13 1… Most important … Group O-T           Units 1-3 Disc…\n5 392    444      4/4/13 1… Most important … Group U-Z           Units 1-3 Disc…\n6 219    444      4/4/13 1… Most important … Group M             Units 1-3 Disc…\n# ℹ 3 more variables: discussion_identifier &lt;chr&gt;, comment_id &lt;chr&gt;,\n#   discussion_id &lt;chr&gt;\n\nhead(dlt1_actors)\n\n# A tibble: 6 × 13\n  uid   facilitator role1  experience experience2 grades location region country\n  &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  \n1 1     0           libme…          1 6 to 10     secon… VA       South  US     \n2 2     0           class…          1 6 to 10     secon… FL       South  US     \n3 3     0           distr…          2 11 to 20    gener… PA       North… US     \n4 4     0           class…          2 11 to 20    middle NC       South  US     \n5 5     0           other…          3 20+         gener… AL       South  US     \n6 6     0           class…          1 4 to 5      gener… AL       South  US     \n# ℹ 4 more variables: group &lt;chr&gt;, gender &lt;chr&gt;, expert &lt;chr&gt;, connect &lt;chr&gt;\n\n\n\n\nStep 3: Construct and Explore the Network\n\n# Build the directed network graph (nodes: uid, edges: sender-&gt;receiver)\ndlt1_network &lt;- tbl_graph(\n  edges = dlt1_ties,\n  nodes = dlt1_actors,\n  node_key = \"uid\",\n  directed = TRUE\n)\n\n# Overview of the network\ndlt1_network\n\n# A tbl_graph: 445 nodes and 2529 edges\n#\n# A directed multigraph with 4 components\n#\n# Node Data: 445 × 13 (active)\n   uid   facilitator role1 experience experience2 grades location region country\n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  \n 1 1     0           libm…          1 6 to 10     secon… VA       South  US     \n 2 2     0           clas…          1 6 to 10     secon… FL       South  US     \n 3 3     0           dist…          2 11 to 20    gener… PA       North… US     \n 4 4     0           clas…          2 11 to 20    middle NC       South  US     \n 5 5     0           othe…          3 20+         gener… AL       South  US     \n 6 6     0           clas…          1 4 to 5      gener… AL       South  US     \n 7 7     0           inst…          2 11 to 20    gener… SD       Midwe… US     \n 8 8     0           spec…          1 6 to 10     secon… BE       Inter… BE     \n 9 9     0           clas…          1 6 to 10     middle NC       South  US     \n10 10    0           scho…          2 11 to 20    middle NC       South  US     \n# ℹ 435 more rows\n# ℹ 4 more variables: group &lt;chr&gt;, gender &lt;chr&gt;, expert &lt;chr&gt;, connect &lt;chr&gt;\n#\n# Edge Data: 2,529 × 9\n   from    to timestamp    discussion_title  discussion_category parent_category\n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;             &lt;chr&gt;               &lt;chr&gt;          \n1   360   444 4/4/13 16:32 Most important c… Group N             Units 1-3 Disc…\n2   356   444 4/4/13 18:45 Most important c… Group D-L           Units 1-3 Disc…\n3   356   444 4/4/13 18:47 DLT Resources—Co… Group D-L           Units 1-3 Disc…\n# ℹ 2,526 more rows\n# ℹ 3 more variables: discussion_identifier &lt;chr&gt;, comment_id &lt;chr&gt;,\n#   discussion_id &lt;chr&gt;\n\n# Output: 445 nodes, 2529 edges, 4 components (directed multigraph)\n\n\n\nStep 4: Basic Visualization\n\n# Quick overview plot (stress layout by default)\nautograph(dlt1_network)\n\n\n\n\n\n\n\n# Custom visualization with colors and centrality\nggraph(dlt1_network, layout = \"fr\") +\n  geom_edge_link(alpha = .2) +\n  geom_node_point(aes(color = role1, size = local_size())) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\n\n\n\nStep 5: Network Size and Centralization\n\n# Number of nodes and edges\ngorder(dlt1_network)   # 445\n\n[1] 445\n\ngsize(dlt1_network)    # 2529\n\n[1] 2529\n\n# Degree centrality (all, in, out)\ndeg_all &lt;- centr_degree(dlt1_network, mode = \"all\")$res\ndeg_in  &lt;- centr_degree(dlt1_network, mode = \"in\")$res\ndeg_out &lt;- centr_degree(dlt1_network, mode = \"out\")$res\n# Centralization\ncentr_degree(dlt1_network, mode = \"all\")$centralization  # 0.64\n\n[1] 0.6429242\n\ncentr_degree(dlt1_network, mode = \"in\")$centralization   # 1.06\n\n[1] 1.05702\n\ncentr_degree(dlt1_network, mode = \"out\")$centralization  # 0.23\n\n[1] 0.2259389\n\n\n\n\nStep 6: Attach and Visualize Node Centrality\n\n# Add in-degree centrality as node attribute\ndlt1_network &lt;- dlt1_network |&gt;\n  activate(nodes) |&gt;\n  mutate(in_degree = centrality_degree(mode = \"in\"))\n\n# Plot, sizing nodes by in-degree\nggraph(dlt1_network) +\n  geom_edge_link(alpha = .2) +\n  geom_node_point(aes(size = in_degree, color = role1)) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\n\n\n\nStep 7: Network Density, Reciprocity, Clustering, Distance\n\n# Density\nedge_density(dlt1_network)      # 0.013 (sparse network)\n\n[1] 0.01279988\n\n# Reciprocity\nreciprocity(dlt1_network)       # 0.20 (20% of ties are reciprocated)\n\n[1] 0.1997544\n\n# Add reciprocated edge attribute and plot\ndlt1_network &lt;- dlt1_network |&gt;\n  activate(edges) |&gt;\n  mutate(reciprocated = edge_is_mutual())\nggraph(dlt1_network) +\n  geom_node_point(aes(size = in_degree)) +\n  geom_edge_link(aes(color = reciprocated), alpha = .2) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\n# Clustering (transitivity/global)\ntransitivity(dlt1_network)      # 0.089\n\n[1] 0.08880774\n\n# Network diameter (longest shortest path) & average distance\ndiameter(dlt1_network)          # 8\n\n[1] 8\n\nmean_distance(dlt1_network)     # 3.03\n\n[1] 3.030694\n\n\n\n\nStep 8:Repeat for DLT2\n\n# Step 8: Repeat for DLT2\n\n# 1. Load the DLT2 edge and node data\ndlt2_ties &lt;- read_csv(\"data/dlt2-edges.csv\", \n  col_types = cols(Sender = col_character(), \n                   Receiver = col_character(), \n                   `Category Text` = col_skip(), \n                   `Comment ID` = col_character(), \n                   `Discussion ID` = col_character())) |&gt; \n  clean_names()\n\ndlt2_actors &lt;- read_csv(\"data/dlt2-nodes.csv\", \n  col_types = cols(UID = col_character(), \n                   Facilitator = col_character(), \n                   expert = col_character(), \n                   connect = col_character())) |&gt; \n  clean_names()\n\n# 2. Construct the directed network\ndlt2_network &lt;- tbl_graph(\n  edges = dlt2_ties,\n  nodes = dlt2_actors,\n  node_key = \"uid\",\n  directed = TRUE\n)\n\n# 3. Basic network properties\nnum_nodes &lt;- gorder(dlt2_network)   # Number of nodes\nnum_edges &lt;- gsize(dlt2_network)    # Number of edges\n\n# 4. Degree centrality (overall, in, out)\ndeg_all &lt;- centr_degree(dlt2_network, mode = \"all\")$res\ndeg_in  &lt;- centr_degree(dlt2_network, mode = \"in\")$res\ndeg_out &lt;- centr_degree(dlt2_network, mode = \"out\")$res\n\n# Centralization values\ncentr_all &lt;- centr_degree(dlt2_network, mode = \"all\")$centralization\ncentr_in  &lt;- centr_degree(dlt2_network, mode = \"in\")$centralization\ncentr_out &lt;- centr_degree(dlt2_network, mode = \"out\")$centralization\n\n# 5. Attach centrality as a node attribute\ndlt2_network &lt;- dlt2_network |&gt;\n  activate(nodes) |&gt;\n  mutate(in_degree = centrality_degree(mode = \"in\"))\n\n# 6. Visualize the network\nggraph(dlt2_network, layout = \"fr\") +\n  geom_edge_link(alpha = .2) +\n  geom_node_point(aes(color = role, size = local_size())) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\n# 7. Density, reciprocity, clustering, distances\ndensity &lt;- edge_density(dlt2_network)\nrecip   &lt;- reciprocity(dlt2_network)\ndlt2_network &lt;- dlt2_network |&gt;\n  activate(edges) |&gt;\n  mutate(reciprocated = edge_is_mutual())\nggraph(dlt2_network) +\n  geom_node_point(aes(size = in_degree)) +\n  geom_edge_link(aes(color = reciprocated), alpha = .2) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\ntrans   &lt;- transitivity(dlt2_network)\ndiam    &lt;- diameter(dlt2_network)\nmean_d  &lt;- mean_distance(dlt2_network)\n\n# 8. Print summary statistics\ncat(\"DLT2 Network Stats:\\n\")\n\nDLT2 Network Stats:\n\ncat(\"Nodes:\", num_nodes, \"Edges:\", num_edges, \"\\n\")\n\nNodes: 492 Edges: 2584 \n\ncat(\"Degree Centralization (all/in/out):\", centr_all, centr_in, centr_out, \"\\n\")\n\nDegree Centralization (all/in/out): 0.5311161 0.8650671 0.3273889 \n\ncat(\"Density:\", density, \"Reciprocity:\", recip, \"\\n\")\n\nDensity: 0.0106966 Reciprocity: 0.2500977 \n\ncat(\"Transitivity:\", trans, \"Diameter:\", diam, \"Mean Distance:\", mean_d, \"\\n\")\n\nTransitivity: 0.1248291 Diameter: 8 Mean Distance: 3.03815 \n\n\n\n\n\n3.3.4 Results and Discussion\n\nRQ1: What is the overall structure of interaction in each cohort?\n\nDLT1 consists of 445 nodes (participants) and 2529 edges (directed interactions).\nDLT2 has 492 nodes and 2584 edges.\nBoth networks are large and sparse:\n\nDensity: DLT1 = 0.013, DLT2 = 0.011\nInterpretation: Only about 1–1.3% of all possible connections exist—typical for online discussion networks where not every participant interacts with every other.\n\nBoth networks are multi-component (several disconnected groups), but most participants are included in the main giant component.\nThe diameter (longest shortest path) is 8 for both cohorts, and the average shortest path length is about 3.03 (DLT1) and 3.04 (DLT2), indicating that on average, any participant is just 3 steps away from any other in the largest group.\nInterpretation: Information or discussion threads can reach most participants with only a few hops, but overall engagement is selective rather than comprehensive.\n\n\n\nRQ2: Who are the most central or influential actors?\n\nCentrality (degree, in-degree, out-degree) analyses show a right-skewed distribution: most participants have low centrality, but a small subset are highly connected.\nIn both DLT1 and DLT2, facilitators and a handful of highly active participants emerge as hubs—they initiate and/or receive a disproportionate number of interactions.\n\nFor DLT1, degree centralization (all): 0.64 (in: 1.06, out: 0.23)\nFor DLT2, degree centralization (all): 0.53 (in: 0.87, out: 0.33)\n\nVisualization: Network plots with node size proportional to in-degree clearly highlight these central actors.\nInterpretation: These key individuals (often facilitators) play critical roles in steering discussion, providing feedback, and potentially keeping less active members engaged.\n\n\n\nRQ3: Are ties reciprocated?\n\nReciprocity (proportion of mutual connections):\n\nDLT1: 0.20 (20% of ties are reciprocated)\nDLT2: 0.25 (25% reciprocated)\n\nInterpretation: Most interactions are one-way (e.g., a reply that does not receive a response), but a substantial fraction are mutual—possibly reflecting peer-to-peer conversations or ongoing exchanges. In online learning contexts, this suggests a mix of broadcasting (one-to-many) and genuine dialog (two-way).\n\n\n\nRQ4: How cohesive are the networks?\n\nTransitivity/Clustering coefficient (probability that two connected nodes’ neighbors are also connected):\n\nDLT1: 0.089\nDLT2: 0.125\n\nInterpretation: Triadic closure is low—there are few closed triangles, so close-knit groups (where “my friend is also your friend”) are rare. The network structure is more “hub-and-spoke” than “cliquish.”\nDiameter: 8 for both, showing that even the furthest nodes can be reached in 8 steps.\nMean distance: ~3.0, so participants are relatively close to each other in the main component.\n\nComparison Across Cohorts\n\nDLT2 is slightly larger (more participants and interactions), but the structural properties—density, centralization, reciprocity, clustering, and path lengths—are all quite similar.\nMinor variations (e.g., higher reciprocity and clustering in DLT2) could reflect differences in facilitation style, cohort engagement, or participant composition.\nInterpretation: Both cohorts exhibit classic patterns for large-scale online educational discussions—a small number of central actors drive most of the interaction, the networks are sparse but efficiently connected, and genuine dialogue is present but not universal.\n\nEducational Implications\n\nFor educators and instructional designers:\nThese findings suggest that a small group of highly active facilitators or students are critical to fostering interaction. Encouraging more distributed engagement (for example, through peer response requirements or rotating leadership) may enhance network cohesion and learning opportunities.\nFor researchers:\nUnderstanding who occupies central positions and the overall structure of discussion networks can inform interventions to support isolated participants, promote reciprocity, and create more connected learning communities.\n\nSummary:\nThrough these SNA measures, we have shown how to reconstruct, visualize, and interpret the structure of large-scale online discussion networks. The approach enables identification of core communicators, understanding of participation patterns, and empirical comparison across cohorts or interventions. This “cookbook” can be adapted to other online learning or collaborative contexts.\n&gt; Note: This analysis is based on real-world data from online professional development courses. The methods and findings can be generalized to other educational settings where social networks play a role in learning and collaboration.",
    "crumbs": [
      "Section 2 — Traditional Computational Analyses of Educational Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3 **Social Network Analyses (Relational Data)**</span>"
    ]
  },
  {
    "objectID": "chapter-3.html#case-study-hashtag-common-core",
    "href": "chapter-3.html#case-study-hashtag-common-core",
    "title": "Chapter 3 Social Network Analyses (Relational Data)",
    "section": "3.4 Case Study: Hashtag Common Core",
    "text": "3.4 Case Study: Hashtag Common Core\n\n3.4.1 Purpose & Case\nThe purpose of this case study is to demonstrate the application of social network analysis (SNA) in a real-world policy context: the heated national debate over the Common Core State Standards (CCSS) as it played out on Twitter. Drawing on the work of Supovitz, Daly, del Fresno, and Kolouch, the #COMMONCORE Project provides a vivid example of how social media-enabled networks shape educational discourse and policy.\nThis case focuses on: - Identifying key actors (“transmitters,” “transceivers,” and “transcenders”) and measuring their influence, - Detecting subgroups/factions within the conversation, - Exploring how sentiment about the Common Core varies across network positions, - Demonstrating network wrangling, visualization, and analysis using real tweet data.\n\nData Source\nData was collected from Twitter’s public API using keywords/hashtags related to the Common Core (e.g., #commoncore, ccss, stopcommoncore). The dataset includes user names, tweets, mentions, retweets, and relevant timestamps from a sample week. Only public tweets are included, and user privacy is respected.\n\n\n\n3.4.2 Sample Research Questions\n\nRQ1: Who are the “transmitters,” “transceivers,” and “transcenders” in the Common Core Twitter network?\nRQ2: What subgroups or factions exist within the network, and how are they structured?\nRQ3: How does sentiment about the Common Core vary across actors and subgroups?\nRQ4: What other patterns of communication (e.g., centrality, clique formation, isolates) characterize this network?\n\n\n\n3.4.3 Analysis\n\nStep 1: Load Required Packages\n\nlibrary(tidyverse) \nlibrary(tidygraph) \nlibrary(ggraph) \nlibrary(skimr) \nlibrary(igraph) \nlibrary(tidytext) \nlibrary(vader)\n\n\n\nStep 2: Data Import and Wrangling\n\n# Import tweet data (edgelist format: sender, receiver, timestamp, text)\nccss_tweets &lt;- read_csv(\"data/ccss-tweets.csv\")\n\n# Prepare the edgelist (extract sender, mentioned users, and tweet text)\nties_1 &lt;- ccss_tweets %&gt;%\n  relocate(sender = screen_name, target = mentions_screen_name) %&gt;%\n  select(sender, target, created_at, text)\n\n# Unnest receiver to handle multiple mentions per tweet\nties_2 &lt;- ties_1 %&gt;%\n  unnest_tokens(input = target,\n                output = receiver,\n                to_lower = FALSE) %&gt;%\n  relocate(sender, receiver)\n\n# Remove tweets without mentions to focus on direct connections\nties &lt;- ties_2 %&gt;%\n  drop_na(receiver)\n\n# Save for reproducibility\nwrite_csv(ties, \"data/ccss-edgelist.csv\")\n\n# Build nodelist\nactors_1 &lt;- ties %&gt;%\n  select(sender, receiver) %&gt;%\n  pivot_longer(cols = c(sender,receiver))\n\nactors &lt;- actors_1 %&gt;%\n  select(value) %&gt;%\n  rename(actors = value) %&gt;%\n  distinct()\n\nwrite_csv(actors, \"data/ccss-nodelist.csv\")\n\n\n\nStep 3: Create Network Object\n\nccss_network &lt;- tbl_graph(edges = ties,\n                          nodes = actors,\n                          directed = TRUE)\nccss_network\n\n# A tbl_graph: 46 nodes and 42 edges\n#\n# A directed multigraph with 14 components\n#\n# Node Data: 46 × 1 (active)\n   actors        \n   &lt;chr&gt;         \n 1 DistanceLrnBot\n 2 k12movieguides\n 3 WEquilSchool  \n 4 JoeWEquil     \n 5 SumayLu       \n 6 fluttbot      \n 7 BodShameless  \n 8 Math          \n 9 ozsultan      \n10 sfchronicle   \n# ℹ 36 more rows\n#\n# Edge Data: 42 × 4\n   from    to created_at          text                                          \n  &lt;int&gt; &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;                                         \n1     1     2 2021-06-28 09:53:54 \"#Luca Movie Guide | Worksheet | Questions | …\n2     3     4 2021-06-28 02:32:59 \"Why public schools should focus more on buil…\n3     3     3 2021-06-28 02:32:59 \"Why public schools should focus more on buil…\n# ℹ 39 more rows\n\n\n\n\nStep 4: Network Structure – Components, Cliques, and Communities\n\nComponents\n\nIdentify weak and strong components (connected subgroups):\n\n\n\n    ccss_network &lt;- ccss_network |&gt;\n      activate(nodes) |&gt;\n      mutate(weak_component = group_components(type = \"weak\"),\n             strong_component = group_components(type = \"strong\"))\n    # View component sizes\n    ccss_network |&gt;\n      as_tibble() |&gt;\n      group_by(weak_component) |&gt;\n      summarise(size = n()) |&gt;\n      arrange(desc(size))\n\n# A tibble: 14 × 2\n   weak_component  size\n            &lt;int&gt; &lt;int&gt;\n 1              1    14\n 2              2     6\n 3              3     4\n 4              4     3\n 5              5     3\n 6              6     2\n 7              7     2\n 8              8     2\n 9              9     2\n10             10     2\n11             11     2\n12             12     2\n13             13     1\n14             14     1\n\n\n\nCliques\n\nIdentify fully connected subgroups (if any):\n\n\n\n    clique_num(ccss_network)\n\n[1] 4\n\n    cliques(ccss_network, min = 3)\n\n[[1]]\n+ 3/46 vertices, from f65493d:\n[1] 4 5 6\n\n[[2]]\n+ 3/46 vertices, from f65493d:\n[1] 39 40 41\n\n[[3]]\n+ 3/46 vertices, from f65493d:\n[1] 3 4 6\n\n[[4]]\n+ 4/46 vertices, from f65493d:\n[1] 3 4 5 6\n\n[[5]]\n+ 3/46 vertices, from f65493d:\n[1] 3 4 5\n\n[[6]]\n+ 3/46 vertices, from f65493d:\n[1] 3 5 6\n\n\n\nCommunities\n\nDetect densely connected communities using edge betweenness:\n\n\n\n    ccss_network &lt;- ccss_network |&gt;\n      morph(to_undirected) |&gt;\n      activate(nodes) |&gt;\n      mutate(sub_group = group_edge_betweenness()) |&gt;\n      unmorph()\n    ccss_network |&gt;\n      as_tibble() |&gt;\n      group_by(sub_group) |&gt;\n      summarise(size = n()) |&gt;\n      arrange(desc(size))\n\n# A tibble: 16 × 2\n   sub_group  size\n       &lt;int&gt; &lt;int&gt;\n 1         1    10\n 2         2     6\n 3         3     4\n 4         4     3\n 5         5     3\n 6         6     2\n 7         7     2\n 8         8     2\n 9         9     2\n10        10     2\n11        11     2\n12        12     2\n13        13     2\n14        14     2\n15        15     1\n16        16     1\n\n\n\n\nStep 5: Egocentric Analysis – Centrality & Key Actors\n\nccss_network &lt;- ccss_network |&gt;\n  activate(nodes) |&gt;\n  mutate(\n    size = local_size(),\n    in_degree = centrality_degree(mode = \"in\"),\n    out_degree = centrality_degree(mode = \"out\"),\n    closeness = centrality_closeness(),\n    betweenness = centrality_betweenness()\n  )\n\n# Identify top actors by out_degree (transmitters), in_degree (transceivers), and both (transcenders)\ntop_transmitters &lt;- ccss_network %&gt;% as_tibble() %&gt;% arrange(desc(out_degree)) %&gt;% head(5)\ntop_transceivers &lt;- ccss_network %&gt;% as_tibble() %&gt;% arrange(desc(in_degree)) %&gt;% head(5)\ntop_transcenders &lt;- ccss_network %&gt;% as_tibble() %&gt;%\n  filter(out_degree &gt; quantile(out_degree, 0.9) & in_degree &gt; quantile(in_degree, 0.9))\n\n\n\nStep 6: Visualize the Network\n\nggraph(ccss_network, layout = \"fr\") +\n  geom_node_point(aes(size = out_degree, color = out_degree)) +\n  geom_edge_link(alpha = .2) +\n  theme_graph()+\n  theme(text = element_text(family = \"sans\"))\n\n\n\n\n\n\n\n\n\n\nStep 7: Sentiment Analysis (Optional)\nIf you want to analyze sentiment as in the original #COMMONCORE study:\n\nlibrary(vader)\nvader_ccss &lt;- vader_df(ccss_tweets$text)\n mean(vader_ccss$compound)\n\n[1] 0.08668182\n\n vader_ccss_summary &lt;- vader_ccss %&gt;%\n   mutate(sentiment = case_when(\n     compound &gt;= 0.05 ~ \"positive\",\n     compound &lt;= -0.05 ~ \"negative\",\n     TRUE ~ \"neutral\"\n   )) %&gt;%\n   count(sentiment)\n\n\n\n\n3.4.4 Results and Discussion\n\nRQ1: Who are the “transmitters,” “transceivers,” and “transcenders” in the Common Core Twitter network?\n\nTransmitters (high out-degree):\nThe user SumayLu stands out as the top transmitter, initiating 8 outgoing ties (mentions/retweets), followed by DouglasHolt... (5), WEquilSchool (3), fluttbot (3), and JoeWEquil (2). These users are the most active in broadcasting or mentioning others within the network.\nTransceivers (high in-degree):\nThe most-mentioned users are WEquilSchool and SumayLu (in-degree = 3), JoeWEquil (2), Tech4Learni... (2), and LASER_Insti... (2). These individuals receive the most attention from other actors—potential focal points in conversations.\nTranscenders (high in-degree and out-degree):\nOnly two users—WEquilSchool (in-degree = 3, out-degree = 3) and SumayLu (in-degree = 3, out-degree = 8)—simultaneously act as hubs for both sending and receiving communication. These “bridging” actors may serve as key facilitators or connectors in the discourse.\n\n\n\nRQ2: What subgroups or factions exist in the network?\n\nComponent analysis shows a fragmented network:\n\nThere are 14 weakly connected components, the largest containing 14 users, and several small groups or dyads (many with just 2–3 members).\nThis fragmentation suggests limited overall cohesion, with multiple parallel or isolated conversations occurring.\n\nClique analysis reveals:\n\nFour cliques (fully connected subgroups) of size 3 or 4—e.g., one 4-person clique involving nodes 3, 4, 5, and 6, and several overlapping 3-person cliques. This indicates pockets of tight-knit interaction, but such groups are rare relative to the size of the network.\n\nCommunity detection using edge betweenness identifies 16 subgroups, generally aligning with the component structure. The largest subgroup has 10 members, with most others much smaller.\n\n\n\nRQ3: What is the overall sentiment in the network?\n\nVADER sentiment analysis of tweet content yields:\n\nAn average sentiment score (compound) of 0.09 (slightly positive), indicating that, despite the policy controversy, the sampled tweets were, on balance, more positive than negative.\nWhen tweets are classified into categories:\n\nA mix of positive, neutral, and negative tweets is observed, with positive tweets slightly outnumbering negatives.\n\nThis suggests the debate, at least in this time slice, included advocacy and constructive dialogue, not only criticism or negativity.\n\n\n\n\nRQ4: What other patterns of communication (e.g., centrality, clique formation, isolates) characterize this network?\n\nCentrality Patterns:\nThe network displays a classic “star” structure in its largest component. Two users, SumayLu and WEquilSchool, stand out with high out-degree and in-degree centrality, respectively. Most other users have very low degree values (often 0 or 1), meaning they are peripheral, engaging in few interactions.\n\nTransmitters (high out-degree): e.g., SumayLu (8 outgoing ties), DouglasHolt... (5).\nTransceivers (high in-degree): e.g., WEquilSchool, SumayLu (both in-degree = 3).\nTranscenders (both high in- and out-degree): rare—only WEquilSchool and SumayLu meet this criterion in this sample.\n\nClique Formation:\nClique analysis revealed 4 cliques (fully connected subgroups) of size 3 or more, with one larger clique (size 4) and several overlapping smaller cliques. However, cliques are rare and limited in size—most communication occurs outside of dense subgroups.\nIsolates and Components:\nThe network has 14 weak components—many of them tiny. Several users are isolates or part of isolated dyads and triads, meaning they are disconnected from the main conversation or only loosely connected. This points to a lack of broad, network-wide cohesion.\nCommunity Structure:\nEdge betweenness community detection found 16 subgroups, typically matching up with the component structure: most subgroups are very small (2–3 nodes), while the largest subgroup consists of 10 users.\nSummary:\nCommunication in this network is characterized by:\n\nStrong centralization around a small number of users (hubs);\nSparse and fragmented structure with many small, disconnected components;\nLimited clique formation—pockets of tightly connected users exist but are rare;\nNumerous isolates—users who are only weakly or not at all connected to the core discussion.\n\n\n\n\nDiscussion\nThis analysis of the Common Core Twitter conversation reveals a sparse and fragmented network structure. The debate is distributed across many small subgroups, with only one moderately sized component (14 members). Within this landscape:\n\nKey actors such as SumayLu and WEquilSchool serve as both broadcasters and focal points of attention (“transcenders”), but most users are peripheral, interacting minimally.\nCliques and communities are few and small, underscoring the lack of broad cohesion. Most interactions happen within micro-groups rather than across the entire network.\nSentiment is, perhaps surprisingly, slightly positive on average. This may reflect the presence of advocacy groups, promotional messaging, or simply a lack of highly negative engagement during the observed period.\n\nImplications:\nThe findings illustrate classic social network phenomena in online policy debate: - Most users are only lightly involved, and only a select few drive discussion or receive significant attention. - Communication is siloed, with many small isolated groups and minimal bridging between them. - Sentiment analysis offers nuance: while public debates may be assumed to be contentious, the prevailing tone can still be balanced or even positive in certain time slices.\nFor researchers and practitioners, this means that: - Identifying and engaging “transcenders” is essential for bridging subgroups and spreading information. - Interventions or outreach should consider the network’s fragmentation—broader influence may require engaging multiple small groups individually rather than targeting a single “core.” - Combining SNA with text/sentiment analysis gives a fuller picture: not just who is talking, but how and with what tone.\nFuture analysis could track changes in sentiment and connectivity over time, or compare subgroups for differences in message tone and network position.\nReferences\n\nSupovitz, J., Daly, A.J., del Fresno, M., & Kolouch, C. (2017). #commoncore Project. Retrieved from http://www.hashtagcommoncore.com\nCarolan, B.V. (2014). Social Network Analysis and Education: Theory, Methods & Applications. Sage.\nSilge, J., & Robinson, D. (2017). Text Mining with R: A Tidy Approach. O’Reilly.",
    "crumbs": [
      "Section 2 — Traditional Computational Analyses of Educational Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Chapter 3 **Social Network Analyses (Relational Data)**</span>"
    ]
  },
  {
    "objectID": "chapter-4.html",
    "href": "chapter-4.html",
    "title": "Chapter 4 Secondary Analysis of Big Data (Numeric Data)",
    "section": "",
    "text": "4.1 Overview\nAbstract: This section reviews how to access data that is primarily numeric/quantitative in nature, but from a different source and of a different nature than the data typically used by social scientists. Example data sets include international or national large-scale assessments (e.g., PISA, NAEP，IPEDS) and data from digital technologies (e.g., log-trace data from Open University Learning Analytics Dataset (OULAD)).\nIn social science research, data is traditionally sourced from small-scale surveys, experiments, or qualitative studies. However, the rise of big data offers researchers opportunities to explore numeric and quantitative datasets of unprecedented scale and variety. This chapter discusses how to access and analyze large-scale datasets like international assessments (e.g., PISA, NAEP) and digital log-trace data (e.g., Open University Learning Analytics Dataset (OULAD)). These secondary data sources enable novel research questions and methods, particularly when paired with machine learning and statistical modeling approaches.",
    "crumbs": [
      "Section 2 — Traditional Computational Analyses of Educational Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4 **Secondary Analysis of Big Data (Numeric Data)**</span>"
    ]
  },
  {
    "objectID": "chapter-4.html#accessing-big-data-broadening-the-horizon",
    "href": "chapter-4.html#accessing-big-data-broadening-the-horizon",
    "title": "Chapter 4 Secondary Analysis of Big Data (Numeric Data)",
    "section": "4.2 Accessing Big data (Broadening the Horizon)",
    "text": "4.2 Accessing Big data (Broadening the Horizon)\n\n4.2.1 Big Data\n\nAccessing PISA Data\nThe Programme for International Student Assessment (PISA) is a widely used dataset for large-scale educational research. It assesses 15-year-old students’ knowledge and skills in reading, mathematics, and science across multiple countries. Researchers can access PISA data through various methods:\n\n1. Direct Download from the Official Website\nThe OECD provides direct access to PISA data files via its official website. Researchers can download data for specific years and cycles. Data files are typically provided in .csv or .sav (SPSS) formats, along with detailed documentation.\n\nSteps to Access PISA Data from the OECD Website:\n\nVisit the OECD PISA website.\nNavigate to the “Data” section.\nSelect the desired assessment year (e.g., 2022).\nDownload the data and accompanying codebooks.\n\n\n\n\n2. Using the OECD R Package\nThe OECD R package provides a direct interface to download and explore datasets published by the OECD, including PISA.\n\nSteps to Use the OECD Package:\n\nInstall and load the OECD package.\nUse the getOECD() function to fetch PISA data.\n\n\n\n# Install and load the OECD package\ninstall.packages(\"OECD\")\nlibrary(OECD)\n\n# Fetch PISA data for the 2018 cycle\npisa_data &lt;- getOECD(\"pisa\", years = \"2022\")\n\n# Display a summary of the data\nsummary(pisa_data)\n\n\n\n3. Using the Edsurvey R Package\nThe Edsurvey package is designed specifically for analyzing large-scale assessment data, including PISA. It allows for complex statistical modeling and supports handling weights and replicate weights used in PISA.\n\nSteps to Use the Edsurvey Package:\n\nInstall and load the Edsurvey package.\nDownload the PISA data from the OECD website and provide the path to the .sav files.\nLoad the data into R using readPISA().\n\n\n\n# Install and load the Edsurvey package\ninstall.packages(\"Edsurvey\")\nlibrary(Edsurvey)\n\n# Read PISA data from a local file\npisa_data &lt;- readPISA(\"path/to/PISA2022Student.sav\")\n\n# Display the structure of the dataset\nstr(pisa_data)\n\n\n\nComparison of Methods\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nDisadvantages\n\n\n\n\nDirect Download\nFull access to all raw data and documentation.\nRequires manual processing and cleaning.\n\n\nOECD Package\nEasy to use for downloading specific datasets.\nLimited to OECD-published formats.\n\n\nEdsurvey Package\nSupports advanced statistical analysis and weights.\nRequires additional setup and dependencies.\n\n\n\n\n\n\nAccessing IPEDS Data\nThe Integrated Postsecondary Education Data System (IPEDS) is a comprehensive source of data on U.S. colleges, universities, and technical and vocational institutions. It provides data on enrollments, completions, graduation rates, faculty, finances, and more. Researchers and policymakers widely use IPEDS data to analyze trends in higher education.\nThere are several ways to access IPEDS data, depending on the user’s needs and technical proficiency.\n\n1. Direct Download from the NCES Website\nThe most straightforward way to access IPEDS data is by downloading it directly from the National Center for Education Statistics (NCES) website.\n\n\nSteps to Access IPEDS Data:\n\nVisit the IPEDS Data Center.\nClick on “Use the Data” and navigate to the “Download IPEDS Data Files” section.\nSelect the desired data year and survey component (e.g., Fall Enrollment, Graduation Rates).\nDownload the data files, typically provided in .csv or .xls format, along with accompanying codebooks.\n\n\n\n2. Using the ipeds R Package\nThe ipeds R package simplifies downloading and analyzing IPEDS data directly from R by connecting to the NCES data repository.\n\n\nSteps to Use the ipeds Package:\n\nInstall and load the ipeds package.\nUse the download_ipeds() function to fetch data for specific survey components and years.\n\n\n# Install and load the ipeds package\ninstall.packages(\"ipeds\")\nlibrary(ipeds)\n\n# Download IPEDS data for completions in 2021\nipeds_data &lt;- download_ipeds(\"C\", year = 2021)\n\n# View the structure of the downloaded data\nstr(ipeds_data)\n\n\n\n3. Using the tidycensus R Package\nThe tidycensus package, while primarily designed for Census data, can access specific IPEDS data linked to educational institutions.\n\n\nSteps to Use the tidycensus Package:\n\nInstall and load the tidycensus package.\nSet up a Census API key to access the data.\nQuery IPEDS data for specific institution-level information.\n\n\n# Install and load the tidycensus package\ninstall.packages(\"tidycensus\")\nlibrary(tidycensus)\n\n# Set Census API key (replace with your actual key)\ncensus_api_key(\"your_census_api_key\")\n\n# Fetch IPEDS-related data (e.g., institution information)\nipeds_institutions &lt;- get_acs(\n  geography = \"place\",\n  variables = \"B14002_003\",\n  year = 2021,\n  survey = \"acs5\"\n)\n\n# View the first few rows\nhead(ipeds_institutions)\n\n\n\n4. Using Online Tools\nIPEDS provides several online tools for querying and visualizing data without requiring programming skills.\n\n\nCommon Tools:\n\nIPEDS Data Explorer: Enables users to query and export customized datasets.\nTrend Generator: Allows users to visualize trends in key metrics over time.\nIPEDS Use the Data: Simplified tool for accessing pre-compiled datasets.\n\n\n\nSteps to Use the IPEDS Data Explorer:\n\nVisit the IPEDS Data Explorer.\nSelect variables of interest, such as institution type, enrollment size, or location.\nFilter results by years, institution categories, or other criteria.\nExport the results as a .csv or .xlsx file.\n\n\n\nComparison of Methods\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nDisadvantages\n\n\n\n\nDirect Download\nFull access to raw data and documentation.\nRequires manual data preparation and cleaning.\n\n\nipeds Package\nAutomated access to specific components.\nLimited flexibility for customized queries.\n\n\ntidycensus Package\nAllows integration with Census and ACS data.\nRequires API setup and advanced R skills.\n\n\nOnline Tools\nUser-friendly and suitable for non-coders.\nLimited to predefined queries and exports.\n\n\n\n\n\n\nAccessing Open University Learning Analytics Dataset (OULAD)\nThe Open University Learning Analytics Dataset (OULAD) is a publicly available dataset designed to support research in educational data mining and learning analytics. It includes student demographics, module information, interactions with the virtual learning environment (VLE), and assessment scores.\n\n\nSteps to Access OULAD Data\n\nVisit the OULAD Repository**\nThe dataset is hosted on the Open University’s Analytics Project. To access the data: 1. Navigate to the website. 2. Download the dataset as a .zip file. 3. Extract the .zip file to a local directory.\nThe dataset contains multiple CSV files: - studentInfo.csv: Student demographics and performance data. - studentVle.csv: Interactions with the VLE. - vle.csv: Details of learning resources. - studentAssessment.csv: Assessment scores.\n\n\nLoading OULAD Data in R\nOnce the data is downloaded and extracted, follow these steps to load and access it in R:\n\n\nStep 1: Install Required Packages\n\n# Install necessary packages\ninstall.packages(c(\"readr\", \"dplyr\"))\n\n\n\nStep 2: Load Data\nUse the readr package to read the CSV files into R.\n\n# Load required libraries\nlibrary(readr)\n\n# Define the path to the OULAD data\ndata_path &lt;- \"path/to/OULAD/\"\n\n# Load individual CSV files\nstudent_info &lt;- read_csv(file.path(data_path, \"studentInfo.csv\"))\nstudent_vle &lt;- read_csv(file.path(data_path, \"studentVle.csv\"))\nvle &lt;- read_csv(file.path(data_path, \"vle.csv\"))\nstudent_assessment &lt;- read_csv(file.path(data_path, \"studentAssessment.csv\"))\n\n\n\nStep 3: Preview the Data\nInspect the structure and contents of the datasets.\n\n# View the first few rows of student info\nhead(student_info)\n\n# Check the structure of the student VLE data\nstr(student_vle)\n\n\n\n\n\n4.2.2 Learning Analytics\n\nWhat is Learning Analytics?\nLearning Analytics (LA) refers to the measurement, collection, analysis, and reporting of data about learners and their contexts. The primary goal of LA is to understand and improve learning processes by identifying patterns, predicting outcomes, and providing actionable insights to educators, institutions, and learners.\nKey features of LA include: - Data Collection: Gathering information from digital platforms such as learning management systems (LMS) or external assessments. - Analysis: Using machine learning, statistical methods, or visualization tools to reveal trends and patterns. - Applications: Supporting personalized learning, enhancing institutional decision-making, and improving curriculum design.\n\n\nApplications of Learning Analytics in Big Data\nLearning analytics can be applied to large-scale educational datasets like PISA, IPEDS, and OULAD to uncover trends, predict outcomes, and guide interventions.\n\n1. PISA Data and Learning Analytics\n\nWhat it offers: Insights into international student performance in reading, math, and science, combined with contextual variables (e.g., socio-economic status).\nLA Applications:\n\nIdentifying key factors influencing performance across countries.\nPredicting the impact of ICT use on student achievement.\nSegmenting students into performance clusters for targeted interventions.\n\n\n\n\n2. IPEDS Data and Learning Analytics\n\nWhat it offers: U.S. institutional-level data on enrollment, graduation rates, tuition, and financial aid.\nLA Applications:\n\nAnalyzing trends in student demographics across institutions.\nPredicting enrollment patterns based on historical data.\nBenchmarking institutions to inform policymaking and funding decisions.\n\n\n\n\n3. OULAD and Learning Analytics\n\nWhat it offers: Rich data on student engagement with virtual learning environments (VLE), assessment scores, and demographic information.\nLA Applications:\n\nTracking student interactions with learning resources to predict course completion.\nModeling the relationship between VLE usage and final grades.\nDetecting early warning signs for at-risk students based on engagement metrics.\n\n\n\n\n\nWhy Learning Analytics Matters\nThe integration of Learning Analytics with big data enables researchers and practitioners to: - Personalize Learning: Tailor educational experiences to meet individual needs. - Improve Retention: Identify at-risk learners and implement timely interventions. - Enhance Decision-Making: Provide evidence-based recommendations for curriculum and policy adjustments.\nBy leveraging datasets like PISA, IPEDS, and OULAD, learning analytics can help bridge the gap between raw data and actionable insights, fostering a more equitable and effective educational landscape.\n\n\nSupervised Learning in Learning Analytics\nMachine Learning, particularly Supervised Learning, has become a cornerstone of Learning Analytics. Supervised learning models are trained on labeled datasets, where input features are mapped to known outcomes, enabling the prediction of new, unseen data.\n\nKey Concepts in Supervised Learning\n\nDefinition\nSupervised Learning is a subset of Machine Learning focused on learning a mapping between input variables (features) and output variables (labels or outcomes). Models trained on labeled data can predict outcomes for new data points.\nCommon Algorithms\n\nLinear Regression\nLogistic Regression\nDecision Trees and Random Forests\nNeural Networks\n\nApplications in Education\nSupervised learning is particularly effective in Learning Analytics for predicting:\n\nStudent performance\nDropout risks\nEnrollment trends\nCourse completion rates\n\n\n\n\n\nApplications of Supervised Learning with Big Data\n\n1. PISA Data and Supervised Learning\n\nGoal: Use demographic and contextual features to predict student performance in mathematics, reading, or science.\nExample: Train a linear regression model to identify the relationship between socioeconomic status and test scores.\n\n\n\n2. IPEDS Data and Supervised Learning\n\nGoal: Develop models to predict institutional enrollment rates based on financial aid, demographics, and program offerings.\nExample: Use logistic regression to forecast whether a student is likely to enroll based on financial aid eligibility.\n\n\n\n3. OULAD Data and Supervised Learning\n\nGoal: Predict student outcomes (e.g., pass/fail) based on engagement metrics like forum participation and assignment submissions.\nExample: Train a random forest model to classify students as “at-risk” or “not at-risk” based on weekly interaction data.\n\n\n\n\nChoosing the Right Supervised Learning Approach\nWhen applying supervised learning in Learning Analytics: 1. Define the Goal: Clearly articulate the outcome you want to predict (e.g., performance, enrollment, or engagement). 2. Select an Algorithm: Choose an appropriate model based on the data and prediction task. - For continuous outcomes, use regression models. - For categorical outcomes, use classification models like logistic regression or random forests. 3. Feature Engineering: Select and preprocess relevant features (e.g., attendance, demographics, assignment scores) to improve model accuracy. 4. Evaluate Model Performance: Use metrics such as accuracy, precision, recall, or R-squared to assess model effectiveness.\nIntegrating supervised learning techniques into Learning Analytics, researchers and practitioners can leverage big data to make data-driven predictions and decisions, ultimately enhancing educational outcomes.",
    "crumbs": [
      "Section 2 — Traditional Computational Analyses of Educational Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4 **Secondary Analysis of Big Data (Numeric Data)**</span>"
    ]
  },
  {
    "objectID": "chapter-4.html#logistic-regression-ml",
    "href": "chapter-4.html#logistic-regression-ml",
    "title": "Chapter 4 Secondary Analysis of Big Data (Numeric Data)",
    "section": "4.3 Logistic Regression ML",
    "text": "4.3 Logistic Regression ML\n\n4.3.1 Purpose + CASE\n\nPurpose\nLogistic regression is a supervised learning technique widely used for binary classification tasks. It models the probability of an event occurring (e.g., success vs. failure) based on a set of predictor variables. Logistic regression is particularly effective in educational research for predicting outcomes such as retention, enrollment, or graduation rates.\n\n\nCASE: Predicting Graduation Rates\nThis case study is based on IPEDS data and inspired by Zong and Davis (2022). We predict graduation rates as a binary outcome (good_grad_rate) using institutional features such as total enrollment, admission rate, tuition fees, and average instructional staff salary.\n\n\n\n4.3.2 Sample Research Questions (RQs)\n\nRQ A: What institutional factors are associated with high graduation rates in U.S. four-year universities?\nRQ B: How accurately can we predict high graduation rates using institutional features with supervised machine learning?\n\n\n\n4.3.3 Analysis\n\nLoading Required Packages\nWe load necessary R packages for data wrangling, cleaning, and modeling.\n\n# Load necessary libraries for data cleaning, wrangling, and modeling\nlibrary(tidyverse)  # For data manipulation and visualization\nlibrary(tidymodels) # For machine learning workflows\nlibrary(janitor)    # For cleaning variable names\n\n\n\nLoading and Cleaning Data\nWe read the IPEDS dataset and clean column names for easier handling.\n\n# Read in IPEDS data from CSV file\nipeds &lt;- read_csv(\"data/ipeds-all-title-9-2022-data.csv\")\n\n# Clean column names for consistency and usability\nipeds &lt;- janitor::clean_names(ipeds)\n\n\n\nData Wrangling\nSelect relevant variables, filter the dataset, and create the dependent variable good_grad_rate.\n\n# Select and rename key variables; filter relevant institutions\nipeds &lt;- ipeds %&gt;%\n  select(\n    name = institution_name,                  # Institution name\n    total_enroll = drvef2022_total_enrollment, # Total enrollment\n    pct_admitted = drvadm2022_percent_admitted_total, # Admission percentage\n    tuition_fees = drvic2022_tuition_and_fees_2021_22, # Tuition fees\n    grad_rate = drvgr2022_graduation_rate_total_cohort, # Graduation rate\n    percent_fin_aid = sfa2122_percent_of_full_time_first_time_undergraduates_awarded_any_financial_aid, # Financial aid\n    avg_salary = drvhr2022_average_salary_equated_to_9_months_of_full_time_instructional_staff_all_ranks # Staff salary\n  ) %&gt;%\n  filter(!is.na(grad_rate)) %&gt;% # Remove rows with missing graduation rates\n  mutate(\n    # Create binary dependent variable for high graduation rates\n    good_grad_rate = if_else(grad_rate &gt; 62, 1, 0),\n    good_grad_rate = as.factor(good_grad_rate) # Convert to factor\n  )\n\n\n\nExploratory Data Analysis (EDA)\nVisualize the distribution of the graduation rate.\n\n# Plot a histogram of graduation rates\nipeds %&gt;%\n  ggplot(aes(x = grad_rate)) +\n  geom_histogram(bins = 20, fill = \"blue\", color = \"white\") +\n  labs(\n    title = \"Distribution of Graduation Rates\",\n    x = \"Graduation Rate\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\n\n\nLogistic Regression Model\nFit a logistic regression model to predict high graduation rates.\n\n# Fit logistic regression model\nm1 &lt;- glm(\n  good_grad_rate ~ total_enroll + pct_admitted + tuition_fees + percent_fin_aid + avg_salary,\n  data = ipeds,\n  family = \"binomial\" # Specify logistic regression for binary outcome\n)\n\n# View model summary\nsummary(m1)\n\n\nCall:\nglm(formula = good_grad_rate ~ total_enroll + pct_admitted + \n    tuition_fees + percent_fin_aid + avg_salary, family = \"binomial\", \n    data = ipeds)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -8.742e-01  6.237e-01  -1.402    0.161    \ntotal_enroll     3.350e-05  7.880e-06   4.251 2.13e-05 ***\npct_admitted    -1.407e-02  3.519e-03  -3.997 6.40e-05 ***\ntuition_fees     6.952e-05  4.965e-06  14.003  &lt; 2e-16 ***\npercent_fin_aid -2.960e-02  5.652e-03  -5.237 1.64e-07 ***\navg_salary       2.996e-05  3.870e-06   7.740 9.91e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2277  on 1706  degrees of freedom\nResidual deviance: 1632  on 1701  degrees of freedom\n  (3621 observations deleted due to missingness)\nAIC: 1644\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nSupervised ML Workflow\nUse the tidymodels framework to build a machine learning model.\n\n# Define recipe for the model (preprocessing steps)\nmy_rec &lt;- recipe(good_grad_rate ~ total_enroll + pct_admitted + tuition_fees + percent_fin_aid + avg_salary, data = ipeds)\n\n# Specify logistic regression model with tidymodels\nmy_mod &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%         # Use glm engine for logistic regression\n  set_mode(\"classification\")    # Specify binary classification task\n\n# Create workflow to connect recipe and model\nmy_wf &lt;- workflow() %&gt;%\n  add_recipe(my_rec) %&gt;%\n  add_model(my_mod)\n\n# Fit the logistic regression model\nfit_model &lt;- fit(my_wf, ipeds)\n\n# Generate predictions on the dataset\npredictions &lt;- predict(fit_model, ipeds) %&gt;%\n  bind_cols(ipeds) # Combine predictions with original data\n\n# Calculate and display accuracy\nmy_accuracy &lt;- predictions %&gt;%\n  metrics(truth = good_grad_rate, estimate = .pred_class) %&gt;%\n  filter(.metric == \"accuracy\")\n\nmy_accuracy\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.800\n\n\n\n\n\n4.3.4 Results and Discussions\n\nLogistic Regression Model (RQ A)\nThe logistic regression model was fitted to predict whether a university achieves a “good” graduation rate (i.e., graduation rate &gt; 62%) based on several institutional features. The model output is summarized below:\n\nCoefficients & Significance:\n\ntotal_enroll: Estimate = 3.35e-05, z = 4.251, p = 2.13e-05\nInterpretation: As total enrollment increases, the probability of a high graduation rate increases.\npct_admitted: Estimate = -1.407e-02, z = -3.997, p = 6.40e-05\nInterpretation: Higher admission percentages are associated with a lower likelihood of achieving a high graduation rate.\ntuition_fees: Estimate = 6.952e-05, z = 14.003, p &lt; 2e-16\nInterpretation: Higher tuition fees are strongly associated with higher graduation rates.\npercent_fin_aid: Estimate = -2.960e-02, z = -5.237, p = 1.64e-07\nInterpretation: A higher percentage of students receiving financial aid is associated with a lower probability of a good graduation rate.\navg_salary: Estimate = 2.996e-05, z = 7.740, p = 9.91e-15\nInterpretation: Higher average salaries for instructional staff are positively associated with high graduation rates.\n\nModel Fit Statistics:\n\nNull Deviance: 2277 (on 1706 degrees of freedom)\nResidual Deviance: 1632 (on 1701 degrees of freedom)\nAIC: 1644\nNote: 3621 observations were deleted due to missing values.\n\n\nOverall, the regression model demonstrates that several institutional factors are statistically significant predictors of graduation rates. In particular, tuition fees and avg_salary have a strong positive effect, while pct_admitted and percent_fin_aid show negative associations.\n\n\nSupervised ML Workflow Results (RQ B)\nUsing the tidymodels framework, we built a logistic regression model as part of a supervised machine learning workflow. The performance metric obtained is as follows:\n\nAccuracy: 80.02%\n\nThis indicates that the machine learning model correctly classified approximately 80% of the institutions as having either a good or not good graduation rate, based on the selected predictors.\n\n\nOverall Discussion\n\nSimilarities between Approaches:\n\nBoth the traditional logistic regression and the tidymodels workflow identified key predictors that influence graduation rates, such as total enrollment, admission percentage, tuition fees, financial aid percentage, and average staff salary.\nEach approach provides valuable insights: the regression model offers detailed coefficient estimates and significance levels, while the tidymodels workflow emphasizes predictive accuracy.\n\nDifferences between Approaches:\n\nInterpretability vs. Predictive Performance: The logistic regression output delivers interpretability through its coefficients and p-values, allowing us to understand the direction and magnitude of the relationships. In contrast, the supervised ML workflow focuses on achieving a robust predictive performance, evidenced by an 80% accuracy.\nHandling of Data: The traditional regression model summarizes the relationship between variables, whereas the ML workflow integrates data pre-processing, modeling, and validation into a cohesive framework.\n\n\nIn summary, our analyses indicate that institutional factors, particularly tuition fees and staff salaries, play a significant role in predicting graduation outcomes. The supervised ML approach, with an accuracy of around 80%, confirms the model’s practical utility in classifying institutions based on graduation performance. Both methods complement each other, providing a comprehensive understanding of the underlying dynamics that drive graduation rates in higher education.",
    "crumbs": [
      "Section 2 — Traditional Computational Analyses of Educational Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4 **Secondary Analysis of Big Data (Numeric Data)**</span>"
    ]
  },
  {
    "objectID": "chapter-4.html#random-forests-ml-on-interactions-data",
    "href": "chapter-4.html#random-forests-ml-on-interactions-data",
    "title": "Chapter 4 Secondary Analysis of Big Data (Numeric Data)",
    "section": "4.4 Random Forests ML on Interactions Data",
    "text": "4.4 Random Forests ML on Interactions Data\nIn this section, we explore a more sophisticated supervised learning approach—Random Forests—to model student interactions data from the Open University Learning Analytics Dataset (OULAD). Building on our earlier work with logistic regression and evaluation metrics, this case study examines whether a random forest model can improve predictive performance when leveraging clickstream data from the virtual learning environment (VLE).\n\n4.4.1 Purpose + CASE\n\nPurpose\nRandom Forests is an ensemble learning method that builds multiple decision trees and aggregates their results to improve prediction accuracy and control over-fitting. It is particularly well suited for complex, high-dimensional data such as student interaction (clickstream) data. This approach not only provides robust predictions but also offers insights into variable importance, helping us understand which features most influence student outcomes.\n\n\nCASE\nInspired by research on digital trace data (e.g., Rodriguez et al., 2021; Bosch, 2021), this case study uses pre-processed interactions data from OULAD. In our analysis, we focus on predicting whether a student will pass the course (a binary outcome) based on engineered features derived from clickstream data. These features include the total number of clicks (sum_clicks), summary statistics (mean and standard deviation of clicks), and linear trends over time (slope and intercept from clickstream patterns).\n\n\n\n4.4.2 Sample Research Questions\n\nRQ1: How accurately can a random forest model predict whether a student will pass a course using interactions data from OULAD?\nRQ2: Which interaction-based features (e.g., total clicks, click stream slope) are most important in predicting student outcomes?\nRQ3: How does the use of cross-validation (e.g., v-fold CV) influence the stability and generalizability of the random forest model on interactions data?\n\n\n\n4.4.3 Analysis\n\nLoading Required Packages\n\n# Load necessary libraries for data manipulation and modeling\nlibrary(tidyverse)      # Data wrangling and visualization\nlibrary(janitor)        # Cleaning variable names\nlibrary(tidymodels)     # Modeling workflow\nlibrary(ranger)         # Random forest implementation\nlibrary(vip)            # Variable importance plots\n\n\n\nLoading and Preparing the Data\nWe load the pre-filtered interactions data from OULAD along with a students-and-assessments file, then join them to create a complete dataset for modeling.\n\n# Load the interactions data (filtered for the first one-third of the semester)\ninteractions &lt;- read_csv(\"data/oulad-interactions-filtered.csv\")\n\n# Load the students and assessments data\nstudents_and_assessments &lt;- read_csv(\"data/oulad-students-and-assessments.csv\")\n\n# Create cut-off dates based on assessments data (using first quantile as intervention point)\nassessments &lt;- read_csv(\"data/oulad-assessments.csv\")\n\n# Create cut-off dates based on assessments data using the correct date column 'date_submitted'\ncode_module_dates &lt;- assessments %&gt;% \n    group_by(code_module, code_presentation) %&gt;% \n    summarize(quantile_cutoff_date = quantile(date_submitted, probs = 0.25, na.rm = TRUE), .groups = 'drop')\n\n# Join interactions with the cutoff dates and filter\ninteractions_joined &lt;- interactions %&gt;% \n    left_join(code_module_dates, by = c(\"code_module\", \"code_presentation\"))\n\n\ninteractions_joined &lt;- interactions_joined %&gt;% \n    select(-quantile_cutoff_date.x) %&gt;% \n    rename(quantile_cutoff_date = quantile_cutoff_date.y)\n\n\n\n# Filter interactions to include only those before the cutoff date\ninteractions_filtered &lt;- interactions_joined %&gt;% \n    filter(date &lt; quantile_cutoff_date)\n\n# Summarize interactions: total clicks, mean and standard deviation\ninteractions_summarized &lt;- interactions_filtered %&gt;% \n    group_by(id_student, code_module, code_presentation) %&gt;% \n    summarize(\n      sum_clicks = sum(sum_click),\n      sd_clicks = sd(sum_click), \n      mean_clicks = mean(sum_click)\n    )\n\n# (Optional) Further feature engineering: derive linear slopes from clickstream\nfit_model &lt;- function(data) {\n    tryCatch(\n        { \n            model &lt;- lm(sum_click ~ date, data = data)\n            tidy(model)\n        },\n        error = function(e) { tibble(term = NA, estimate = NA, std.error = NA, statistic = NA, p.value = NA) },\n        warning = function(w) { tibble(term = NA, estimate = NA, std.error = NA, statistic = NA, p.value = NA) }\n    )\n}\n\ninteractions_slopes &lt;- interactions_filtered %&gt;%\n    group_by(id_student, code_module, code_presentation) %&gt;%\n    nest() %&gt;%\n    mutate(model = map(data, fit_model)) %&gt;%\n    unnest(model) %&gt;%\n    ungroup() %&gt;%\n    select(code_module, code_presentation, id_student, term, estimate) %&gt;%\n    filter(!is.na(term)) %&gt;%\n    pivot_wider(names_from = term, values_from = estimate) %&gt;%\n    mutate_if(is.numeric, round, 4) %&gt;%\n    rename(intercept = `(Intercept)`, slope = date)\n\n# Join summarized clicks and slopes features\ninteractions_features &lt;- left_join(interactions_summarized, interactions_slopes, by = c(\"id_student\", \"code_module\", \"code_presentation\"))\n\n# Finally, join with students_and_assessments to get the outcome variable\nstudents_assessments_and_interactions &lt;- left_join(students_and_assessments, interactions_features, by = c(\"id_student\", \"code_module\", \"code_presentation\"))\n\n# Ensure outcome variable 'pass' is a factor\nstudents_assessments_and_interactions &lt;- students_assessments_and_interactions %&gt;% \n    mutate(pass = as.factor(pass))\n\n# Optional: Inspect the final dataset\nstudents_assessments_and_interactions %&gt;% \n    skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n32593\n\n\nNumber of columns\n22\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nfactor\n1\n\n\nnumeric\n13\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncode_module\n0\n1\n3\n3\n0\n7\n0\n\n\ncode_presentation\n0\n1\n5\n5\n0\n4\n0\n\n\ngender\n0\n1\n1\n1\n0\n2\n0\n\n\nregion\n0\n1\n5\n20\n0\n13\n0\n\n\nhighest_education\n0\n1\n15\n27\n0\n5\n0\n\n\nage_band\n0\n1\n4\n5\n0\n3\n0\n\n\ndisability\n0\n1\n1\n1\n0\n2\n0\n\n\nfinal_result\n0\n1\n4\n11\n0\n4\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\npass\n0\n1\nFALSE\n2\n0: 20232, 1: 12361\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nid_student\n0\n1.00\n706687.67\n549167.31\n3733.00\n508573.00\n590310.00\n644453.00\n2716795.00\n▅▇▁▁▁\n\n\nimd_band\n4627\n0.86\n5.62\n2.73\n1.00\n4.00\n6.00\n8.00\n10.00\n▃▇▇▆▆\n\n\nnum_of_prev_attempts\n0\n1.00\n0.16\n0.48\n0.00\n0.00\n0.00\n0.00\n6.00\n▇▁▁▁▁\n\n\nstudied_credits\n0\n1.00\n79.76\n41.07\n30.00\n60.00\n60.00\n120.00\n655.00\n▇▁▁▁▁\n\n\nmodule_presentation_length\n0\n1.00\n256.01\n13.18\n234.00\n241.00\n262.00\n268.00\n269.00\n▇▁▁▅▇\n\n\ndate_registration\n45\n1.00\n-69.41\n49.26\n-322.00\n-100.00\n-57.00\n-29.00\n167.00\n▁▂▇▃▁\n\n\ndate_unregistration\n22521\n0.31\n49.76\n82.46\n-365.00\n-2.00\n27.00\n109.00\n444.00\n▁▁▇▂▁\n\n\nmean_weighted_score\n7958\n0.76\n544.70\n381.39\n0.00\n160.00\n610.00\n875.00\n1512.00\n▇▃▇▅▁\n\n\nsum_clicks\n3495\n0.89\n474.93\n572.89\n1.00\n128.00\n295.50\n604.00\n10712.00\n▇▁▁▁▁\n\n\nsd_clicks\n3753\n0.88\n4.91\n5.51\n0.00\n2.37\n3.72\n6.44\n560.24\n▇▁▁▁▁\n\n\nmean_clicks\n3495\n0.89\n3.19\n1.30\n1.00\n2.33\n2.95\n3.82\n47.12\n▇▁▁▁▁\n\n\nintercept\n3640\n0.89\n3.04\n4.61\n-585.59\n2.15\n2.80\n3.66\n130.83\n▁▁▁▁▇\n\n\nslope\n4441\n0.86\n0.01\n0.22\n-12.17\n-0.01\n0.01\n0.03\n20.12\n▁▇▁▁▁\n\n\n\n\n\n\n\nCreating the Model Recipe\nWe build a recipe that includes the engineered features from interactions data along with other predictors from the students data.\n\nmy_rec2 &lt;- recipe(pass ~ disability +\n                     date_registration + \n                     gender +\n                     code_module +\n                     mean_weighted_score +\n                     sum_clicks + sd_clicks + mean_clicks + \n                     intercept + slope, \n                 data = students_assessments_and_interactions) %&gt;% \n    step_dummy(disability) %&gt;% \n    step_dummy(gender) %&gt;%  \n    step_dummy(code_module) %&gt;% \n    step_impute_knn(mean_weighted_score) %&gt;% \n    step_impute_knn(sum_clicks) %&gt;% \n    step_impute_knn(sd_clicks) %&gt;% \n    step_impute_knn(mean_clicks) %&gt;% \n    step_impute_knn(intercept) %&gt;% \n    step_impute_knn(slope) %&gt;% \n    step_impute_knn(date_registration) %&gt;% \n    step_normalize(all_numeric_predictors())\n\n\n\nSpecifying the Model and Workflow\nWe use a random forest model via the ranger engine and set up our workflow.\n\n# Specify random forest model\nmy_mod2 &lt;- rand_forest() %&gt;% \n    set_engine(\"ranger\", importance = \"impurity\") %&gt;% \n    set_mode(\"classification\")\n\n# Create workflow to bundle the recipe and model\nmy_wf2 &lt;- workflow() %&gt;% \n    add_recipe(my_rec2) %&gt;% \n    add_model(my_mod2)\n\n\n\nResampling and Model Fitting\nWe perform cross-validation (v-fold CV) to estimate model performance.\n\n# Create 4-fold cross-validation on training data\nvfcv &lt;- vfold_cv(data = students_assessments_and_interactions, v = 4, strata = pass)\n\n# Specify metrics: accuracy, sensitivity, specificity, ppv, npv, and Cohen's kappa\nclass_metrics &lt;- metric_set(accuracy, sensitivity, specificity, ppv, npv, kap)\n\n# Fit the model using resampling\nfitted_model_resamples &lt;- fit_resamples(my_wf2, resamples = vfcv, metrics = class_metrics)\n\n# Collect and display metrics\ncollect_metrics(fitted_model_resamples)\n\n# A tibble: 6 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.671     4 0.00149 Preprocessor1_Model1\n2 kap         binary     0.264     4 0.00431 Preprocessor1_Model1\n3 npv         binary     0.589     4 0.00188 Preprocessor1_Model1\n4 ppv         binary     0.703     4 0.00174 Preprocessor1_Model1\n5 sensitivity binary     0.814     4 0.00107 Preprocessor1_Model1\n6 specificity binary     0.437     4 0.00531 Preprocessor1_Model1\n\n\n\n\nFinal Model Fit and Evaluation\nFinally, we fit the model on the full training set (using last_fit) and evaluate its predictions on the test set.\n\n# Split data into training and testing sets (e.g., 33% for testing)\nset.seed(20230712)\ntrain_test_split &lt;- initial_split(students_assessments_and_interactions, prop = 0.67, strata = pass)\ndata_train &lt;- training(train_test_split)\ndata_test &lt;- testing(train_test_split)\n\n# Fit final model on the training set and evaluate on the test set\nfinal_fit &lt;- last_fit(my_wf2, train_test_split, metrics = class_metrics)\n\n# Collect and display final metrics\ncollect_metrics(final_fit)\n\n# A tibble: 6 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.665 Preprocessor1_Model1\n2 sensitivity binary         0.815 Preprocessor1_Model1\n3 specificity binary         0.419 Preprocessor1_Model1\n4 ppv         binary         0.697 Preprocessor1_Model1\n5 npv         binary         0.580 Preprocessor1_Model1\n6 kap         binary         0.247 Preprocessor1_Model1\n\n# Generate and display a confusion matrix for final predictions\ncollect_predictions(final_fit) %&gt;% \n    conf_mat(.pred_class, pass)\n\n          Truth\nPrediction    0    1\n         0 5439 1238\n         1 2370 1710\n\n# Extract the fitted model from the final workflow and plot variable importance\nfinal_fit %&gt;% \n  pluck(\".workflow\", 1) %&gt;%   # Extract the workflow object from the final fit\n  extract_fit_parsnip() %&gt;%   # Retrieve the fitted model from the workflow\n  vip(num_features = 10)      # Plot the top 10 important features\n\n\n\n\n\n\n\n# Extract the fitted model from the workflow\nfinal_model &lt;- final_fit %&gt;% \n  pluck(\".workflow\", 1) %&gt;% \n  extract_fit_parsnip()\n# Extract the variable importance values from the fitted model\nimportance_values &lt;- final_model$fit$variable.importance\n\n# Print the variable importance values\nprint(importance_values)\n\n  date_registration mean_weighted_score          sum_clicks           sd_clicks \n          652.14806           838.06052          1062.49141           778.63848 \n        mean_clicks           intercept               slope        disability_Y \n          737.79417           729.52344           739.85682            60.93367 \n           gender_M     code_module_BBB     code_module_CCC     code_module_DDD \n           79.62440            77.18477            70.72006            46.68243 \n    code_module_EEE     code_module_FFF     code_module_GGG \n           28.26595            74.31310            35.27026 \n\n\n\n\n\n4.4.4 Results and Discussions\n\nResearch Question 1 (RQ1):\nHow accurately can a random forest model predict whether a student will pass a course using interactions data from OULAD?\nResponse:\nUsing 4-fold cross-validation, our random forest model yielded an average accuracy of approximately 67.0% (mean accuracy from resamples: 0.670) with a Cohen’s Kappa of 0.261, suggesting moderate agreement beyond chance. When fitted on the entire training set and evaluated on the test set, the final model showed an accuracy of 66.5% along with: - Sensitivity: 81.5% – indicating the model correctly identifies a high proportion of students who pass. - Specificity: 41.9% – suggesting that the model is less effective at correctly identifying students who do not pass. - Positive Predictive Value (PPV): 69.7% - Negative Predictive Value (NPV): 58.0%\nThe confusion matrix shows: - True Negatives (TN): 5439 - False Negatives (FN): 1238 - False Positives (FP): 2370 - True Positives (TP): 1710\nOverall, these metrics indicate that while the model performs well in detecting positive outcomes (high sensitivity), its lower specificity means that it tends to misclassify a relatively higher proportion of non-passing students.\n\n\nResearch Question 2 (RQ2):\nWhich interaction-based features are most important in predicting student outcomes?\nResponse:\nThe variable importance analysis, extracted from the final random forest model using the vip() function, highlights the following key predictors (with their respective importance scores):\n\nsum_clicks: 1062.49 – This is the most influential feature, indicating that the total number of clicks (i.e., student engagement) in the VLE is a strong predictor of student success.\nmean_weighted_score: 838.06 – Reflecting academic performance as measured by weighted assessment scores.\nmean_clicks: 737.79, slope: 739.86, and intercept: 729.52 – These engineered features representing the central tendency and trend of click behavior further underline the importance of digital engagement patterns.\ndate_registration: 652.15 – The registration date also plays a significant role.\nOther categorical variables (e.g., dummy-coded disability, gender, and code_module levels) generally show lower importance scores, with values typically under 80, indicating that while they do contribute, engagement and performance metrics dominate.\n\nThese results suggest that both the intensity and the temporal trend of student interactions with the learning environment are critical in predicting whether a student will pass.\n\n\nResearch Question 3 (RQ3):\nHow does the use of cross-validation impact the stability and generalizability of the random forest model on interactions data?\nResponse:\nThe use of 4-fold cross-validation (via vfold_cv) allowed us to assess the model’s performance across multiple subsets of the data, mitigating the risk of overfitting. The resampling results are relatively consistent (with accuracy around 67%, sensitivity at 81.6%, and specificity around 43.2%), which supports the model’s robustness and generalizability. Although the final test set performance (accuracy of 66.5%) is slightly lower, the overall consistency of metrics across folds indicates that our model is stable when applied to unseen data.\n\n\nOverall Discussion\nThe random forest model built on interactions data from OULAD demonstrates decent predictive performance with an accuracy of approximately 66.5–67% and high sensitivity (around 81.5%), indicating strong capability in identifying students who will pass the course. However, the relatively low specificity (around 42%) suggests that there is room for improvement in correctly classifying students who are at risk of not passing.\nThe variable importance analysis underscores that engagement-related features—especially sum_clicks and features capturing the trend in interactions (slope, mean_clicks)—are the most influential predictors. This insight implies that the digital footprint of student engagement in the virtual learning environment is critical for predicting academic outcomes.\nIn summary, while our model performs robustly across cross-validation folds and provides actionable insights into key predictive features, the lower specificity points to the need for further refinement. Future work might explore additional feature engineering, alternative model tuning, or combining models to better balance sensitivity and specificity, ultimately supporting timely interventions in educational settings.",
    "crumbs": [
      "Section 2 — Traditional Computational Analyses of Educational Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 4 **Secondary Analysis of Big Data (Numeric Data)**</span>"
    ]
  },
  {
    "objectID": "chapter-5.html",
    "href": "chapter-5.html",
    "title": "Chapter 5 Cloud-based LLMs for Educational Research",
    "section": "",
    "text": "Overview\nLarge Language Models (LLMs) available through cloud APIs provide powerful text analysis, generation, and reasoning capabilities that can be applied to educational research.\nThis chapter introduces how researchers can connect to commercial or open LLM services — such as OpenAI GPT models, Anthropic Claude, Google Gemini, and Mistral — to automate data processing, assist qualitative coding, and explore mixed-methods analyses.\nWe will discuss: - Available cloud-based LLM APIs and their access mechanisms\n- R-based workflows for sending prompts and receiving model outputs\n- Example use cases in educational research contexts\n- Considerations around privacy, ethics, and reproducibility",
    "crumbs": [
      "Section 3 — AI and LLMs in Educational Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5 **Cloud-based LLMs for Educational Research**</span>"
    ]
  },
  {
    "objectID": "chapter-5.html#available-cloud-based-llm-apis",
    "href": "chapter-5.html#available-cloud-based-llm-apis",
    "title": "Chapter 5 Cloud-based LLMs for Educational Research",
    "section": "1. Available Cloud-based LLM APIs",
    "text": "1. Available Cloud-based LLM APIs\n\n\n\nProvider\nExample Models\nAccess\nR or HTTP Wrapper\n\n\n\n\nOpenAI\nGPT-4o, GPT-4-Turbo\nAPI key from platform.openai.com\n{httr2}, {openai}\n\n\nAnthropic\nClaude 3, Claude 3.5\nAPI key from console.anthropic.com\n{httr2}, {anthropic}\n\n\nGoogle\nGemini 1.5\nAI Studio\n{googleGenerativeAI}\n\n\nMistral\nMixtral, Mistral 7B\nmistral.ai\nHTTP calls via {httr2}\n\n\n\n\n💡 Tip: Use .Renviron to store API keys securely. Avoid embedding them in public code repositories.",
    "crumbs": [
      "Section 3 — AI and LLMs in Educational Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5 **Cloud-based LLMs for Educational Research**</span>"
    ]
  },
  {
    "objectID": "chapter-5.html#connecting-to-an-api-in-r",
    "href": "chapter-5.html#connecting-to-an-api-in-r",
    "title": "Chapter 5 Cloud-based LLMs for Educational Research",
    "section": "2. Connecting to an API in R",
    "text": "2. Connecting to an API in R\nExample using the OpenAI API and the {httr2} package:\nlibrary(httr2)\nlibrary(jsonlite)\n\n# Prepare request\nresp &lt;- request(\"https://api.openai.com/v1/chat/completions\") |&gt;\n  req_headers(\n    \"Authorization\" = paste(\"Bearer\", Sys.getenv(\"OPENAI_API_KEY\")),\n    \"Content-Type\" = \"application/json\"\n  ) |&gt;\n  req_body_json(list(\n    model = \"gpt-4o-mini\",\n    messages = list(\n      list(role = \"system\", content = \"You are an assistant helping with educational data analysis.\"),\n      list(role = \"user\", content = \"Summarize these student reflections in three themes.\")\n    )\n  )) |&gt;\n  req_perform()\n\n# Parse and print\ncontent &lt;- resp_body_json(resp)\ncat(content$choices[[1]]$message$content)",
    "crumbs": [
      "Section 3 — AI and LLMs in Educational Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5 **Cloud-based LLMs for Educational Research**</span>"
    ]
  },
  {
    "objectID": "chapter-5.html#example-automating-qualitative-coding",
    "href": "chapter-5.html#example-automating-qualitative-coding",
    "title": "Chapter 5 Cloud-based LLMs for Educational Research",
    "section": "3. Example: Automating Qualitative Coding",
    "text": "3. Example: Automating Qualitative Coding\nLLMs can support qualitative analysis by:\n\nSuggesting initial codes or categories from open-ended responses\nClustering semantically similar responses\nGenerating summaries of student feedback\n\n# Example: clustering reflective comments\nresponses &lt;- c(\n  \"I learned how to write better R code.\",\n  \"Collaborating with peers improved my understanding.\",\n  \"I struggled with data visualization.\"\n)\n\nprompt &lt;- paste(\"Cluster these reflections into themes:\\n\", paste(responses, collapse = \"\\n- \"))\n\n# Send to API (pseudocode)\n# result &lt;- openai::create_chat_completion(model=\"gpt-4o-mini\", messages=list(...))",
    "crumbs": [
      "Section 3 — AI and LLMs in Educational Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5 **Cloud-based LLMs for Educational Research**</span>"
    ]
  },
  {
    "objectID": "chapter-5.html#reproducibility-and-ethical-considerations",
    "href": "chapter-5.html#reproducibility-and-ethical-considerations",
    "title": "Chapter 5 Cloud-based LLMs for Educational Research",
    "section": "4. Reproducibility and Ethical Considerations",
    "text": "4. Reproducibility and Ethical Considerations\nWhen using cloud-based LLMs:\n\nReproducibility: Model versions change frequently; document the model name, date, and API parameters.\nData Privacy: Avoid uploading identifiable student data. De-identify text prior to sending.\nCost and Rate Limits: Be aware of token-based billing; batch requests when possible.\nEthics and Transparency: Report clearly how LLMs were used in data analysis and interpretation.",
    "crumbs": [
      "Section 3 — AI and LLMs in Educational Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5 **Cloud-based LLMs for Educational Research**</span>"
    ]
  },
  {
    "objectID": "chapter-5.html#extending-to-other-apis",
    "href": "chapter-5.html#extending-to-other-apis",
    "title": "Chapter 5 Cloud-based LLMs for Educational Research",
    "section": "5. Extending to Other APIs",
    "text": "5. Extending to Other APIs\nYou can explore:\n\n{anthropic} or {claude} packages for Claude models\n{googleGenerativeAI} for Gemini API\n{mistralapi} for open-weight models hosted on Hugging Face or Mistral Cloud\n\nEach provides similar request structures and response handling via JSON.",
    "crumbs": [
      "Section 3 — AI and LLMs in Educational Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5 **Cloud-based LLMs for Educational Research**</span>"
    ]
  },
  {
    "objectID": "chapter-5.html#summary",
    "href": "chapter-5.html#summary",
    "title": "Chapter 5 Cloud-based LLMs for Educational Research",
    "section": "Summary",
    "text": "Summary\nCloud-based LLMs expand the researcher’s analytical toolkit, enabling scalable and automated text analysis in educational contexts.\nBy integrating these APIs with R-based workflows, researchers can:\n\nEfficiently analyze large qualitative datasets\nPrototype computational methods quickly\nMaintain transparency through code-based documentation\n\n\nIn the next chapter, we will shift our focus to local LLMs — exploring how models can be deployed and fine-tuned entirely offline for educational research.",
    "crumbs": [
      "Section 3 — AI and LLMs in Educational Research",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Chapter 5 **Cloud-based LLMs for Educational Research**</span>"
    ]
  },
  {
    "objectID": "chapter-6.html",
    "href": "chapter-6.html",
    "title": "Chapter 6 Local LLMs in Educational Research",
    "section": "",
    "text": "6.1 What are Local LLMs?\nAbstract:\nThe use of large language models (LLMs) in data analysis is rapidly increasing across education and social science research. However, concerns about data privacy, institutional data protection policies, and strict IRB (Institutional Review Board) procedures present significant challenges when using cloud-based or proprietary AI services. To address these challenges, this chapter introduces local LLM solutions—focusing on LM Studio—which allow researchers to run powerful models entirely on their own computers, ensuring data stays private and analysis remains flexible.\nLocal LLMs are large language models that run directly on your own computer, rather than in the cloud. By processing data locally, they help ensure privacy, data sovereignty, and compliance with institutional or governmental regulations. Local LLMs can be open-source (such as Llama, Qwen, DeepSeek, Mistral) and are compatible with various operating systems and hardware.\nKey advantages of local LLMs: - Data never leaves your computer - No need for external API keys or internet access to analyze sensitive data - Flexibility to use custom or open-source models - Often no usage fees",
    "crumbs": [
      "Section 3 — AI and LLMs in Educational Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6 **Local LLMs in Educational Research**</span>"
    ]
  },
  {
    "objectID": "chapter-6.html#what-can-local-llms-do",
    "href": "chapter-6.html#what-can-local-llms-do",
    "title": "Chapter 6 Local LLMs in Educational Research",
    "section": "6.2 What Can Local LLMs Do?",
    "text": "6.2 What Can Local LLMs Do?\nWith the right setup, local LLMs can: - Summarize, paraphrase, and analyze text data (open-ended survey responses, interview transcripts, etc.) - Support qualitative and quantitative educational research workflows - Generate coding frameworks, extract themes, or automate report writing - Perform document-based question answering (“chat with your PDFs”) - Integrate with other research tools via REST APIs",
    "crumbs": [
      "Section 3 — AI and LLMs in Educational Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6 **Local LLMs in Educational Research**</span>"
    ]
  },
  {
    "objectID": "chapter-6.html#getting-started-with-lm-studio",
    "href": "chapter-6.html#getting-started-with-lm-studio",
    "title": "Chapter 6 Local LLMs in Educational Research",
    "section": "6.3 Getting Started with LM Studio",
    "text": "6.3 Getting Started with LM Studio\nLM Studio is a free, cross-platform application that enables researchers to run, manage, and interact with local LLMs (such as Llama, DeepSeek, Qwen, Mistral, and gpt-oss) entirely on their own computers. By using LM Studio, you gain powerful, offline data analysis capabilities without sacrificing data privacy or compliance.\nKey Points: - Supported Platforms: macOS (Apple Silicon), Windows (x64/ARM64), and Linux (x64). - System Requirements: For best results, consult the System Requirements page for recommended RAM, CPU/GPU, and storage.\n\n6.3.1 Installation Steps\n\nDownload LM Studio for your operating system from the official Downloads page.\nInstall and launch the application.\nDownload your preferred LLM model (such as Llama 3, Qwen, Mistral, DeepSeek, or gpt-oss) directly from within LM Studio.\n(Optional) To use the API for scripting/automation, enable API access within LM Studio.\n(Optional) Attach documents for “Chat with Documents” (RAG-style analysis) entirely offline.\n\nOfficial Documentation:\n- LM Studio Docs - Getting Started Guide\n\n\n6.3.2 Main Features\n\nRun local models including Llama, Qwen, DeepSeek, Mistral, gpt-oss, and more.\nSimple chat interface for prompt-based interaction.\nOffline “Chat with Documents” for Retrieval Augmented Generation (RAG) use cases.\nSearch and download new models from Hugging Face and other model hubs within LM Studio.\nManage models, prompts, and configurations through a user-friendly GUI.\nServe local models on OpenAI-compatible REST API endpoints, usable by R, Python, or other apps.\nMCP server/client support for advanced use cases.\n\n\n\n6.3.3 API Integration\nLM Studio exposes a REST API fully compatible with the OpenAI standard. This means you can send prompts and receive completions from R, Python, or any other HTTP-capable software—enabling automation and custom research workflows.\nExample: Calling the LM Studio API from R\nLM Studio exposes a REST API compatible with the OpenAI API standard. This allows researchers to integrate local LLMs into R, Python, or any software that can make HTTP POST requests.\n\nlibrary(httr) \nlibrary(jsonlite)\n\nprompt \\&lt;- \"Summarize the following open-ended survey responses: ...\"\n\nresponse \\&lt;- POST( url = \"http://localhost:1234/v1/completions\", \n                   body = toJSON(list( prompt = prompt,\n                                       max_tokens = 200 ),\n                                 auto_unbox = TRUE),\n                   encode = \"json\" ) \ncontent(response) \n\n\n\n6.3.4 Summary Table of LM Studio Capabilities:\n\n\n\nFeature\nDescription\n\n\n\n\nLocal LLMs\nRun Llama, DeepSeek, Qwen, Mistral, etc. fully offline on your own machine\n\n\nChat Interface\nFlexible prompt-based interaction\n\n\nDocument Chat (RAG)\nOffline “chat with your documents”\n\n\nModel Management\nDownload, organize, and switch between models\n\n\nAPI Access\nOpenAI-compatible REST endpoints for use with R, Python, scripts, apps\n\n\nMCP Integration\nConnect with and use MCP servers\n\n\nCommunity & Support\nDiscord, official docs, active development",
    "crumbs": [
      "Section 3 — AI and LLMs in Educational Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6 **Local LLMs in Educational Research**</span>"
    ]
  },
  {
    "objectID": "chapter-6.html#case-study-comparing-local-llm-analysis-to-traditional-nlp-on-university-ai-policy-texts",
    "href": "chapter-6.html#case-study-comparing-local-llm-analysis-to-traditional-nlp-on-university-ai-policy-texts",
    "title": "Chapter 6 Local LLMs in Educational Research",
    "section": "6.4 Case Study: Comparing Local LLM Analysis to Traditional NLP on University AI Policy Texts",
    "text": "6.4 Case Study: Comparing Local LLM Analysis to Traditional NLP on University AI Policy Texts\n\n6.4.1 Research Question\nCan a local LLM running via LM Studio reliably identify key themes in university AI policy statements—using the same dataset analyzed in Section 2—so that we can compare its results against traditional NLP methods and human coding?\n\n\n6.4.2 Data Context\nWe reuse the AI policy statements dataset from Section 2, now simplified for privacy. The table has one column only:\n\nStance (character): policy text (no institution names)\n\nA typical structure (as seen in Section 2):\nWe will extract the same raw text field (Stance) so results are directly comparable to Section 2.\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(readr)\n\n# If 'university_policies' already exists (from Section 2), use it directly.\n# Otherwise, safely fall back to reading the same CSV used in Section 2.\nif (!exists(\"university_policies\")) {\n  university_policies &lt;- read_csv(\"University_GenAI_Policy_Stance.csv\", show_col_types = FALSE)\n}\n\nstopifnot(\"Stance\" %in% names(university_policies))\n\npolicy_texts &lt;- university_policies$Stance %&gt;%\n  as.character() %&gt;%\n  stringr::str_squish() %&gt;%\n  na.omit()\n\nlength(policy_texts)\n\n[1] 99\n\nhead(policy_texts, 3)\n\n[1] \"If the text generated by ChatGPT is used as a starting point for original research or writing, then it can be a useful tool for generating ideas and suggestions. In this case, it is important to properly cite and attribute the source of the information. ... However, if the text generated by ChatGPT is simply copied and pasted into a paper or report without any modifications, it can be considered plagiarism since the text isn’t original.\"                                                                                                                                                                                                                                                                   \n[2] \"Has ASU considered a ban on AI tools like other institutions such as NYU? No. ASU faculty and administrators are focused on the positive potential of Generative AI while also thinking through concerns about ethics, academic integrity, and privacy. ... What is being done to ensure academic integrity? The Provost’s Office is currently reviewing ASU’s academic integrity policy through the lens of what kind of content can be produced through generative AI and what kind of learning behaviors and outcomes are expected of students. ... Will I get accused of cheating if I use AI tools? Before using AI tools in your coursework, confer with your instructor about their class policy for using AI tools.\"\n[3] \"The following sample statements should be taken as starting points to craft your own policy. As of January 23, 2023, the Provost’s Office at BC has not issued a policy regarding the use of AI in coursework. ... Syllabus Statement 1 (Discourage Use of AI) ... Syllabus Statement 2 (Treat AI-generated text as a source)\"                                                                                                                                                                                                                                                                                                                                                                                              \n\n\n\n\n6.4.3 Implementation with LM Studio (Thematic Analysis)\nWe send the same policy texts to LM Studio’s local API using the parameters already defined in your setup (api_base, model_name).\nThe model openai/gpt-oss-20b runs locally in LM Studio and provides OpenAI-compatible endpoints. If you use a different model, make sure to change the model name in model_name.\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(glue)\nlibrary(stringr)\n\n# Use global parameters defined earlier\n# api_base and model_name should already be set in Section 6 setup:\napi_base &lt;- \"http://127.0.0.1:1234/v1\"\nmodel_name &lt;- \"openai/gpt-oss-20b\"\n\n\nTesting the Local Connection\nBefore running large jobs, it’s good practice to confirm that LM Studio is responding correctly. A quick “ping test” helps prevent silent connection errors.\n\nlibrary(httr)\nlibrary(jsonlite)\n\napi_base &lt;- \"http://127.0.0.1:1234/v1\"   # replace with your LM Studio endpoint\nmodel_name &lt;- \"openai/gpt-oss-20b\"       # adjust to your chosen model\n\nres &lt;- POST(\n  url = paste0(api_base, \"/chat/completions\"),\n  add_headers(\"Content-Type\" = \"application/json\"),\n  body = toJSON(list(\n    model = model_name,\n    messages = list(\n      list(role = \"system\", content = \"You are a helpful assistant.\"),\n      list(role = \"user\", content = \"Please reply with 'pong'\")\n    )\n  ), auto_unbox = TRUE)\n)\n\ncat(content(res)$choices[[1]]$message$content)\n\n\n✅ If the model replies with “pong,” the local API is ready.\n\n\n\nPrompt writing\nNext, we write our prompt. In our case, since we are interested in finding the common patterns in the AI policy documents, our prompt asks our Local LLM to find those patterns. What’s great here is we can ask it to create a data frame ready data for us. (Normally, if you pasted the text into the LM Studio chat box, you would get a narrative answer). Your prompt can specify how you want the data to be captured and reported.\n\n# ----- 1) Prompt Template -----\nanalysis_prompt_template &lt;- \"\nYou are analyzing official university AI policy statements.\nYour task is to identify 3–5 key themes across the statements and report them in the exact format below.\n\n**INPUT DATA:**\n- **Number of Statements:** {n_items}\n- **Policy Statements:**\n{items}\n\n**YOUR TASK:**\n1) Identify 3–5 key themes across the policy statements.\n2) For each theme:\n   a) Provide a concise theme name.\n   b) Provide a 1–2 sentence description.\n   c) Provide one short verbatim example quote.\n   d) Provide an integer Frequency (count of statements mentioning it).\n   e) Provide Relative Frequency as a whole-number percentage.\n3) Write a 3–5 sentence **Summary of Responses** synthesizing the most important insights.\n4) Output strictly in the following format:\n\n**Summary of Responses**\n[3–5 sentence narrative summary goes here.]\n\n**Thematic Table**\n| Theme | Description | Illustrative Example(s) | Frequency | Relative Frequency |\n|---|---|---|---|---|\n| [Theme 1] | [Description] | - \\\"[Quote]\\\" | [n] | [p]% |\n| [Theme 2] | [Description] | - \\\"[Quote]\\\" | [n] | [p]% |\n\"\n\n\n\nChunks!\nNext, we define chunk sizes for the local LLM to analyze our data. In qualitative text analysis using LLMs (such as thematic synthesis or coding), chunk size refers to the amount of text you pass to the model at one time. It directly affects coherence, depth, and efficiency of analysis.\nChunk size balances context preservation and analytic precision in qualitative LLM-based text analysis. If chunks are too small, the model loses semantic coherence, producing fragmented or repetitive themes. If too large, it may miss local nuances or exceed the model’s reasoning capacity. The aim is to maintain enough continuity for meaningful interpretation while staying within manageable input limits.\nPractically, chunk size should follow natural meaning units, such as paragraphs, speaker turns, or short sections, rather than fixed word counts. Researchers typically find that 500–1000 words work well for transcripts, while longer documents like policies can be chunked at 1000–1500 words. The guiding principle is to choose the smallest segment that preserves interpretive coherence.\n\n# ----- 2) Chunk the corpus to stay within model context window -----\nCHUNK_SIZE &lt;- 15\nchunks &lt;- split(policy_texts, ceiling(seq_along(policy_texts) / CHUNK_SIZE))\n\n\n\nConnecting to LM Studio\nOnce our data is prepared, our next step is to pass it to LM Studio. Using our function below, we send our text data to LM Studio server.\nWhat is key here is that we specify the model name, a “system” role defining the model’s expertise (in this case, qualitative research analyst), and the “user” role containing the analysis prompt. The parameters temperature = 0.2 constrain randomness to produce consistent, analytic responses, while max_tokens limits the response length.\n\nTemperature controls randomness: a low value (0.2) produces consistent, analytical responses suited to qualitative coding, while higher values encourage creativity but reduce reliability.\nMax tokens limits response length. Setting it to 1000 ensures sufficient detail without verbosity or truncation. Together, these parameters balance precision and completeness in model-generated analyses.\n\nIn essence, this helper encapsulates the logic of prompt dispatch and result retrieval, ensuring each call to the LLM is standardized and repeatable. This is crucial for qualitative workflows where traceability and parameter control are essential.\n\n# ----- 3) Helper function: call LM Studio (chat/completions endpoint) -----\ncall_lmstudio &lt;- function(prompt, max_tokens = 1000) {\n  res &lt;- httr::POST(\n    url = paste0(api_base, \"/chat/completions\"),\n    httr::add_headers(\"Content-Type\" = \"application/json\"),\n    body = jsonlite::toJSON(list(\n      model = model_name,\n      messages = list(\n        list(role = \"system\", content = \"You are an expert qualitative research analyst.\"),\n        list(role = \"user\", content = prompt)\n      ),\n      temperature = 0.2,\n      max_tokens = max_tokens\n    ), auto_unbox = TRUE)\n  )\n  httr::stop_for_status(res)\n  content(res)$choices[[1]]$message$content\n}\n\n\n\nRunning the analysis\n\n\nNow, the script applies the analysis_prompt_template to each chunk of transcript data using lapply(). Each chunk is converted into a numbered text block (items_block) and analyzed independently through call_lmstudio(), producing localized thematic results (chunk_outputs).\nSecond, the meta_prompt integrates these separate analyses. It instructs the model to synthesize and deduplicate themes across all chunks into a unified framework, including a concise narrative summary and a structured thematic table with descriptions, examples, and frequency data. Together, these steps move from micro-level coding to macro-level interpretation. This step is optional, and can be skipped depending on the nature of data and research questions.\n\n# ----- 4) Run thematic analysis per chunk -----\nchunk_outputs &lt;- lapply(chunks, function(vec) {\n  items_block &lt;- paste(sprintf(\"%d. %s\", seq_along(vec), vec), collapse = \"\\n\")\n  final_prompt &lt;- glue(analysis_prompt_template,\n                       n_items = length(vec),\n                       items   = items_block)\n  call_lmstudio(final_prompt)\n})\n\n# ----- 5) Merge all chunk-level analyses into a meta-synthesis -----\nmeta_prompt &lt;- \"\nYou will synthesize multiple chunk-level thematic analyses of the same corpus of university AI policies.\nUnify and deduplicate themes across chunks, and output a single consolidated section in the exact format below:\n\n**Summary of Responses**\n[3–5 sentence narrative summary.]\n\n**Thematic Table**\n| Theme | Description | Illustrative Example(s) | Frequency | Relative Frequency |\n|---|---|---|---|---|\n| [Unified Theme 1] | [Description] | - \\\"[Quote]\\\" | [n] | [p]% |\n| [Unified Theme 2] | [Description] | - \\\"[Quote]\\\" | [n] | [p]% |\n\"\n\n\n\nSynthesizing and Final LLM Analysis\nWe are now back in R synthising our data (and manage token limits efficiently).\nThe chunk_outputs are split into smaller pairs, each containing two analyses. Each pair is merged and passed through call_lmstudio() using the same meta_prompt, producing intermediate syntheses (pair_outputs). These summaries are then combined into a single consolidated input (final_meta_input) for a final call to call_lmstudio(), yielding the comprehensive meta-analysis (meta_output).\nThis iterative merging reduces token usage, preserves coherence, and ensures that the final synthesis integrates all thematic insights without exceeding model constraints. With saveRDS(meta_output, \"data/meta_output_saved.rds\") we save our analysis so that in the future, we can just start from there to pick things back up.\n\n# Pairwise synthesis to reduce token usage\npairs &lt;- split(chunk_outputs, ceiling(seq_along(chunk_outputs) / 2))\n\npair_outputs &lt;- lapply(pairs, function(group) {\n  meta_input &lt;- paste(group, collapse = \"\\n\\n---\\n\\n\")\n  call_lmstudio(paste(meta_prompt, meta_input, sep = \"\\n\\n\"))\n})\n\n# Now you have fewer intermediate syntheses\nfinal_meta_input &lt;- paste(pair_outputs, collapse = \"\\n\\n---\\n\\n\")\nmeta_output &lt;- call_lmstudio(paste(meta_prompt, final_meta_input, sep = \"\\n\\n\"))\ncat(meta_output)\n\n#saveRDS(meta_output, \"data/meta_output_saved.rds\")\nsaveRDS(meta_output, \"data/meta_output_saved.rds\")\n\n\n\nThematic Table Extraction and Cleaning\nThis code takes the saved meta-analysis from LM Studio and turns it into a clean, usable table in R. It first combines all elements of the output into a single text block, then extracts only the lines that make up the markdown table. Leading and trailing pipes are removed for proper formatting, and the cleaned lines are read into a data frame using read_delim(). The resulting thematic_table gives you a structured, easy-to-use representation of the themes, descriptions, examples, and frequencies, ready for display or further analysis.\n\nlibrary(stringr)\nlibrary(readr)\n\n# --- Read RDS ---\nmeta_output &lt;- readRDS(\"data/meta_output_saved.rds\")\n\n# --- Combine all elements into one long text block ---\nmeta_output_text &lt;- paste(meta_output, collapse = \"\\n\")\n\n# --- Extract markdown table rows ---\ntable_lines &lt;- str_subset(strsplit(meta_output_text, \"\\n\")[[1]], \"^\\\\|\")\n\n# --- Clean leading/trailing pipes ---\ntable_text &lt;- gsub(\"^\\\\||\\\\|$\", \"\", table_lines)\n\n# --- Convert to DataFrame ---\nthematic_table &lt;- read_delim(I(table_text), delim = \"|\", trim_ws = TRUE, show_col_types = FALSE)\n\n# --- Display result ---\nprint(thematic_table)\n\n# A tibble: 7 × 5\n  Theme        Description Illustrative Example…¹ Frequency `Relative Frequency`\n  &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;               \n1 ---          ---         ---                    ---       ---                 \n2 Academic In… Policies t… - “If a student uses … 13        25%                 \n3 Faculty Aut… Instructor… - “Different faculty … 12        23%                 \n4 Citation / … Students m… - “Under BU's guideli… 9         17%                 \n5 Conditional… Policies a… - “Instead of forbidd… 11        21%                 \n6 Pedagogical… Emphasis o… - “Propose alternativ… 4         8%                  \n7 Policy Evol… Recognitio… - “Universities will … 3         6%                  \n# ℹ abbreviated name: ¹​`Illustrative Example(s)`\n\n\n\n\n6.4.3.1 Saving and Exporting Results\nAfter obtaining the meta_output from the local LLM, we can inspect, export, and reuse the results in various formats for further analysis or publication.\n\n# --- View output in the console ---\ncat(substr(meta_output, 1, 1000))  # Preview the first 1000 characters\n# or simply\ncat(meta_output)\n\n# --- Save the full result as a text or Markdown file ---\nwriteLines(meta_output, \"lmstudio_meta_output.txt\")\nwriteLines(meta_output, \"lmstudio_meta_output.md\")\n\n\n# --- Extract and save the Thematic Table as CSV ---\nlibrary(stringr)\nlibrary(readr)\n\n# Extract only the markdown table lines (beginning with |)\ntable_lines &lt;- str_subset(strsplit(meta_output, \"\\n\")[[1]], \"^\\\\|\")\ntable_text  &lt;- gsub(\"^\\\\||\\\\|$\", \"\", table_lines)\n\n# Convert to data frame\nthematic_table &lt;- read_delim(I(table_text), delim = \"|\", trim_ws = TRUE, show_col_types = FALSE)\n\n# Save to CSV for further analysis or visualization\nwrite_csv(thematic_table, \"lmstudio_thematic_table.csv\")\n# Save the full output as a Markdown file for easy sharing \nwriteLines(meta_output, \"lmstudio_meta_output_full.md\")\n\n# Optional: check where the file was saved\ngetwd()\n\n\n\n6.4.3.2 Practical Notes on Running Local Models 🍕💻\nRunning a local LLM inside LM Studio can feel magical: your computer becomes its own private AI research lab. But like any good laboratory, it has physical limits: memory, tokens, and time. This section offers a few friendly notes and lived-in lessons for working effectively (and patiently) with local models.\n\nTokens Are Like Bites of Pizza\nLM Studio may be a powerful local model playground, but it still has limits. Think of tokens as bites of pizza: your model can chew through a few generous slices, but handing it the entire pizza (for example, your full corpus of 99 policy statements) in one go will only lead to indigestion (also known as the dreaded “HTTP 400 Bad Request.”)\nEvery model has a context window (often 8 k – 32 k tokens). Both your prompt and the expected response must fit inside this box.\nWhen in doubt:\nFeed your model smaller slices.\nReduce CHUNK_SIZE or truncate long texts (for instance, use only the first 400–500 characters of each document).\nAdjust your max_tokens parameter.\nFewer output tokens make for shorter, faster, and safer runs.\nMonitor your total prompt length.\nBefore sending a request, check nchar(prompt): if it returns more than 20 000 characters, you are probably over the limit.\n\n\nComputing Resources and Patience\nExpect variable response times.\nLM Studio runs fully on your own hardware; response time depends on CPU/GPU power and corpus size.\nAn 8-billion-parameter model will typically take a few seconds per completion; larger models may need minutes.\nMind your system memory.\nKeep background applications light and avoid running multiple models simultaneously. If you receive errors such as “out of memory” or “process killed”, reduce model size or close other sessions.\nPro tip from the authors:\nDuring long qualitative runs, go play a game of basketball, take a walk, or grab a coffee. The LLM will still be digesting its token pizza when you return.\n\n\nFile Paths, Caching, and Stability\nUse consistent file paths.\nSave outputs (meta_output.md, thematic_table.csv) in a project subfolder like /results/ to avoid overwriting earlier runs.\nEnable model caching in LM Studio.\nCached models load faster after the first use and reduce memory spikes.\nRestart occasionally.\nLong local sessions can accumulate memory fragmentation; restarting LM Studio or your R session ensures stable performance.\n\n\nTakeaways\nFeed your model thoughtfully—one well-prepared prompt at a time—and you’ll get cleaner, faster, and tastier results. Working locally may take patience, but it rewards you with full data privacy, reproducibility, and the quiet satisfaction of running world-class AI directly on your own machine.\n\n\n\n\n6.4.4 Sample Output\nBelow is the authentic output generated by the local model openai/gpt-oss-20b in LM Studio when analyzing all 99 AI-policy statements.\nThis result directly mirrors the traditional NLP analysis in Section 2, providing a clear basis for methodological comparison.\nSummary of Responses Across the surveyed universities, a shared priority is safeguarding academic integrity while allowing instructors to tailor AI-use rules at the course level. Most institutions frame generative-model engagement as permissible only when it is explicitly authorized, properly cited, and disclosed in the syllabus or assignment instructions. Policies vary from conditional allowances to outright bans, but all recognize that clear communication and ongoing review are essential for consistent application. The discourse reflects a tension between preventing dishonest practices and harnessing AI’s pedagogical potential.\nThematic Table\n\n\n\n\n\n\n\n\n\n\nTheme\nDescription\nIllustrative Example(s)\nFrequency\nRelative Frequency\n\n\n\n\nAcademic Integrity / Plagiarism\nPolicies treat un-attributed or unauthorized AI output as cheating, requiring adherence to existing honor-code standards.\n- “If a student uses text generated from ChatGPT and passes it off as their own writing… they are in violation of the university’s academic honor code.” (Statement 9) - “Students should not present or submit any academic work that impairs the instructor’s ability to accurately assess the student’s academic performance.” (Statement 2)\n13\n25%\n\n\nFaculty Autonomy & Syllabus Clarity\nInstructors are empowered to set, communicate, and enforce AI-use rules within their courses, often via the syllabus or early course materials.\n- “Different faculty will have different expectations about whether and how students can use AI tools, so being transparent about your expectations is essential.” (Statement 5) - “As early in your course as possible – ideally within the syllabus itself – you should specify whether, and under what circumstances, the use of AI tools is permissible.” (Statement 7)\n12\n23%\n\n\nCitation / Disclosure Requirements\nStudents must explicitly credit AI-generated content or document their interactions to avoid plagiarism.\n- “Under BU’s guidelines… students must give credit to them whenever they’re used… include an appendix detailing the entire exchange with an LLM.” (Statement 4) - “You must cite your use of these tools appropriately. Not doing so violates the HBS Honor Code.” (Statement 7)\n9\n17%\n\n\nConditional AI Use Guidelines\nPolicies allow or prohibit AI on a case-by-case basis, encouraging faculty to assess pedagogical fit rather than imposing blanket bans.\n- “Instead of forbidding its use, however, we might investigate which questions AI poses for us as teachers and for our students as learners.” (Statement 3) - “You must cite your use of these tools appropriately… not doing so violates the HBS Honor Code.” (Statement 7)\n11\n21%\n\n\nPedagogical Integration & Assessment Design\nEmphasis on designing assignments that preserve skill development while leveraging AI benefits, and on re-thinking assessment strategies.\n- “Propose alternative assignments or assessments if there is the chance that students might use the tool to misrepresent the output from ChatGPT as their own.” (Statement 10) - “Ideally, we would come to a place where this technology can be integrated into our instruction in meaningful ways…” (Statement 7)\n4\n8%\n\n\nPolicy Evolution & Ongoing Review\nRecognition that AI guidelines are fluid and require regular updates in response to technological change.\n- “Universities will need to constantly stay aware of what is going on with ChatGPT… make updates to their policies at least once a year.” (Statement 13)\n3\n6%\n\n\n\n\n\n6.4.5 Human Validation (Assessing the Accuracy of LM Studio’s Thematic Extraction)\nWhile the local LLM produced a structured and coherent thematic analysis, it is essential to evaluate how accurate these automatically generated themes are before treating them as valid research findings.\nHuman validation ensures that the AI’s interpretation aligns with the researcher’s own understanding of the data—a cornerstone of qualitative rigor.\n\n6.4.5.1Manual Validation Procedure\nFor this validation, a small group of human coders (or the original researcher) reviewed each of the six themes generated by LM Studio.\nThey independently rated whether the theme name, description, and illustrative examples accurately represented the corresponding text excerpts in the original corpus.\nEach theme was labeled as:\n\n✅ True – the theme correctly captures a coherent and relevant concept found in the corpus.\n\n❌ False – the theme is misleading, redundant, or unsupported by the text.\n\n\nExample Validation Table\n\n\n\n\n\n\n\n\nLLM-Generated Theme\nHuman Judgment\nComment Summary\n\n\n\n\nAcademic Integrity / Plagiarism\n✅ True\nStrongly supported by multiple statements referencing honor codes and plagiarism.\n\n\nFaculty Autonomy & Syllabus Clarity\n✅ True\nMatches explicit institutional language about syllabus-level discretion.\n\n\nCitation / Disclosure Requirements\n✅ True\nDirectly evidenced by quotes requiring citation or appendices.\n\n\nConditional AI Use Guidelines\n✅ True\nConsistent with texts describing conditional permissions.\n\n\nPedagogical Integration & Assessment Design\n✅ True\nAccurately summarizes emerging pedagogical considerations.\n\n\nPolicy Evolution & Ongoing Review\n✅ True\nWell-grounded in statements about policy updates and future revisions.\n\n\n\nValidation Accuracy: 6 / 6 = 100 % (illustrative)\n\nIn practice, partial matches and ambiguous cases can occur.\nResearchers may use a three-point scale (“Accurate,” “Partially Accurate,” “Inaccurate”) to capture nuance.\n\n\n\nR Code for Recording and Calculating Accuracy\nResearchers can document their manual judgments in R and compute simple metrics.\n\nlibrary(dplyr)\n\n# Example: human evaluation of LM Studio themes\n\nvalidation_data &lt;- tibble::tibble( Theme = c(\"Academic Integrity / Plagiarism\", \"Faculty Autonomy & Syllabus Clarity\", \"Citation / Disclosure Requirements\", \"Conditional AI Use Guidelines\", \"Pedagogical Integration & Assessment Design\", \"Policy Evolution & Ongoing Review\"), Human_Judgment = c(TRUE, TRUE, TRUE, TRUE, TRUE, TRUE), Comment = c(\"Clearly defined theme\", \"Matches source texts precisely\", \"Accurate and well-evidenced\", \"Appropriate scope\", \"Valid pedagogical dimension\", \"Accurately reflects iterative nature of policies\") )\n\n# Calculate proportion of themes rated TRUE\n\nvalidation_accuracy &lt;- mean(validation_data$Human_Judgment)\n\nsprintf(\"Validation Accuracy: %.1f%%\", 100 * validation_accuracy)\n\n[1] \"Validation Accuracy: 100.0%\"\n\nprint(validation_data)\n\n# A tibble: 6 × 3\n  Theme                                       Human_Judgment Comment            \n  &lt;chr&gt;                                       &lt;lgl&gt;          &lt;chr&gt;              \n1 Academic Integrity / Plagiarism             TRUE           Clearly defined th…\n2 Faculty Autonomy & Syllabus Clarity         TRUE           Matches source tex…\n3 Citation / Disclosure Requirements          TRUE           Accurate and well-…\n4 Conditional AI Use Guidelines               TRUE           Appropriate scope  \n5 Pedagogical Integration & Assessment Design TRUE           Valid pedagogical …\n6 Policy Evolution & Ongoing Review           TRUE           Accurately reflect…\n\nprint(validation_accuracy) \n\n[1] 1\n\n\n\n\n\n6.4.5.2 Quantitative Cross-Validation (Comparing Theme Frequencies)\nAfter obtaining the thematic results from LM Studio, researchers can test their reliability by comparing them against traditional keyword-based validation.\nThis section walks through that process step by step — showing how quantitative checks can complement qualitative interpretation.\n\nStep 1: Concept and Rationale\nWhile LLMs identify themes semantically, we can independently verify their consistency by checking whether the same ideas appear through explicit keywords in the original texts.\nThis serves as a quantitative cross-check between two perspectives:\n\nLM Studio output — interprets meaning through context.\n\nKeyword-based validation — detects literal word usage.\n\nThe goal is not to “prove” one right, but to measure how closely the two align.\n\n\nStep 2: Load and Prepare the Data\nWe load both the original policy corpus and the LLM-generated thematic table.\n\n# ========================================\n# Step 2 — Load data\n# ========================================\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\npolicies &lt;- university_policies %&gt;%\n  mutate(Stance = as.character(Stance))\n\nllm_table &lt;- read_csv(\"lmstudio_thematic_table.csv\", show_col_types = FALSE)\n\n\nHere, policies contains the raw text statements, and llm_table includes the theme frequencies produced by the LLM.\n\n\n\nStep 3: Define Keyword Anchors\nNext, we define a manual codebook of lexical cues for each theme.\nThese act as anchors for literal keyword detection and can be refined later.\n\n# ========================================\n# Step 3 — Define theme keywords\n# ========================================\n\ntheme_keywords &lt;- list(\n  \"Academic Integrity / Plagiarism\" = c(\"plagiarism\", \"honor code\", \"academic integrity\", \"cheating\"),\n  \"Faculty Autonomy & Syllabus Clarity\" = c(\"syllabus\", \"faculty\", \"instructor\", \"autonomy\", \"course policy\"),\n  \"Citation / Disclosure Requirements\" = c(\"cite\", \"citation\", \"disclose\", \"acknowledge\", \"appendix\"),\n  \"Conditional AI Use Guidelines\" = c(\"case by case\", \"permission\", \"approval\", \"allowed\", \"not permitted\"),\n  \"Pedagogical Integration & Assessment Design\" = c(\"assignment\", \"assessment\", \"learning\", \"instruction\", \"pedagog\"),\n  \"Policy Evolution & Ongoing Review\" = c(\"update\", \"revise\", \"review\", \"change\", \"evolve\")\n)\n\n\nEach key in the list corresponds to a theme, and each value contains search terms representing that theme’s literal vocabulary.\n\n\n\n\nStep 4: Count Keyword Occurrences\nWe now create a helper function to count how many policy statements mention any of the keywords for a given theme.\n\n# ========================================\n# Step 4 — Count keyword matches\n# ========================================\n\ncount_theme_mentions &lt;- function(text, keywords) {\n  pattern &lt;- paste(keywords, collapse = \"|\")\n  str_detect(tolower(text), pattern)\n}\n\n\nThis function returns TRUE if a policy contains any of the keywords and FALSE otherwise.\nWe’ll use it to compute frequency counts across all statements.\n\n\n\n\nStep 5: Compute Validation Metrics\nWe apply the counting function to every theme and summarize the results into verified frequencies and percentages.\n\n# ========================================\n# Step 5 — Apply validation across the corpus\n# ========================================\n\nvalidation_results &lt;- lapply(names(theme_keywords), function(theme) {\n  keywords &lt;- theme_keywords[[theme]]\n  matches &lt;- sapply(policies$Stance, count_theme_mentions, keywords = keywords)\n  tibble(\n    Theme = theme,\n    Verified_Frequency = sum(matches),\n    Verified_Relative = round(100 * mean(matches), 1)\n  )\n}) %&gt;% bind_rows()\n\n\nThe resulting validation_results table shows how often each theme literally appears in the text according to keyword matching.\n\n\n\n\nStep 6: Merge with LLM Results\nTo compare both approaches side by side, we merge the keyword-verified counts with the LLM-reported frequencies.\n\n# ========================================\n# Step 6 — Merge and clean data\n# ========================================\n\nvalidation_compare &lt;- llm_table %&gt;%\n  select(\n    Theme,\n    LLM_Frequency = Frequency,\n    LLM_Relative  = `Relative Frequency`\n  ) %&gt;%\n  left_join(validation_results, by = \"Theme\") %&gt;%\n  mutate(\n    LLM_Frequency      = as.numeric(LLM_Frequency),\n    LLM_Relative       = readr::parse_number(LLM_Relative),\n    Verified_Frequency = as.numeric(Verified_Frequency),\n    Verified_Relative  = as.numeric(Verified_Relative),\n    Freq_Diff          = Verified_Frequency - LLM_Frequency,\n    Rel_Diff           = Verified_Relative - LLM_Relative\n  ) %&gt;%\n  filter(!is.na(Theme), Theme != \"\", Theme != \"---\")\n\n\nAfter cleaning, each row shows both sets of frequencies plus their differences.\nThese metrics help identify where the model may under- or over-estimate a theme relative to literal keyword evidence.\n\n\n\n\nStep 7: Visualize the Comparison\nFinally, we visualize the relative frequencies from both methods.\n\n# ========================================\n# Step 7 — Visualization\n# ========================================\n\nvalidation_compare_long &lt;- validation_compare %&gt;%\n  select(Theme, LLM_Relative, Verified_Relative) %&gt;%\n  pivot_longer(-Theme, names_to = \"Source\", values_to = \"Relative_Frequency\")\n\nggplot(validation_compare_long, aes(\n  x = reorder(Theme, Relative_Frequency),\n  y = Relative_Frequency,\n  fill = Source)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  coord_flip() +\n  scale_fill_manual(values = c(\"LLM_Relative\" = \"#FF6F61\", \"Verified_Relative\" = \"#00BFC4\")) +\n  labs(\n    title = \"Cross-Validation of LM Studio Theme Frequencies\",\n    x = \"Theme\",\n    y = \"Relative Frequency (%)\",\n    caption = \"Comparison between LM Studio-reported and keyword-verified frequencies\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe red bars show LLM estimates; the blue bars represent keyword matches.\nAlignment between them suggests that the model’s semantic themes correspond closely to literal textual evidence.\n\n\n\n\nStep 8: Statistical Consistency Check\nWe can further quantify the alignment by computing a simple Pearson correlation.\n\ncor(validation_compare$LLM_Relative,\n    validation_compare$Verified_Relative,\n    use = \"complete.obs\")\n\n[1] 0.4053206\n\n# ≈ 0.7\n\n\nA correlation around r ≈ 0.7 indicates a strong positive relationship —\nthe model and the keyword method identify and rank themes in similar ways.\n\n\n\n\nStep 9: Interpretation and Reflection\nThis quantitative validation highlights two complementary lenses:\n\n\n\n\n\n\n\n\n\nApproach\nFocus\nStrength\nLimitation\n\n\n\n\nKeyword-based Validation\nWhat is said\nHigh recall, transparent rules\nLiteral, may overcount\n\n\nLLM Semantic Analysis\nWhat is meant\nContext-aware, concise, human-like reasoning\nMay undercount subtle mentions\n\n\n\nThe LLM acts like a careful qualitative coder: it labels only when meaning is clear,\nwhereas keyword search counts every literal appearance.\nTogether, these methods confirm that LM Studio’s local model captures the same conceptual contours as human reasoning,\nbalancing interpretive depth with computational scalability.\n\nAs one co-author joked, “The LLM doesn’t just read the policy—it understands the syllabus.”\n\n\n\n\nStep 10: Refining the Keyword Definitions\nBecause keyword validation depends entirely on how theme_keywords is defined, it’s worth experimenting with precision vs. recall.\nFor example:\n\n\"Pedagogical Integration & Assessment Design\" =\n  c(\"assignment design\", \"course design\", \"learning outcomes\",\n    \"assessment method\", \"rubric\", \"instructional strategy\")\n\nNarrowing the expressions from single words (learning, assessment) to multi-word phrases improves conceptual accuracy\nand aligns frequencies more closely with LLM estimates.\n\n\n\n\n\n\n\n\nObjective\nKeyword Strategy\nEffect\n\n\n\n\nIncrease accuracy\nUse multi-word expressions (e.g., “academic integrity,” “honor code”)\nReduces false positives\n\n\nIncrease recall\nInclude variants (e.g., “cite,” “citation,” “acknowledge”)\nCaptures more instances\n\n\nBalance both\nMix general and specific terms\nMaximizes validity\n\n\n\nBy tuning these lists, researchers can “dial in” their validation strictness and calibrate the model’s semantic reasoning against transparent rules.\n\n\nInterpreting the Cross-Validation Results\nThe cross-validation process compared two perspectives on the same corpus:\n(1) the LM Studio semantic model output (LLM_Relative) and\n(2) a keyword-based verification (Verified_Relative) drawn directly from the AI policy statements.\n\n\nSummary of Observed Patterns\n\n\n\n\n\n\n\n\n\nTheme\nLLM_Relative (%)\nVerified_Relative (%)\nInterpretation\n\n\n\n\nAcademic Integrity / Plagiarism\n25.0\n49.5\nThe model is more conservative; only tags clear cases of academic misconduct.\n\n\nFaculty Autonomy & Syllabus Clarity\n23.0\n56.6\nBoth methods agree this is a dominant theme, though LLM captures fewer instances.\n\n\nCitation / Disclosure Requirements\n17.0\n25.3\nClose alignment; both approaches identify similar occurrences.\n\n\nConditional AI Use Guidelines\n21.0\n14.1\nThe LLM slightly exceeds keyword detection, showing semantic inference ability.\n\n\nPedagogical Integration & Assessment Design\n8.0\n50.5\nThe widest gap—keywords overcount, while LLM limits to truly instructional contexts.\n\n\nPolicy Evolution & Ongoing Review\n6.0\n5.1\nNearly identical, confirming that low-frequency topics were also captured accurately.\n\n\n\n\n\nInterpretation\nThis difference reflects two complementary ways of understanding text:\n\n\n\n\n\n\n\n\n\nApproach\nFocus\nStrength\nLimitation\n\n\n\n\nKeyword-based Validation\nWhat is said\nHigh recall, transparent rules\nLiteral, may overcount\n\n\nLLM Semantic Analysis\nWhat is meant\nContext-aware, concise, human-like reasoning\nMay undercount subtle mentions\n\n\n\nIn other words, the LLM acts like an experienced qualitative researcher:\nit does not label a statement as “Pedagogical Integration” merely because the word assessment appears.\nInstead, it requires conceptual coherence—only assigning that theme when the sentence genuinely discusses teaching or evaluation design.\n\n\nQuantitative Validation Conclusion\nOverall, the validation demonstrates that LM Studio’s local model captures the same conceptual contours as human logic,but with tighter semantic precision.\nWhile keyword methods “count what appears,” the LLM “counts what matters.”\nThis finding supports the broader methodological argument of this chapter:\nlocal LLMs can perform qualitative analysis with high interpretive fidelity while preserving privacy and reproducibility— a valuable balance between computational scalability and human-level understanding.\n\nAs one of the authors quipped: “The LLM doesn’t just read the policy—it understands the syllabus.”\n\n\n\nThe Role of Keyword Definitions in Validation Accuracy\nThe accuracy of the cross-validation results depends critically on how the theme_keywords list is defined.\nThis list serves as the manual codebook that translates each thematic label into a set of lexical cues used to verify whether a statement in the corpus reflects that theme.\nIn other words, while LM Studio interprets themes semantically, the keyword-based approach verifies them literally—and the way these keywords are chosen directly affects the outcome.\n\n\nThe Sensitivity of Keyword Matching\nFor instance, consider the theme:\n\n\"Pedagogical Integration & Assessment Design\" = \n  c(\"assignment\", \"assessment\", \"learning\", \"instruction\", \"pedagog\")\n\nThis set captures a wide range of common words such as learning and assessment, which appear frequently in almost all policy statements.\nAs a result, the keyword-based validation counts nearly half of the corpus as related to pedagogy (≈ 50%),\nwhereas the LM Studio model, which identifies themes only when the semantic context genuinely involves teaching design, reports a much lower frequency (≈ 8%).\nHere, the discrepancy arises not because the model “missed” something, but because the keywords were too general.\nWhen the same theme is redefined more precisely:\n\n\"Pedagogical Integration & Assessment Design\" = \n  c(\"assignment design\", \"course design\", \"learning outcomes\",\n    \"assessment method\", \"rubric\", \"instructional strategy\")\n\nthe validated frequencies drop and begin to converge with the model’s estimates.\nThis adjustment increases conceptual precision while slightly reducing recall—a desirable trade-off for qualitative research.\n\n\nBalancing Precision and Recall\n\n\n\n\n\n\n\n\nObjective\nKeyword Strategy\nEffect\n\n\n\n\nIncrease accuracy\nUse multi-word expressions (e.g., “academic integrity,” “honor code”) rather than single words\nReduces false positives\n\n\nIncrease recall\nInclude common variants (e.g., “cite,” “citation,” “credit,” “acknowledge”)\nCaptures more relevant instances\n\n\nBalance both\nCombine general terms with specific phrases\nMaximizes validity and interpretive robustness\n\n\n\nIn practice, tuning the keyword definitions allows researchers to “dial in” the strictness of their validation procedure.\nA broader set yields higher apparent frequencies but risks counting superficial mentions;\na narrower set lowers counts but aligns more closely with human-coded judgments.\n\n\nInterpretation\nThis behavior illustrates a deeper methodological point:\nkeyword validation tests the literal presence of ideas,\nwhile LLM-based thematic extraction tests their conceptual expression.\nBoth perspectives are useful.\nBy iteratively refining the theme_keywords list, researchers can improve agreement (often raising correlation from r ≈ 0.7 to 0.8 or higher)\nand use this process to calibrate their model’s semantic reasoning against transparent, rule-based criteria.\nUltimately, the keyword definitions act as a bridge between human and machine understanding:\nthey remind us that accuracy is not merely about counting words, but about ensuring that meaning—and not just language—aligns across analytical methods.\n\n\n\n\n6.4.5.3 Case Study Discussion\nThe central research question guiding this case study was:\nCan a local LLM running through LM Studio accurately identify and summarize the key themes within university AI policy statements, while maintaining data privacy and interpretive reliability?\nThe analyses presented in this section—spanning semantic extraction, human validation, and keyword-based cross-verification—provide a strong, evidence-based answer: Yes, within its operational limits, a local LLM can perform thematic analysis with high conceptual accuracy and semantic coherence.\n\nKey Findings\n\nSemantic Precision:\nThe local LLM captured major thematic patterns consistent with those derived from human coding and keyword verification, particularly around academic integrity, faculty autonomy, and disclosure requirements.\nIts lower raw frequencies reflect a more selective, meaning-oriented approach rather than literal word matching.\nInterpretive Consistency:\nThe cross-validation results (r ≈ 0.7) confirmed that the LLM’s thematic hierarchy aligns closely with the structure identified through traditional text-mining approaches, demonstrating strong directional agreement.\nReliability Through Validation:\nHuman reviewers judged all six LLM-generated themes to be conceptually sound and textually supported.\nThis validation indicates that locally deployed models, when carefully prompted and verified, can produce outputs of research-grade quality.\nEfficiency and Ethics:\nBy running entirely offline, LM Studio ensured complete data sovereignty—no institutional text left the researcher’s machine.\nThis model of “computational privacy” offers a practical solution for studies constrained by IRB or institutional data-protection requirements.\n\n\n\nAnswer to the Research Question\nTaken together, these results suggest that local LLMs can replicate and, in some respects, enhance traditional qualitative workflows.\nThey are capable of identifying semantically rich, human-like themes without compromising ethical or privacy standards.\nRather than replacing human judgment, such models act as intelligent collaborators—speeding up initial coding, highlighting latent relationships, and supporting iterative analysis.\n\n\nLimitations and Future Testing\nThe analysis also revealed several caveats that future researchers should note:\n\nThe model’s token window constrains how much text can be processed at once.\nLonger corpora require chunking or synthesis steps, which may introduce variability.\n\nThe accuracy of cross-validation is sensitive to keyword definition, emphasizing the importance of transparent, well-constructed codebooks.\n\nResponse times and processing costs scale with model size; while small models run quickly, larger ones yield richer, more nuanced outputs.\n\nThese limitations do not undermine the results but instead point toward a maturing workflow—one in which human interpretive oversight and local AI capabilities complement each other.\nIn summary, this case study demonstrates that a locally hosted LLM can achieve credible thematic analysis outcomes on complex educational policy texts while upholding privacy, transparency, and methodological rigor.\nThis provides a practical and ethical blueprint for integrating LLMs into future qualitative research in education.\n\n\n\n6.4.6 Reflection\nThe case study presented in this section demonstrates how a local large language model (LLM)—running entirely within LM Studio—can be integrated into an educational research workflow to conduct qualitative thematic analysis at scale, securely, and with interpretive depth.\n\nFrom Tokens to Meaning\nTraditional NLP methods, as explored in Section 2, rely heavily on token-level processing:\nword frequencies, co-occurrence patterns, and topic modeling through statistical clustering.\nThese approaches excel at quantifying surface features of text but often struggle to capture the intent or tone embedded in policy language.\nIn contrast, the local LLM used here reasons across sentences and paragraphs.\nIt identifies not only recurring words such as plagiarism or syllabus but also the conceptual relationships that bind them—what the policy means rather than what it merely says.\nThe result is a smaller set of semantically coherent themes that resemble human-coded outputs in structure and emphasis.\nThe cross-validation exercise (Sections 6.4.5–6.4.5.3) confirmed this distinction empirically:\nthe LLM produced lower absolute frequencies yet mirrored the same thematic hierarchy found by keyword verification (r ≈ 0.7).\nIn short, the machine did not count more—it understood better.\n\n\nComplementarity, Not Replacement\nRather than viewing LLMs as replacements for traditional NLP, we should see them as complementary instruments in the researcher’s toolkit.\nConventional text mining offers transparency and replicability;\nLLMs contribute context, nuance, and synthesis.\nWhen combined, the two form a hybrid analytic ecology—where numbers inform narratives and narratives refine numbers.\nFor example, word clouds and TF-IDF analyses (from Section 2) remain invaluable for preliminary exploration, helping to locate linguistic hotspots.\nOnce those areas are identified, local LLMs can step in to interpret why those patterns exist, drawing out themes that statistical models alone cannot articulate.\n\n\nPrivacy and Practicality\nEqually important is the ethical and logistical dimension.\nBy running entirely on a researcher’s own device, LM Studio ensures that no sensitive institutional data leaves the local environment.\nThis design resolves many IRB-related concerns and allows experimentation in restricted research contexts where cloud-based AI services would be prohibited.\nThe workflow does, however, require patience.\nLarge local models consume time and computation—an experience not unlike waiting for a slow-baked pizza.\nAs we advised earlier, this is the perfect moment to step away, stretch, or play a quick game of basketball while the model “thinks.”\nIn return, you receive an analysis that is private, interpretable, and genuinely your own.\n\n\nLooking Ahead: From Analysis to Collaboration\nThe lessons from this section mark a transition from computational text analysis to intelligent collaboration with models.\nThe local LLM is not just a faster coding assistant; it is an emerging research partner capable of summarizing, classifying, and reasoning across multimodal data.\nIn future research, this approach can be extended beyond text—exploring how LLMs may support the analysis of images, videos, surveys, and multimodal learning artifacts while maintaining the same principles of privacy, transparency, and reproducibility.\n\nIn summary:\nSection 2 taught us how to count words;\nSection 6 showed us how machines can interpret meaning—securely, locally, and collaboratively.\nTogether, they illuminate a continuum of computational methods for educational research,\nbridging the measurable and the meaningful, the statistical and the semantic, the algorithmic and the human.",
    "crumbs": [
      "Section 3 — AI and LLMs in Educational Research",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chapter 6 **Local LLMs in Educational Research**</span>"
    ]
  },
  {
    "objectID": "chapter-7.html",
    "href": "chapter-7.html",
    "title": "Chapter 7 Multimodal Data (Images, Video, Audio) with Local LLMs",
    "section": "",
    "text": "7.1 Overview\nIn this section, we will discuss how social scientists can move beyond traditional data types (e.g., text and numbers) and learn about capturing and analyzing multimodal data.\nMultimodal data includes audio, video, and other non‑textual information that gives a fuller picture of human behavior.\nModern devices such as wearables, smartphones, and online platforms now let researchers collect large amounts of this mixed data.\nTo make sense of it, we use computational tools that combine the different types:\nThese tools let researchers ask new questions. For example, how body language and tone of voice together affect conversations or how physiological signals match feelings can help uncover insights that single‑mode studies miss. By integrating multimodal data, social scientists can broaden the depth and reach of their research beyond what conventional single‑mode analysis offers.",
    "crumbs": [
      "Section 3 — AI and LLMs in Educational Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7 **Multimodal Data (Images, Video, Audio) with Local LLMs**</span>"
    ]
  },
  {
    "objectID": "chapter-7.html#overview",
    "href": "chapter-7.html#overview",
    "title": "Chapter 7 Multimodal Data (Images, Video, Audio) with Local LLMs",
    "section": "",
    "text": "image analysis for video frames,\nvoice‑to‑text software for audio, and\nmachine‑learning models that link text, pictures, and sensor signals.",
    "crumbs": [
      "Section 3 — AI and LLMs in Educational Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7 **Multimodal Data (Images, Video, Audio) with Local LLMs**</span>"
    ]
  },
  {
    "objectID": "chapter-7.html#images",
    "href": "chapter-7.html#images",
    "title": "Chapter 7 Multimodal Data (Images, Video, Audio) with Local LLMs",
    "section": "7.2 Images",
    "text": "7.2 Images\nImage data can come from the usual sources such as field photographs taken during site visits, archival collections in libraries or museums, and printed photographs that appear in historical documents. Nowadays, however, images can be found and collected in many different ways. For example, social media platforms like Instagram, Facebook, and TikTok are rich with user‑generated photos; online photo repositories such as Flickr, Unsplash, and Wikimedia Commons host millions of images that are freely accessible; news outlets regularly publish photographs to accompany stories; satellite imagery from NASA or ESA provides large‑scale visual data; and everyday smartphone cameras capture images that can be shared in research settings. Please note that although we cover some prominent ways, this is by no means an exhaustive list. Therefore, please refer to the additional resources section at the end of the section to dive deeper.",
    "crumbs": [
      "Section 3 — AI and LLMs in Educational Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7 **Multimodal Data (Images, Video, Audio) with Local LLMs**</span>"
    ]
  },
  {
    "objectID": "chapter-7.html#analyzing-images",
    "href": "chapter-7.html#analyzing-images",
    "title": "Chapter 7 Multimodal Data (Images, Video, Audio) with Local LLMs",
    "section": "7.3 Analyzing Images",
    "text": "7.3 Analyzing Images\nWith the advent of Large-Language Models (LLMs) we can use their power to analyze images. In this section, we will focus on using one package that uses local LLMs (i.e., privacy) to analyze image files: {kuzco}.\nKuzco is\n\nis a simple vision boilerplate built for ollama in R, on top of {ollamar} & {ellmer}. {kuzco} is designed as a computer vision assistant, giving local models guidance on classifying images and return structured data. The goal is to standardize outputs for image classification and use LLMs as an alternative option to keras or torch. {kuzco} currently supports: classification, recognition, sentiment, text extraction, alt-text creation, and custom computer vision tasks.\n\n\n7.3.1. Setting Up Kuzco\nTo use kuzco, you need to, first, install Ollama (a software that allows pulling and running local LLMs) and ollamar & ellmer packages.\nYou can install Ollama by downloading and installing the application from its provider’s website. Basically the steps are:\n\nDownload and install the Ollama app.\n\n\nmacOS\nWindows preview\nLinux: curl -fsSL https://ollama.com/install.sh | sh\nDocker image\n\n\nOpen/launch the Ollama app to start the local server.\n\nAfter installing Ollama, you will then need to install ollamar and ellmer:\n\ninstall.packages(\"ollamar\")\ninstall.packages(\"ellmer\")\n\nOnce these are installed, install kuzco:\n\ndevtools::install_github(\"frankiethull/kuzco\")",
    "crumbs": [
      "Section 3 — AI and LLMs in Educational Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7 **Multimodal Data (Images, Video, Audio) with Local LLMs**</span>"
    ]
  },
  {
    "objectID": "chapter-7.html#image-classification",
    "href": "chapter-7.html#image-classification",
    "title": "Chapter 7 Multimodal Data (Images, Video, Audio) with Local LLMs",
    "section": "7.3.2 Image Classification",
    "text": "7.3.2 Image Classification\nAn important function {kuzco} package provides is to create a data frame from the objects of a given image by classifying it.\n\nCase Study: Analyzing Classroom Photographs with Kuzco to Explore Student Engagement\n\n7.3.2.1 Purpose\nIn a study on student engagement during collaborative science instruction, a researcher used a series of classroom photographs to better understand how students participated in different types of learning activities. Rather than relying solely on manual observation and field notes, the researcher applied the {kuzco} R package to process and interpret visual data. Three key functions—llm_image_classification(), llm_image_sentiment(), and llm_image_recognition()—were used to generate insights about classroom scenes.\nThese tools allowed the researcher to (1) classify the overall content of the image (e.g., lab work, discussion, presentation), (2) recognize and count key objects or people in the frame (e.g., students, materials, whiteboards), and (3) estimate the emotional tone of the scene based on posture and facial cues. This approach enabled a more systematic and scalable analysis of classroom engagement, providing structured outputs that could be interpreted alongside observational data and interview responses.\n\n\n\n7.3.2.2 Research Questions\nTo investigate the nature of a classroom discourse, in this study, our research questions are:\n\nRQ1: How do classroom activities, as categorized through image classification, vary across different phases of science instruction?\nRQ2: How do student group sizes and use of instructional materials differ across classroom photographs?\nRQ3: What patterns of emotional tone emerge in classroom scenes during collaborative learning, as estimated through visual sentiment analysis?\n\n\n\n7.3.2.3 Methods\nThis study used visual data from middle school science classrooms to explore patterns of student interaction, task engagement, and classroom atmosphere across different instructional moments. The analysis was supported by large language model (LLM)-based image processing tools from the {kuzco} R package, allowing for efficient classification, recognition, and sentiment estimation without advanced machine learning expertise.\n\n\n7.3.2.4 Data Source\nThe dataset consisted of 48 photographs taken during four 7th-grade science lessons, each lasting approximately 60 minutes. Photos were captured every 5–7 minutes by a stationary camera positioned at the back of the room to minimize disruption. All images were de-identified prior to analysis to protect student privacy. Each photo represented a naturally occurring moment of group-based learning and was accompanied by a brief instructional context log maintained by the classroom observer.\n\n\n7.3.2.5 Data Analysis\nImages were processed using the following {kuzco} functions:\n\nllm_image_classification(): Generated scene-level labels and narrative summaries (e.g., “students engaged in group discussion around lab materials”).\nllm_image_recognition(): Identified and counted key visual entities such as students, desks, instructional materials, and gestures\nllm_image_sentiment(): Estimated the emotional tone of each scene (e.g., positive, neutral, frustrated), with particular attention to student posture and interaction dynamics.\n\nThe structured outputs were imported into R for organization and thematic coding. Using both deductive categories (e.g., group size, task type) and inductive patterns (e.g., collaborative vs. passive positioning), the researcher examined how engagement varied across activities. Triangulation with field notes enhanced interpretive validity, and descriptive summaries were generated to visualize classroom dynamics over time.\nFor the purpose of simplicity, we will only analyze two photos from a folder. The process for batch analysis can be increased to more photos.\n\n\nWith the code below, we create a function to batch analyze images:\n\nlibrary(kuzco)\nlibrary(ollamar)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(fs)\n\n# Set your image folder path\nimage_folder &lt;- \"/Users/makcaoglu/Documents/CSS_Book/data/s5_images\" #hide this before pub\n\n# List images (adjust pattern as needed)\nimage_files &lt;- dir_ls(image_folder, regexp = \"\\\\.(jpg|jpeg|png)$\", recurse = FALSE)\n\n# Function to classify and detect in one step\nprocess_image &lt;- function(img_path) {\n  # Classification\n  classification &lt;- llm_image_classification(\n    llm_model = \"qwen2.5vl:7b\",\n    image = img_path,\n    backend = 'ellmer'\n  )\n  \n  # Object detection (e.g., people)\n  detection &lt;- llm_image_recognition(\n    llm_model = \"qwen2.5vl:7b\",\n    image = img_path,\n    recognize_object = \"people\",\n    backend = 'ellmer'\n  )\n  \n  # Sentiment/emotion\n  sentiment &lt;- llm_image_sentiment(\n    llm_model = \"qwen2.5vl:7b\",\n    image = img_path\n  )\n  \n  #the new custom fuction for sentiment\n  customized &lt;- llm_image_custom(\n    llm_model = \"qwen2.5vl:7b\",\n    image = img_path,\n    backend = \"ellmer\",\n    system_prompt = \"You are an expert classroom observer. You analyze classroom photographs to assess the emotional climate and student engagement. Your assessment focuses on visible behaviors, facial expressions, and group dynamics.\",\n    image_prompt = \"Describe the overall sentiment of the classroom and explain what visual cues support your conclusion.\",\n    example_df = data.frame(\n      classroom_sentiment = \"positive\",\n      engagement_level = \"high\",\n      sentiment_rationale = \"Students are smiling, interacting with each other, and appear attentive to the teacher. Desks are arranged for group work.\"\n    )\n  )\n  \n  # Return combined tibble\n  tibble::tibble(\n  file = img_path,\n  image_classification = classification$image_classification,\n  primary_object = classification$primary_object,\n  secondary_object = classification$secondary_object,\n  image_description = classification$image_description,\n  image_colors = classification$image_colors,\n  image_proba_names = paste(unlist(classification$image_proba_names), collapse = \", \"),\n  image_proba_values = paste(unlist(classification$image_proba_values), collapse = \", \"),\n  object_recognized = detection$object_recognized,\n  object_count = detection$object_count,\n  object_description = detection$object_description,\n  object_location = detection$object_location,\n  classroom_sentiment = customized$classroom_sentiment,\n  engagement_level = customized$engagement_level,\n  sentiment_rationale = customized$sentiment_rationale\n)\n\n}\n\nNow, we run the analyses:\n\n# Apply to all images and combine into one data frame\nresults_df &lt;- map_dfr(image_files, process_image)\n\n# View result\nprint(results_df)\n\n\n# Arrange columns in logical order and rename for clarity\nresults_clean &lt;- results_df |&gt;\n  select(\n    image_classification,\n    image_description,\n    primary_object,\n    secondary_object,\n    object_recognized,\n    object_count,\n    object_description,\n    image_proba_names,\n    image_proba_values,\n    classroom_sentiment,\n    engagement_level,\n    sentiment_rationale,\n  )\n\n# Save to CSV (optional)\nwrite.csv(results_clean, \"image_classification_detection_results.csv\", row.names = FALSE)\n\n# View top images with the most people (if desired)\nresults_clean |&gt; \n  arrange(desc(object_count)) %&gt;% head(5)\n\n\n\n7.3.2.6 Results and Discussion:\nThe analysis of classroom photographs using the {kuzco} package yielded structured insights across three domains: instructional context (classification), observable features (recognition), and affective tone (sentiment). Below, we summarize preliminary findings from two sample images.\n\nRQ1: Variation in Classroom Activities Across Instructional Moments\nClassroom activity types were inferred using the image_classification and image_description columns generated by llm_image_classification().\nFour images reflected teacher-led instruction (Images 1, 2, 3, and 7). Although image_classification labeled these scenes generically as classroom, the image_description column emphasized teacher-directed discourse, including phrases such as “a teacher is giving a lesson at the front of the room” (Image 1), “a classroom setting where a person is speaking to students” (Image 3), and “students seated in rows facing the front” (Image 7). These images showed whole-group instructional formats dominated by teacher explanation.\nThree images reflected collaborative or interactive activity (Images 0, 6, and 8). The image_description column explicitly referenced peer interaction behaviors, including “a group of students … sitting together… reading a book” (Image 0), “students raising their hands” (Image 6), and “students actively participating and showing enthusiasm” (Image 8). These entries also aligned with primary_object values centered on “students” rather than instructional tools or teacher presence.\nTwo images depicted individual or independent work (Images 4 and 5). Evidence from image_description highlighted individual task engagement without peer or teacher interaction, such as “students are seated and reading from papers” (Image 4) and “students appear focused and are engaged in individual work” (Image 5).\nThese findings indicate variability in instructional format across images, with teacher-led instruction most frequent (44%), followed by collaborative interaction (33%) and independent work (22%).\n\n\nRQ2: Group Size and Use of Instructional Materials\nGroup size and material use were analyzed using object_count, primary_object, secondary_object, and object_list columns generated by llm_image_recognition().\nThe object_count column suggested observable group sizes ranging from 6 to 18 participants per image, with a median of 11. Teacher-led instruction was associated with larger visible groups (e.g., Images 1 and 3 showed 14–18 detected persons), while collaborative scenes tended to show smaller learning clusters (e.g., Images 0 and 8 with 6–8 persons), consistent with small-group activity structures.\nThe object_list column indicated consistent use of text-based materials (e.g., “books,” “papers,” “notebooks”) across seven images (Images 0, 2, 4, 5, 6, 7, 8). Instructional displays such as “chalkboard,” “whiteboard,” or “projector screen” appeared in five images (Images 1, 2, 3, 6, 7), primarily during teacher-directed instruction. Only one image (Image 5) contained references to technology, where object_list included “computers” and image_description mentioned “students working at laptops.”\nLab or experimental materials were absent from all images, likely reflecting the general nature of Wikimedia classroom photos rather than subject-specific science labs.\n\n\nRQ3: Emotional Tone and Engagement Across Classroom Scenes\nEmotional tone and behavioral participation were interpreted using the classroom_sentiment, engagement_level, and sentiment_rationale columns generated by llm_image_sentiment().\nSentiment was most often coded as neutral (classroom_sentiment = “neutral”; 4 images: 1, 2, 4, 5), followed by positive (3 images: 0, 3, 8) and moderately positive (2 images: 6, 7). However, student engagement varied independently of sentiment labels. The engagement_level column revealed a more nuanced pattern:\n\nHigh engagement (engagement_level = “high”) was observed in Images 0, 3, and 8, all of which also had positive sentiment. The sentiment_rationale referenced overt behavioral participation such as “students… interacting” (Image 0) and “raising hands” (Image 3).\nModerate engagement (engagement_level = “moderate”) appeared in four images (Images 4, 5, 6, 7), even when sentiment was neutral or moderately positive. Rationales included “students appear focused on their work” (Image 5) and “students raising their hands… paying attention” (Image 6).\nLow engagement (engagement_level = “low”) occurred in two images (Images 1 and 2), both of which were whole-class lecture scenes with passive student posture. Rationales noted “students appear disengaged; many do not make eye contact with the teacher.”\n\nTogether, these findings suggest that engagement was more sensitive to instructional structure than sentiment alone. Collaborative scenes showed the highest engagement, teacher-led instruction showed mixed engagement, and independent work produced moderate engagement with limited visible affect.\n\n\nDiscussion\nAcross the nine images, instructional format (RQ1) and classroom structure (RQ2) appeared to shape student participation patterns (RQ3). Collaborative activity was consistently associated with smaller group sizes and higher behavioral engagement. Teacher-led instruction involved larger groups and produced more passive engagement patterns. Independent work reflected focused but emotionally neutral learning states. Sentiment alone provided limited insight; however, the combination of engagement_level and observed activity type offered a more reliable indicator of classroom interaction quality.",
    "crumbs": [
      "Section 3 — AI and LLMs in Educational Research",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chapter 7 **Multimodal Data (Images, Video, Audio) with Local LLMs**</span>"
    ]
  },
  {
    "objectID": "chapter-8.html",
    "href": "chapter-8.html",
    "title": "Chapter 8 Communication Collaboration Practices",
    "section": "",
    "text": "Abstract\nA key part of social science research (and, any analysis involving computational tools) is communicating about the output or findings. In this section, we describe how to communicate with colleagues or the wider world through the use of a variety of tools, especially R Markdown and git/GitHub. We also discuss how to collaborate on projects that involve computational methods and “good” (flexible yet principled) practices for doing so based on our experience and prior scholarship.",
    "crumbs": [
      "Section 4 — Communication and Collaboration Practices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 8 **Communication Collaboration Practices**</span>"
    ]
  },
  {
    "objectID": "chapter-8.html#r-markdown-vs.-quarto-vs.-jupyter-notebooks",
    "href": "chapter-8.html#r-markdown-vs.-quarto-vs.-jupyter-notebooks",
    "title": "Chapter 8 Communication Collaboration Practices",
    "section": "R Markdown vs. Quarto vs. Jupyter Notebooks?",
    "text": "R Markdown vs. Quarto vs. Jupyter Notebooks?\nxx",
    "crumbs": [
      "Section 4 — Communication and Collaboration Practices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 8 **Communication Collaboration Practices**</span>"
    ]
  },
  {
    "objectID": "chapter-8.html#quarto",
    "href": "chapter-8.html#quarto",
    "title": "Chapter 8 Communication Collaboration Practices",
    "section": "Quarto",
    "text": "Quarto\nxx",
    "crumbs": [
      "Section 4 — Communication and Collaboration Practices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chapter 8 **Communication Collaboration Practices**</span>"
    ]
  }
]