# Section 5 **Multimodal Data (Images, Video, Audio) with LLMs**

## 5.1 Overview

In this section, we will discuss how social scientists can move beyond traditional data types (e.g., text and numbers) and learn about capturing and analyzing multimodal data.\

Multimodal data includes audio, video, and other non‑textual information that gives a fuller picture of human behavior.\

Modern devices such as wearables, smartphones, and online platforms now let researchers collect large amounts of this mixed data.\

To make sense of it, we use computational tools that combine the different types:

-   image analysis for video frames,

-   voice‑to‑text software for audio, and

-   machine‑learning models that link text, pictures, and sensor signals.

These tools let researchers ask new questions. For example, how body language and tone of voice together affect conversations or how physiological signals match feelings can help uncover insights that single‑mode studies miss. By integrating multimodal data, social scientists can broaden the depth and reach of their research beyond what conventional single‑mode analysis offers.

## 5.2 Images

Image data can come from the usual sources such as field photographs taken during site visits, archival collections in libraries or museums, and printed photographs that appear in historical documents. Nowadays, however, images can be found and collected in many different ways. For example, social media platforms like Instagram, Facebook, and TikTok are rich with user‑generated photos; online photo repositories such as Flickr, Unsplash, and Wikimedia Commons host millions of images that are freely accessible; news outlets regularly publish photographs to accompany stories; satellite imagery from NASA or ESA provides large‑scale visual data; and everyday smartphone cameras capture images that can be shared in research settings. Please note that although we cover some prominent ways, this is by no means an exhaustive list. Therefore, please refer to the additional resources section at the end of the section to dive deeper.

## 3.3 Analyzing Images

With the advent of Large-Language Models (LLMs) we can use their power to analyze images. In this section, we will focus on using one package that uses local LLMs (i.e., privacy) to analyze image files: [{kuzco}](https://github.com/frankiethull/kuzco).

Kuzco is

> is a simple vision boilerplate built for ollama in R, on top of {ollamar} & {ellmer}. {kuzco} is designed as a computer vision assistant, giving local models guidance on classifying images and return structured data. The goal is to standardize outputs for image classification and use LLMs as an alternative option to keras or torch. {kuzco} currently supports: classification, recognition, sentiment, text extraction, alt-text creation, and **custom** computer vision tasks.

### 5.3.1. Setting Up Kuzco

To use kuzco, you need to, first, install Ollama (a software that allows pulling and running local LLMs) and ollamar & ellmer packages.

You can install Ollama by downloading and installing the application from its provider's website. Basically the steps are:

1.  Download and install the [Ollama](https://ollama.com/) app.

-   [macOS](https://ollama.com/download/Ollama-darwin.zip)

-   [Windows preview](https://ollama.com/download/OllamaSetup.exe)

-   Linux: `curl -fsSL https://ollama.com/install.sh | sh`

-   [Docker image](https://hub.docker.com/r/ollama/ollama)

2.  Open/launch the Ollama app to start the local server.

After installing Ollama, you will then need to install ollamar and ellmer:

```{r eval=FALSE}
install.packages("ollamar")
install.packages("ellmer")
```

Once these are installed, install kuzco:

```{r eval=FALSE}
devtools::install_github("frankiethull/kuzco")
```

## 5.3.2 Image Classification

An important function {kuzco} package provides is to create a data frame from the objects of a given image by classifying it.

### Case Study: Analyzing Classroom Photographs with Kuzco to Explore Student Engagement

#### 5.3.2.1 Purpose

In a study on student engagement during collaborative science instruction, a researcher used a series of classroom photographs to better understand how students participated in different types of learning activities. Rather than relying solely on manual observation and field notes, the researcher applied the `{kuzco}` R package to process and interpret visual data. Three key functions—`llm_image_classification()`, `llm_image_sentiment()`, and `llm_image_recognition()`—were used to generate insights about classroom scenes.

These tools allowed the researcher to (1) classify the overall content of the image (e.g., lab work, discussion, presentation), (2) recognize and count key objects or people in the frame (e.g., students, materials, whiteboards), and (3) estimate the emotional tone of the scene based on posture and facial cues. This approach enabled a more systematic and scalable analysis of classroom engagement, providing structured outputs that could be interpreted alongside observational data and interview responses.

### 5.3.2.2 Research Questions

To investigate the nature of a classroom discourse, in this study, our research questions are:

-   **RQ1:** How do classroom activities, as categorized through image classification, vary across different phases of science instruction?
-   **RQ2:** How do student group sizes and use of instructional materials differ across classroom photographs?
-   **RQ3:** What patterns of emotional tone emerge in classroom scenes during collaborative learning, as estimated through visual sentiment analysis?

### 5.3.2.3 Methods

This study used visual data from middle school science classrooms to explore patterns of student interaction, task engagement, and classroom atmosphere across different instructional moments. The analysis was supported by large language model (LLM)-based image processing tools from the `{kuzco}` R package, allowing for efficient classification, recognition, and sentiment estimation without advanced machine learning expertise.

### 5.3.2.4 Data Source

The dataset consisted of 48 photographs taken during four 7th-grade science lessons, each lasting approximately 60 minutes. Photos were captured every 5–7 minutes by a stationary camera positioned at the back of the room to minimize disruption. All images were de-identified prior to analysis to protect student privacy. Each photo represented a naturally occurring moment of group-based learning and was accompanied by a brief instructional context log maintained by the classroom observer.

### 5.3.2.5 Data Analysis

Images were processed using the following `{kuzco}` functions:

-   **`llm_image_classification()`**: Generated scene-level labels and narrative summaries (e.g., "students engaged in group discussion around lab materials").

-   **`llm_image_recognition()`**: Identified and counted key visual entities such as students, desks, instructional materials, and gestures

-   **`llm_image_sentiment()`**: Estimated the emotional tone of each scene (e.g., positive, neutral, frustrated), with particular attention to student posture and interaction dynamics.

The structured outputs were imported into R for organization and thematic coding. Using both deductive categories (e.g., group size, task type) and inductive patterns (e.g., collaborative vs. passive positioning), the researcher examined how engagement varied across activities. Triangulation with field notes enhanced interpretive validity, and descriptive summaries were generated to visualize classroom dynamics over time.

*For the purpose of simplicity, we will only analyze two photos from a folder. The process for batch analysis can be increased to more photos.*

![](data/s5_images/primary-school-teacher-working-with-students-in-classroom-838559688.jpg){width="575"}

![](data/s5_images/35320686-2551789306.jpg){width="576"}

With the code below, we create a function to batch analyze images:

```{r eval=TRUE}
library(kuzco)
library(ollamar)
library(tibble)
library(purrr)
library(dplyr)
library(fs)

# Set your image folder path
image_folder <- "/Users/makcaoglu/Documents/CSS_Book/data/s5_images" #hide this before pub

# List images (adjust pattern as needed)
image_files <- dir_ls(image_folder, regexp = "\\.(jpg|jpeg|png)$", recurse = FALSE)

# Function to classify and detect in one step
process_image <- function(img_path) {
  # Classification
  classification <- llm_image_classification(
    llm_model = "qwen2.5vl:7b",
    image = img_path,
    backend = 'ellmer'
  )
  
  # Object detection (e.g., people)
  detection <- llm_image_recognition(
    llm_model = "qwen2.5vl:7b",
    image = img_path,
    recognize_object = "people",
    backend = 'ellmer'
  )
  
  # Sentiment/emotion
  sentiment <- llm_image_sentiment(
    llm_model = "qwen2.5vl:7b",
    image = img_path
  )
  
  #the new custom fuction for sentiment
  customized <- llm_image_custom(
    llm_model = "qwen2.5vl:7b",
    image = img_path,
    backend = "ellmer",
    system_prompt = "You are an expert classroom observer. You analyze classroom photographs to assess the emotional climate and student engagement. Your assessment focuses on visible behaviors, facial expressions, and group dynamics.",
    image_prompt = "Describe the overall sentiment of the classroom and explain what visual cues support your conclusion.",
    example_df = data.frame(
      classroom_sentiment = "positive",
      engagement_level = "high",
      sentiment_rationale = "Students are smiling, interacting with each other, and appear attentive to the teacher. Desks are arranged for group work."
    )
  )
  
  # Return combined tibble
  tibble::tibble(
  file = img_path,
  image_classification = classification$image_classification,
  primary_object = classification$primary_object,
  secondary_object = classification$secondary_object,
  image_description = classification$image_description,
  image_colors = classification$image_colors,
  image_proba_names = paste(unlist(classification$image_proba_names), collapse = ", "),
  image_proba_values = paste(unlist(classification$image_proba_values), collapse = ", "),
  object_recognized = detection$object_recognized,
  object_count = detection$object_count,
  object_description = detection$object_description,
  object_location = detection$object_location,
  classroom_sentiment = customized$classroom_sentiment,
  engagement_level = customized$engagement_level,
  sentiment_rationale = customized$sentiment_rationale
)

}
```

Now, we run the analyses:

```{r eval=TRUE}

# Apply to all images and combine into one data frame
results_df <- map_dfr(image_files, process_image)

# View result
print(results_df)


# Arrange columns in logical order and rename for clarity
results_clean <- results_df |>
  select(
    file,
    image_classification,
    image_description,
    primary_object,
    secondary_object,
    object_recognized,
    object_count,
    object_description,
    image_proba_names,
    image_proba_values,
    classroom_sentiment,
    engagement_level,
    sentiment_rationale,
  )

# Save to CSV (optional)
write.csv(results_clean, "image_classification_detection_results.csv", row.names = FALSE)

# View top images with the most people (if desired)
results_clean |> 
  arrange(desc(object_count))

knitr::kable(results_clean)

```

### 5.3.2.6 Results and Discussion:

The analysis of classroom photographs using the `{kuzco}` package yielded structured insights across three domains: instructional context (classification), observable features (recognition), and affective tone (sentiment). Below, we summarize preliminary findings from two sample images.

#### RQ1: Activity Variation Across Instructional Phases

One image was labeled as “students engaged in group discussion,” reflecting a student‐centered phase of collaborative inquiry. The other was classified as “teacher demonstration at front of class,” corresponding to an instructor‐led modeling phase. These two snapshots illustrate how **collaborative group work** and **direct instruction** alternate during a lesson, supporting mixed‐mode science pedagogy.

#### RQ2: Student Group Sizes and Materials Use

The group discussion scene showed **6 students**, suggesting small‐group clusters. The demonstration scene captured **12 students** focused on the teacher, indicating whole‐class attention. Both images contained visible instructional materials (lab equipment, whiteboard), but object recognition focused on counting people. Future analyses should expand **`recognize_object`** to include discrete material counts. These counts align with expectations: small groups during hands‐on inquiry and full class during teacher‐led explanation.

#### RQ3: Emotional Tone Patterns in Collaborative Learning

The group discussion image was labeled **positive**, with cues of smiling and open body language. The demonstration image was rated **neutral**, consistent with attentive but less expressive posture. This suggests that **active collaboration** elicits more positive affect compared to passive observation phases.
